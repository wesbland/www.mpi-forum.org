<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Reduce</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node101">100. Reduce</a></H2>
<A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node102.htm#Node102"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node102.htm#Node102"> Predefined Reduction Operations</a>
<b>Previous: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<P>
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_REDUCE( sendbuf, recvbuf, count, datatype, op,  
root, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> address of send buffer (choice)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice,  
significant only at root)</TD></TR>  
<TR><TD> IN  count</TD><TD> number of elements in send buffer (non-negative  
integer)</TD></TR>  
<TR><TD> IN  datatype</TD><TD> data type of elements of send buffer (handle)</TD></TR>  
<TR><TD> IN  op</TD><TD> reduce operation (handle)</TD></TR>  
<TR><TD> IN  root</TD><TD> rank of root process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD>  communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Reduce(void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_REDUCE(SENDBUF, RECVBUF, COUNT, DATATYPE, OP, ROOT, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER COUNT, DATATYPE, OP, ROOT, COMM, IERROR <BR></tt>  
  
  
 <tt> void MPI::Comm::Reduce(const void* sendbuf, void* recvbuf,  int count, const MPI::Datatype&amp; datatype, const MPI::Op&amp; op,  int root) const = 0 <BR></tt>  
  
  
If  comm is an intracommunicator,  
 MPI_REDUCE combines the elements provided  
in the input buffer of each process in the  
group, using the operation  op, and returns the combined value in  
the output buffer of the process with rank  root.  
The input buffer is defined by the arguments  sendbuf,  
 count and  datatype; the output buffer is defined by  
the arguments  recvbuf,  count and  datatype;  
both have the same number of elements, with the same type.  
The routine is called by all group members using the same arguments  
for  count, datatype, op, root and  comm.  
Thus, all processes provide input buffers and output buffers of the  
same length, with elements of the same type.  
Each process can provide one element, or a sequence of elements,  
in which case the  
combine operation is executed element-wise on each entry of the sequence.  
For example, if the operation is  MPI_MAX and the send buffer  
contains two elements that are floating point numbers ( count = 2 and  
 datatype =  MPI_FLOAT), then <IMG WIDTH=118 HEIGHT=29 SRC="img46.gif">
 and <IMG WIDTH=67 HEIGHT=11 SRC="img47.gif">
.  
<P> 
Section <a href="node102.htm#Node102">Predefined Reduction Operations 
</a>, lists the set of predefined operations  
provided by  MPI. That section also enumerates  
the datatypes each operation can be applied to.  
In addition, users may define their own operations that can be  
overloaded to operate on several datatypes, either basic or derived.  
This is further explained in Section <a href="node105.htm#Node105">User-Defined Reduction Operations 
</a>.  
<P> 
The operation  op is always assumed to be  
associative.  All predefined operations are also assumed to be  
commutative.  Users may define operations that are assumed to be  
associative, but not commutative.  The ``canonical'' evaluation order  
of a reduction is determined by the ranks of the processes in the  
group.  However, the implementation can take  
advantage of associativity, or associativity and commutativity  
in order to change the order of evaluation.  
This may change the result of the reduction for operations that are not  
strictly associative and commutative, such as floating point addition.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
It is strongly recommended that  MPI_REDUCE be implemented so  
that the same result be obtained  
whenever the function is applied on the same arguments,  
appearing in the same order.  Note that this may  
prevent optimizations that take  
advantage of the physical location of processors.  
 (<em> End of advice to implementors.</em>) <BR> 
  
  
The  datatype argument of  MPI_REDUCE must be  
compatible with   
 op.    
Predefined operators work only with  
the  MPI types listed in Section <a href="node102.htm#Node102">Predefined Reduction Operations 
</a> and   
Section <a href="node104.htm#Node104">MINLOC and MAXLOC 
</a>.  Furthermore, the  
 datatype and  op given for predefined operators  
must be the same on all processes.  
<P> 
Note that it is possible for users to supply different user-defined operations  
to  MPI_REDUCE in each process.   MPI does not define which  
operations are used on which operands in this case.  
User-defined operators may operate on general, derived datatypes.  
  
In this case, each argument that  
the reduce operation is applied to is one element described by such a datatype,  
which may contain several basic values.  
This is further explained in Section <a href="node105.htm#Node105">User-Defined Reduction Operations 
</a>.  
  
 
<BR> 
<em> Advice to users.</em>  
<P> 
Users should make no assumptions about how  MPI_REDUCE is  
  implemented.  Safest is to ensure that the same function is passed to  
   MPI_REDUCE by each process.  
 (<em> End of advice to users.</em>) <BR> 
  
Overlapping datatypes are permitted in ``send'' buffers.   
Overlapping datatypes in ``receive'' buffers are erroneous  
and may give unpredictable results.  
<P> 
  
The ``in place'' option  for intracommunicators is specified by passing the  
value   
 MPI_IN_PLACE to the argument  sendbuf at the root.  
In such case, the input data is taken at the root from the receive buffer,  
where it will be replaced by the output data.  
<P> 
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Only send buffer arguments are significant in group B and only receive  
buffer arguments are significant at the root.  
  

<P>
<HR>
<A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node102.htm#Node102"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node102.htm#Node102"> Predefined Reduction Operations</a>
<b>Previous: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
