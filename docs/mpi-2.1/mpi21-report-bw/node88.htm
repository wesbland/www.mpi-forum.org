<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Applying Collective Operations to Intercommunicators</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node88">87. Applying Collective Operations to Intercommunicators</a></H2>
<A HREF="node87.htm#Node87"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node86.htm#Node86"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node89.htm#Node89"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node86.htm#Node86"> Communicator Argument</a>
<b>Next: </b><A HREF="node89.htm#Node89"> Specifics for Intercommunicator Collective Operations</a>
<b>Previous: </b><A HREF="node87.htm#Node87"> Specifics for Intracommunicator Collective Operations</a>
<P>
  
  
  
  
  
<P> 
  
<P> 
<P> 
  
  
  
<P> 
<P> 
To understand how collective operations apply to intercommunicators,  
we can view most  MPI intracommunicator  
collective operations as fitting one of the following categories (see, for  
instance,   
[<a href="node427.htm#-Bib43">43</a>]):  
<dl> 
 
<dt> 
<b>All-To-All</b><dd> 
All processes contribute to the result.  All processes  
  receive the result.    
  <ul> 
   
<li> MPI_ALLGATHER,  MPI_ALLGATHERV  
   
<li> MPI_ALLTOALL,  MPI_ALLTOALLV,  
         MPI_ALLTOALLW  
   
<li> MPI_ALLREDUCE,  MPI_REDUCE_SCATTER  
  </ul> 
<BR> 
 
<dt> 
<b>All-To-One</b><dd> 
All processes contribute to the result.  One process  
  receives the result.  
  <ul> 
   
<li> MPI_GATHER,  MPI_GATHERV  
   
<li> MPI_REDUCE  
  </ul> 
<BR> 
 
<dt> 
<b>One-To-All</b><dd> 
One process contributes to the result.  All processes  
  receive the result.  
  <ul> 
   
<li> MPI_BCAST  
   
<li> MPI_SCATTER,  MPI_SCATTERV  
  </ul> 
<BR> 
 
<dt> 
<b>Other</b><dd> 
Collective operations that do not fit into one of the above  
  categories.  
  <ul> 
   
<li> MPI_SCAN,  MPI_EXSCAN  
   
<li> MPI_BARRIER  
  </ul> 
<BR> 
</dl> 
<BR> 
The  MPI_BARRIER operation does not fit into this classification  
since no data is being moved (other than the implicit fact that a barrier has  
been called).  The data movement patterns of  MPI_SCAN   
and  MPI_EXSCAN  
do not fit this taxonomy.  
<P> 
<P> 
The application of collective communication to  
intercommunicators is best described in terms of two groups.  
For example, an all-to-all  
 MPI_ALLGATHER operation can be described as collecting data  
from all members of one group with the result appearing in all members  
of the other group (see Figure <a href="node88.htm#Figure2">2 
</a>).  As another  
example, a one-to-all  MPI_BCAST operation sends data from one  
member of one group to all members of the other group.  
Collective computation operations such as  MPI_REDUCE_SCATTER have  
a   
similar interpretation (see Figure <a href="node88.htm#Figure3">3 
</a>).  
For intracommunicators, these two groups  
are the same.  For intercommunicators, these two groups are distinct.  
For the all-to-all operations, each such operation is described in two phases,  
so that it   
has a symmetric, full-duplex behavior.  
<P> 
The following collective operations also apply to intercommunicators:  
<ul> 
 
<li> MPI_BARRIER,  
 
<li> MPI_BCAST,  
 
<li> MPI_GATHER,  MPI_GATHERV,  
 
<li> MPI_SCATTER,  MPI_SCATTERV,  
 
<li> MPI_ALLGATHER,  MPI_ALLGATHERV,  
 
<li> MPI_ALLTOALL,  MPI_ALLTOALLV,  MPI_ALLTOALLW,  
 
<li> MPI_ALLREDUCE,  MPI_REDUCE,  
 
<li> MPI_REDUCE_SCATTER.  
</ul> 
<BR> 
In C++, the bindings for these functions are in the <tt> MPI::Comm</tt> class.  
However,  
since the collective operations do not make  
sense on a C++ <tt> MPI::Comm</tt>  
(as  
it is neither an intercommunicator nor an intracommunicator),  
the functions are all pure virtual.  
<P> 
<P> 
  <CENTER><P><IMG WIDTH=533 HEIGHT=406 SRC="collective-allgather.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 2: </b><A NAME="Figure2">Intercommunicator allgather.  The focus of data to one process is
    represented, not mandated by the semantics. 
    The two phases do allgathers in both directions.</a><P> 
  
    
  
  <CENTER><P><IMG WIDTH=533 HEIGHT=406 SRC="collective-reduce_scatter.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 3: </b><A NAME="Figure3">Intercommunicator reduce-scatter.  The focus of data to one process
    is represented, not mandated by the semantics.
    The two phases do reduce-scatters in both directions.</a><P> 
  
    
  

<P>
<HR>
<A HREF="node87.htm#Node87"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node86.htm#Node86"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node89.htm#Node89"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node86.htm#Node86"> Communicator Argument</a>
<b>Next: </b><A HREF="node89.htm#Node89"> Specifics for Intercommunicator Collective Operations</a>
<b>Previous: </b><A HREF="node87.htm#Node87"> Specifics for Intracommunicator Collective Operations</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
