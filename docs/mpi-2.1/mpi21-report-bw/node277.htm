<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-io/io-2.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Data Access with Individual File Pointers</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node277">256. Data Access with Individual File Pointers</a></H2>
<A HREF="node276.htm#Node276"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node278.htm#Node278"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node270.htm#Node270"> Data Access</a>
<b>Next: </b><A HREF="node278.htm#Node278"> Data Access with Shared File Pointers</a>
<b>Previous: </b><A HREF="node276.htm#Node276"> Data Access with Explicit Offsets</a>
<P>
  
<P> 
 MPI maintains one individual file pointer  
per process per  
file handle.  
The current value of this pointer implicitly specifies  
the offset in the data access routines described in this section.  
  
These routines only use and update the individual file pointers  
maintained by  MPI.  
The shared file pointer is not used nor updated.  
<P> 
The individual file pointer routines  
have the same semantics as the data access with explicit offset routines  
described in Section <a href="node276.htm#Node276">Data Access with Explicit Offsets 
</a>, page <a href="node276.htm#Node276">Data Access with Explicit Offsets 
</a>,  
with the following modification:  
<ul> 
 
<li>the  offset is defined to be the current value  
        of the  MPI-maintained individual file pointer.  
</ul> 
<BR> 
After an individual file pointer operation is initiated,  
the individual file pointer is updated to point  
to the next etype after the last one   
  
that will be accessed.  
  
The file pointer is updated relative to the current view of the file.  
<P> 
If  MPI_MODE_SEQUENTIAL mode was specified when the file  
was opened, it is erroneous to call the routines in this   
section, with  
the exception of  MPI_FILE_GET_BYTE_OFFSET.  
  
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ(fh, buf, count, datatype, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_read(MPI_File fh, void *buf, int count, MPI_Datatype datatype, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_READ(FH, BUF, COUNT, DATATYPE, STATUS, IERROR) <BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Read(void* buf, int count, const MPI::Datatype&amp; datatype, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Read(void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
 MPI_FILE_READ reads a file using the individual file pointer.  
  
  
<P> 
<BR><b> Example</b>  
The following Fortran code fragment is an example of reading  
a file until the end of file is reached:  
<P> 
  
  
  
  
<BR> 
<pre><tt>!   Read a preexisting input file until all data has been read. 
!   Call routine "process_input" if all requested data is read. 
!   The Fortran 90 "exit" statement exits the loop. 
 
      integer   bufsize, numread, totprocessed, status(MPI_STATUS_SIZE) 
      parameter (bufsize=100) 
      real      localbuffer(bufsize) 
 
      call MPI_FILE_OPEN( MPI_COMM_WORLD, 'myoldfile', &amp; 
                          MPI_MODE_RDONLY, MPI_INFO_NULL, myfh, ierr ) 
      call MPI_FILE_SET_VIEW( myfh, 0, MPI_REAL, MPI_REAL, 'native', &amp; 
                          MPI_INFO_NULL, ierr ) 
      totprocessed = 0 
      do 
         call MPI_FILE_READ( myfh, localbuffer, bufsize, MPI_REAL, &amp; 
                             status, ierr ) 
         call MPI_GET_COUNT( status, MPI_REAL, numread, ierr ) 
         call process_input( localbuffer, numread ) 
         totprocessed = totprocessed + numread 
         if ( numread &lt; bufsize ) exit 
      enddo 
 
      write(6,1001) numread, bufsize, totprocessed 
1001  format( "No more data:  read", I3, "and expected", I3, &amp; 
              "Processed total of", I6, "before terminating job." ) 
 
      call MPI_FILE_CLOSE( myfh, ierr ) 
</tt></pre> 
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ALL(fh, buf, count, datatype, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_all(MPI_File fh, void *buf, int count, MPI_Datatype datatype, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_READ_ALL(FH, BUF, COUNT, DATATYPE, STATUS, IERROR) <BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Read_all(void* buf, int count, const MPI::Datatype&amp; datatype, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Read_all(void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
 MPI_FILE_READ_ALL is a collective version  
of the blocking  MPI_FILE_READ interface.  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE(fh, buf, count, datatype, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_write(MPI_File fh, void *buf, int count, MPI_Datatype datatype, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_WRITE(FH, BUF, COUNT, DATATYPE, STATUS, IERROR) <BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Write(const void* buf, int count, const MPI::Datatype&amp; datatype, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Write(const void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
 MPI_FILE_WRITE writes a file using the individual file pointer.  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ALL(fh, buf, count, datatype, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_write_all(MPI_File fh, void *buf, int count, MPI_Datatype datatype, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_WRITE_ALL(FH, BUF, COUNT, DATATYPE, STATUS, IERROR) <BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Write_all(const void* buf, int count, const MPI::Datatype&amp; datatype, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Write_all(const void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
 MPI_FILE_WRITE_ALL is a collective version  
of the blocking  MPI_FILE_WRITE interface.  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_IREAD(fh, buf, count, datatype, request)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
<TR><TD> OUT request</TD><TD>request object (handle)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_iread(MPI_File fh, void *buf, int count, MPI_Datatype datatype, MPI_Request *request) <BR></tt>  
 <tt> MPI_FILE_IREAD(FH, BUF, COUNT, DATATYPE, REQUEST, IERROR) <BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, REQUEST, IERROR <BR></tt>  
 <tt> MPI::Request MPI::File::Iread(void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
 MPI_FILE_IREAD is a nonblocking version of the  MPI_FILE_READ interface.  
  
  
<P> 
<BR><b> Example</b>  
The following Fortran code fragment illustrates file pointer  
update semantics:  
<P> 
  
  
  
  
  
<BR> 
<pre><tt>!   Read the first twenty real words in a file into two local 
!   buffers.  Note that when the first MPI_FILE_IREAD returns, 
!   the file pointer has been updated to point to the  
!   eleventh real word in the file. 
 
      integer   bufsize, req1, req2 
      integer, dimension(MPI_STATUS_SIZE) :: status1, status2 
      parameter (bufsize=10) 
      real      buf1(bufsize), buf2(bufsize) 
 
      call MPI_FILE_OPEN( MPI_COMM_WORLD, 'myoldfile', &amp; 
                          MPI_MODE_RDONLY, MPI_INFO_NULL, myfh, ierr ) 
      call MPI_FILE_SET_VIEW( myfh, 0, MPI_REAL, MPI_REAL, 'native', &amp; 
                         MPI_INFO_NULL, ierr ) 
      call MPI_FILE_IREAD( myfh, buf1, bufsize, MPI_REAL, &amp; 
                           req1, ierr ) 
      call MPI_FILE_IREAD( myfh, buf2, bufsize, MPI_REAL, &amp; 
                           req2, ierr ) 
 
      call MPI_WAIT( req1, status1, ierr ) 
      call MPI_WAIT( req2, status2, ierr ) 
 
      call MPI_FILE_CLOSE( myfh, ierr ) 
</tt></pre> 
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_IWRITE(fh, buf, count, datatype, request)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
<TR><TD> OUT request</TD><TD>request object (handle)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_iwrite(MPI_File fh, void *buf, int count, MPI_Datatype datatype, MPI_Request *request) <BR></tt>  
 <tt> MPI_FILE_IWRITE(FH, BUF, COUNT, DATATYPE, REQUEST, IERROR) <BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, REQUEST, IERROR <BR></tt>  
 <tt> MPI::Request MPI::File::Iwrite(const void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
 MPI_FILE_IWRITE is a nonblocking version of the  MPI_FILE_WRITE interface.  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_SEEK(fh, offset, whence)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN offset</TD><TD>file offset (integer)</TD></TR>  
<TR><TD> IN whence</TD><TD>update mode (state)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_seek(MPI_File fh, MPI_Offset offset, int whence) <BR></tt>  
 <tt> MPI_FILE_SEEK(FH, OFFSET, WHENCE, IERROR)<BR> INTEGER FH, WHENCE, IERROR<BR>INTEGER(KIND=MPI_OFFSET_KIND) OFFSET <BR></tt>  
 <tt> void MPI::File::Seek(MPI::Offset offset, int whence) <BR></tt>  
 <P> 
 MPI_FILE_SEEK updates the individual file pointer according to   
 whence,  
which has the following possible values:  
  
<ul> 
 
<li> MPI_SEEK_SET:  
        the pointer is set to  offset  
 
<li> MPI_SEEK_CUR:  
        the pointer is set to the current pointer position plus  offset  
 
<li> MPI_SEEK_END:  
        the pointer is set to the end of   
  
file  
  
plus  offset  
</ul> 
<BR> 
The  offset can be negative, which allows seeking backwards.  
It is erroneous to seek to a negative position in the view.  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_GET_POSITION(fh, offset)</TD></TR>  
<TR><TD> IN fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT offset</TD><TD>offset of individual pointer (integer)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_get_position(MPI_File fh, MPI_Offset *offset) <BR></tt>  
 <tt> MPI_FILE_GET_POSITION(FH, OFFSET, IERROR)<BR> INTEGER FH, IERROR <BR>INTEGER(KIND=MPI_OFFSET_KIND) OFFSET <BR></tt>  
  
 <tt> MPI::Offset MPI::File::Get_position() const <BR></tt>  
  
<P> 
 MPI_FILE_GET_POSITION returns, in  offset,  
the current position of the individual file pointer in etype units  
relative to the current view.  
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
The  offset can be used in a future call to  MPI_FILE_SEEK  
using  whence =  MPI_SEEK_SET to return to the current position.  
To set the displacement to the current file pointer position,  
first convert  offset into an absolute byte position using  
 MPI_FILE_GET_BYTE_OFFSET,  
then call  MPI_FILE_SET_VIEW with the resulting  
displacement.  
 (<em> End of advice to users.</em>) <BR> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_GET_BYTE_OFFSET(fh, offset, disp)</TD></TR>  
<TR><TD> IN fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN offset</TD><TD>offset (integer)</TD></TR>  
<TR><TD> OUT disp</TD><TD>absolute byte position of offset (integer)</TD></TR>  
</TABLE>  
 <P> 
 <tt> int MPI_File_get_byte_offset(MPI_File fh, MPI_Offset offset, MPI_Offset *disp) <BR></tt>  
 <tt> MPI_FILE_GET_BYTE_OFFSET(FH, OFFSET, DISP, IERROR)<BR> INTEGER FH, IERROR <BR>INTEGER(KIND=MPI_OFFSET_KIND) OFFSET, DISP <BR></tt>  
 <tt> MPI::Offset MPI::File::Get_byte_offset(const MPI::Offset disp) const <BR></tt>  
  
<P> 
 MPI_FILE_GET_BYTE_OFFSET converts a view-relative offset  
into an absolute byte position.  
The absolute byte position (from the beginning of the file)  
of  offset relative to the current view of  fh  
is returned in  disp.  
<P> 

<P>
<HR>
<A HREF="node276.htm#Node276"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node278.htm#Node278"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node270.htm#Node270"> Data Access</a>
<b>Next: </b><A HREF="node278.htm#Node278"> Data Access with Shared File Pointers</a>
<b>Previous: </b><A HREF="node276.htm#Node276"> Data Access with Explicit Offsets</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
