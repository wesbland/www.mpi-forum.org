<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Predefined Reduction Operations</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node102">101. Predefined Reduction Operations</a></H2>
<A HREF="node101.htm#Node101"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node103.htm#Node103"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node103.htm#Node103"> Signed Characters and Reductions</a>
<b>Previous: </b><A HREF="node101.htm#Node101"> Reduce</a>
<P>
  
<P> 
The following predefined operations are supplied for  MPI_REDUCE  
and related functions  MPI_ALLREDUCE,  MPI_REDUCE_SCATTER,  
 MPI_SCAN,  
and  MPI_EXSCAN.  
These operations are invoked by placing the following in  op.  
<P> 
    
      
      
     
<BR> 
 Name Meaning  
 
<BR> 
    
 
<BR> 
 MPI_MAX maximum  
 
<BR> 
 MPI_MIN minimum  
 
<BR> 
 MPI_SUM sum  
 
<BR> 
 MPI_PROD product  
 
<BR> 
 MPI_LAND logical and   
 
<BR> 
 MPI_BAND bit-wise and   
 
<BR> 
 MPI_LOR logical or   
 
<BR> 
 MPI_BOR bit-wise or   
 
<BR> 
 MPI_LXOR logical exclusive or (xor)  
 
<BR> 
 MPI_BXOR bit-wise exclusive or (xor)  
 
<BR> 
 MPI_MAXLOC max value and location  
 
<BR> 
 MPI_MINLOC min value and location  
<BR> 
  
<P> 
The two operations  MPI_MINLOC and  MPI_MAXLOC are  
discussed separately in Section <a href="node104.htm#Node104">MINLOC and MAXLOC 
</a>.  
For the other predefined operations,  
we enumerate below the allowed combinations of  op and  
 datatype arguments.  
First, define groups of  MPI basic datatypes  
in the following way.  
<P> 
    
      
      
     
<BR> 
 C integer: MPI_INT,  MPI_LONG,  MPI_SHORT,  
 
<BR> 
            MPI_UNSIGNED_SHORT,  MPI_UNSIGNED,  
 
<BR> 
            MPI_UNSIGNED_LONG,  
 
<BR> 
            MPI_LONG_LONG_INT,  
 
<BR> 
            MPI_LONG_LONG (as synonym),  
 
<BR> 
            MPI_UNSIGNED_LONG_LONG,  
 
<BR> 
            MPI_SIGNED_CHAR,  MPI_UNSIGNED_CHAR  
 
<BR> 
 Fortran integer: MPI_INTEGER  
 
<BR> 
 Floating point: MPI_FLOAT,  MPI_DOUBLE,  MPI_REAL,  
 
<BR> 
            MPI_DOUBLE_PRECISION  
 
<BR> 
            MPI_LONG_DOUBLE  
 
<BR> 
 Logical: MPI_LOGICAL  
 
<BR> 
 Complex: MPI_COMPLEX  
 
<BR> 
 Byte: MPI_BYTE  
<BR> 
  
<P> 
Now, the valid datatypes for each option is specified below.  
<P> 
    
      
      
     
<BR> 
 Op Allowed Types  
 
<BR> 
    
  
 
<BR> 
 MPI_MAX,  MPI_MIN C integer, Fortran integer, Floating point  
  
 
<BR> 
 MPI_SUM,  MPI_PROD C integer, Fortran integer, Floating point, Complex  
 
<BR> 
 MPI_LAND,  MPI_LOR,  MPI_LXOR C integer, Logical  
 
<BR> 
 MPI_BAND,  MPI_BOR,  MPI_BXOR C integer, Fortran integer, Byte  
<BR> 
  
<P> 
The following examples use intracommunicators.  
   
<BR><b> Example</b>   
  
  
<P> 
A routine that computes  
the dot product of two vectors that are distributed across a  
group of processes and returns the answer at node zero.  
<P> 
<BR> 
<pre><tt>SUBROUTINE PAR_BLAS1(m, a, b, c, comm) 
REAL a(m), b(m)       ! local slice of array 
REAL c                ! result (at node zero) 
REAL sum 
INTEGER m, comm, i, ierr 
 
! local sum 
sum = 0.0 
DO i = 1, m 
   sum = sum + a(i)*b(i) 
END DO 
 
! global sum 
CALL MPI_REDUCE(sum, c, 1, MPI_REAL, MPI_SUM, 0, comm, ierr) 
RETURN 
</tt></pre> 
  
  
<P> 
<BR><b> Example</b>   
  
  
<P> 
A routine that computes  
the product of a vector and an array that are distributed across a  
group of processes and returns the answer at node zero.  
<P> 
<BR> 
<pre><tt>SUBROUTINE PAR_BLAS2(m, n, a, b, c, comm) 
REAL a(m), b(m,n)    ! local slice of array 
REAL c(n)            ! result 
REAL sum(n) 
INTEGER n, comm, i, j, ierr 
 
! local sum 
DO j= 1, n 
  sum(j) = 0.0 
  DO i = 1, m 
    sum(j) = sum(j) + a(i)*b(i,j) 
  END DO 
END DO 
 
! global sum 
CALL MPI_REDUCE(sum, c, n, MPI_REAL, MPI_SUM, 0, comm, ierr) 
 
! return result at node zero (and garbage at the other nodes) 
RETURN 
</tt></pre> 
  
  
  

<P>
<HR>
<A HREF="node101.htm#Node101"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node103.htm#Node103"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node103.htm#Node103"> Signed Characters and Reductions</a>
<b>Previous: </b><A HREF="node101.htm#Node101"> Reduce</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
