<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-io/io-2.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Split Collective Data Access Routines</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node282">258. Split Collective Data Access Routines</a></H2>
<A HREF="node278.htm#Node281"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node283.htm#Node283"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node270.htm#Node270"> Data Access</a>
<b>Next: </b><A HREF="node283.htm#Node283"> File Interoperability</a>
<b>Previous: </b><A HREF="node278.htm#Node281"> Seek</a>
<P>
  
<P> 
 MPI provides a restricted form of ``nonblocking collective'' I/O  
operations for all data accesses  
using split collective data access routines.  
These routines are referred to as ``split'' collective routines  
because a single collective operation is split in two:  
a begin routine and an end routine.  
The begin routine begins the operation,  
much like a nonblocking data access (e.g.,  MPI_FILE_IREAD).  
The end routine completes the operation,  
much like the matching test or wait (e.g.,  MPI_WAIT).  
As with nonblocking data access operations,  
the user must not use the buffer  
  
passed to a begin routine while the routine is outstanding;  
the operation must be completed with an end routine before it  
is safe to free buffers, etc.  
<P> 
Split collective data access operations on a file handle  fh  
are subject to the semantic rules given below.  
<P> 
<ul> 
 
<li>On any  MPI process,  
      each file handle may have at most one active split collective  
      operation at any time.  
  
<P> 
 
<li>Begin calls are collective over the group of processes  
that participated in the collective open and follow the ordering  
rules for collective calls.  
<P> 
 
<li>End calls are collective over the group of processes that  
participated in the collective open and follow the ordering  
  rules for collective calls.  
  Each end call matches  
  
  the preceding begin call for the same collective  
  operation.  When an ``end'' call is made, exactly one unmatched ``begin''  
  call for the same operation must precede it.  
<P> 
 
<li>An implementation is free to implement any split collective  
data access routine using the corresponding blocking collective  
routine when either the begin call (e.g.,  
 MPI_FILE_READ_ALL_BEGIN) or the end call (e.g.,  
 MPI_FILE_READ_ALL_END) is issued.  The begin and end calls are  
provided to allow the user and  MPI implementation to optimize the  
collective operation.  
<P> 
 
<li>Split collective operations do not match the corresponding  
regular collective operation.  
For example, in a single collective read operation,  
an  MPI_FILE_READ_ALL on one process does not match  
an  MPI_FILE_READ_ALL_BEGIN/ MPI_FILE_READ_ALL_END pair  
on another process.  
  
<P> 
 
<li>Split collective routines must specify a buffer  
in both the begin and end routines.  By specifying the buffer  
that receives data in the end routine, we can avoid many (though not  
all) of the problems described   
in ``A Problem with Register Optimization,''  
Section <a href="node337.htm#Node342">A Problem with Register Optimization 
</a>, page <a href="node337.htm#Node342">A Problem with Register Optimization 
</a>.  
<P> 
 
<li>No collective I/O operations are permitted on a file handle  
concurrently with a split collective access on that file handle  
(i.e., between the begin and end of the access). That is  
<BR> 
<pre><tt>                MPI_File_read_all_begin(fh, ...); 
                ... 
                MPI_File_read_all(fh, ...); 
                ... 
                MPI_File_read_all_end(fh, ...); 
</tt></pre> 
is erroneous.  
<P> 
 
<li>In a multithreaded implementation, any split collective  
                begin and end operation called by a process must be  
                called from the same thread.  This restriction is made  
                to simplify the implementation in the multithreaded  
                case.  
                (Note that we have already disallowed having two  
                threads begin a split collective operation on the same  
                file handle since only one split collective operation can be  
                active on a file handle at any time.)  
</ul> 
<BR> 
The arguments for these routines have the same meaning as for  
the equivalent collective versions  
(e.g., the argument definitions for  MPI_FILE_READ_ALL_BEGIN  
and  MPI_FILE_READ_ALL_END are equivalent to the  
arguments for  MPI_FILE_READ_ALL).  
The begin routine (e.g.,  MPI_FILE_READ_ALL_BEGIN)  
begins a split collective operation that,  
when completed with the matching end routine  
(i.e.,  MPI_FILE_READ_ALL_END)  
produces the result as defined for the equivalent collective  
routine (i.e.,  MPI_FILE_READ_ALL).  
<P> 
For the purpose of consistency semantics  
(Section <a href="node291.htm#Node291">File Consistency 
</a>,  
page <a href="node291.htm#Node291">File Consistency 
</a>),  
a matched pair of split collective data access operations  
(e.g.,  MPI_FILE_READ_ALL_BEGIN and  MPI_FILE_READ_ALL_END)  
compose a single data access.  
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_AT_ALL_BEGIN(fh, offset, buf, count, datatype)</TD></TR>  
<TR><TD> IN fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN offset</TD><TD>file offset (integer)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_at_all_begin(MPI_File fh, MPI_Offset offset, void *buf, int count, MPI_Datatype datatype) <BR></tt>  
 <tt> MPI_FILE_READ_AT_ALL_BEGIN(FH, OFFSET, BUF, COUNT, DATATYPE, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, IERROR <BR>INTEGER(KIND=MPI_OFFSET_KIND) OFFSET <BR></tt>  
 <tt> void MPI::File::Read_at_all_begin(MPI::Offset offset, void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_AT_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> IN fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_at_all_end(MPI_File fh, void *buf, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_READ_AT_ALL_END(FH, BUF, STATUS, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Read_at_all_end(void* buf, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Read_at_all_end(void* buf) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_AT_ALL_BEGIN(fh, offset, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN offset</TD><TD>file offset (integer)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_at_all_begin(MPI_File fh, MPI_Offset offset, void *buf, int count, MPI_Datatype datatype) <BR></tt>  
 <tt> MPI_FILE_WRITE_AT_ALL_BEGIN(FH, OFFSET, BUF, COUNT, DATATYPE, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, IERROR <BR>INTEGER(KIND=MPI_OFFSET_KIND) OFFSET <BR></tt>  
 <tt> void MPI::File::Write_at_all_begin(MPI::Offset offset, const void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_AT_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_at_all_end(MPI_File fh, void *buf, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_WRITE_AT_ALL_END(FH, BUF, STATUS, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Write_at_all_end(const void* buf, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Write_at_all_end(const void* buf) <BR></tt>  
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ALL_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_all_begin(MPI_File fh, void *buf, int count, MPI_Datatype datatype) <BR></tt>  
 <tt> MPI_FILE_READ_ALL_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, IERROR <BR></tt>  
 <tt> void MPI::File::Read_all_begin(void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_all_end(MPI_File fh, void *buf, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_READ_ALL_END(FH, BUF, STATUS, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Read_all_end(void* buf, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Read_all_end(void* buf) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ALL_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_all_begin(MPI_File fh, void *buf, int count, MPI_Datatype datatype) <BR></tt>  
 <tt> MPI_FILE_WRITE_ALL_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, IERROR <BR></tt>  
 <tt> void MPI::File::Write_all_begin(const void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_all_end(MPI_File fh, void *buf, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_WRITE_ALL_END(FH, BUF, STATUS, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Write_all_end(const void* buf, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Write_all_end(const void* buf) <BR></tt>  
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ORDERED_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_ordered_begin(MPI_File fh, void *buf, int count, MPI_Datatype datatype) <BR></tt>  
 <tt> MPI_FILE_READ_ORDERED_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, IERROR <BR></tt>  
 <tt> void MPI::File::Read_ordered_begin(void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ORDERED_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_ordered_end(MPI_File fh, void *buf, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_READ_ORDERED_END(FH, BUF, STATUS, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Read_ordered_end(void* buf, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Read_ordered_end(void* buf) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ORDERED_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_ordered_begin(MPI_File fh, void *buf, int count, MPI_Datatype datatype) <BR></tt>  
 <tt> MPI_FILE_WRITE_ORDERED_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, COUNT, DATATYPE, IERROR <BR></tt>  
 <tt> void MPI::File::Write_ordered_begin(const void* buf, int count, const MPI::Datatype&amp; datatype) <BR></tt>  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ORDERED_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_ordered_end(MPI_File fh, void *buf, MPI_Status *status) <BR></tt>  
 <tt> MPI_FILE_WRITE_ORDERED_END(FH, BUF, STATUS, IERROR)<BR> &lt;type&gt; BUF(*) <BR>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <BR></tt>  
 <tt> void MPI::File::Write_ordered_end(const void* buf, MPI::Status&amp; status) <BR></tt>  
 <tt> void MPI::File::Write_ordered_end(const void* buf) <BR></tt>  
<P> 

<P>
<HR>
<A HREF="node278.htm#Node281"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node283.htm#Node283"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node270.htm#Node270"> Data Access</a>
<b>Next: </b><A HREF="node283.htm#Node283"> File Interoperability</a>
<b>Previous: </b><A HREF="node278.htm#Node281"> Seek</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
