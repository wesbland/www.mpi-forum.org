<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-io/io-2.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Data Access Routines</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node271">254. Data Access Routines</a></H2>
<A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node272"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node270.htm#Node270"> Data Access</a>
<b>Next: </b><A HREF="node271.htm#Node272"> Positioning</a>
<b>Previous: </b><A HREF="node270.htm#Node270"> Data Access</a>
<P>
  
Data is moved between files and processes by issuing read and write calls.  
There are three orthogonal aspects to data access:  
positioning (explicit offset <em> vs.</em> implicit file pointer),  
synchronism (blocking <em> vs.</em> nonblocking and split collective),  
  
and coordination (noncollective <em> vs.</em> collective).  
The following combinations of these data access routines,  
including two types of file pointers (individual and shared)  
are provided in Table <a href="node271.htm#Table9">9 
</a>.  
<P> 
<CENTER>  
  
<TABLE><TR><TD ALIGN="LEFT">  
<b> positioning</TD><TD ALIGN="LEFT"> </b><b> synchronism</TD><TD ALIGN="LEFT"> 2c|<b> coordination</b> </TD></TR> 
<TR><TD ALIGN="LEFT">  
3-4  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT"> <em> noncollective</TD><TD ALIGN="LEFT"> </em><em> collective </TD></TR> 
<TR><TD ALIGN="LEFT">  
  
</em><em> explicit</TD><TD ALIGN="LEFT"> </em><em> blocking   
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_READ_AT</TD><TD ALIGN="LEFT">  MPI_FILE_READ_AT_ALL </TD></TR> 
<TR><TD ALIGN="LEFT">  
<em> offsets</TD><TD ALIGN="LEFT">   
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_WRITE_AT</TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_AT_ALL </TD></TR> 
<TR><TD ALIGN="LEFT">  
2-4  
</TD><TD ALIGN="LEFT"> <em> nonblocking &amp;  
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_IREAD_AT</TD><TD ALIGN="LEFT">  MPI_FILE_READ_AT_ALL_BEGIN </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"> <em> split collective  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT"> </em> MPI_FILE_READ_AT_ALL_END </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT">  MPI_FILE_IWRITE_AT</TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_AT_ALL_BEGIN </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_AT_ALL_END </TD></TR> 
<TR><TD ALIGN="LEFT">  
<em> individual</TD><TD ALIGN="LEFT"> </em><em> blocking  
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_READ</TD><TD ALIGN="LEFT">  MPI_FILE_READ_ALL </TD></TR> 
<TR><TD ALIGN="LEFT">  
<em> file pointers</TD><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_WRITE</TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_ALL </TD></TR> 
<TR><TD ALIGN="LEFT">  
2-4  
</TD><TD ALIGN="LEFT"> <em> nonblocking &amp;  
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_IREAD</TD><TD ALIGN="LEFT">  MPI_FILE_READ_ALL_BEGIN </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"> <em> split collective  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT"> </em> MPI_FILE_READ_ALL_END </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT">  MPI_FILE_IWRITE</TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_ALL_BEGIN </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_ALL_END </TD></TR> 
<TR><TD ALIGN="LEFT">  
<em> shared</TD><TD ALIGN="LEFT"> </em><em> blocking  
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_READ_SHARED</TD><TD ALIGN="LEFT">  MPI_FILE_READ_ORDERED </TD></TR> 
<TR><TD ALIGN="LEFT">  
<em> file pointer</TD><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_WRITE_SHARED</TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_ORDERED </TD></TR> 
<TR><TD ALIGN="LEFT">  
2-4  
</TD><TD ALIGN="LEFT"> <em> nonblocking &amp;  
</TD><TD ALIGN="LEFT"> </em> MPI_FILE_IREAD_SHARED</TD><TD ALIGN="LEFT">  MPI_FILE_READ_ORDERED_BEGIN </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"> <em> split collective  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT"> </em> MPI_FILE_READ_ORDERED_END </TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT">  MPI_FILE_IWRITE_SHARED</TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_ORDERED_BEGIN</TD></TR> 
<TR><TD ALIGN="LEFT">  
</TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT"></TD><TD ALIGN="LEFT">  MPI_FILE_WRITE_ORDERED_END </TD></TR> 
<TR><TD ALIGN="LEFT">  
  
</em></TD></TR></TABLE> 
</CENTER>  
<BR> 
<b>Table 9: </b><A NAME="Table9">Data access routines
</a><P> 
   
  
  
<P> 
POSIX read()/fread() and write()/fwrite() are blocking,  
noncollective operations and use individual file pointers.  
The  MPI equivalents are  MPI_FILE_READ and  MPI_FILE_WRITE.  
<P> 
Implementations of data access routines may buffer data to improve  
performance.  This does not affect reads, as the data is always  
available in the user's buffer after a read operation completes.  
For writes, however, the  MPI_FILE_SYNC routine provides the only  
guarantee that data has been transferred to the storage device.  
  
<P> 
<menu> 
</menu> 

<P>
<HR>
<A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node270.htm#Node270"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node272"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node270.htm#Node270"> Data Access</a>
<b>Next: </b><A HREF="node271.htm#Node272"> Positioning</a>
<b>Previous: </b><A HREF="node270.htm#Node270"> Data Access</a>
<P>
<HR><H3><A NAME="Node272">254.1. Positioning</a></H3>
<A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node273"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node271.htm#Node273"> Synchronism</a>
<b>Previous: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<P>
  
 MPI provides three types of positioning for data access routines:  
explicit offsets, individual file pointers, and shared file pointers.  
The different positioning methods may be mixed within the same program  
and do not affect each other.  
<P> 
The data access routines that accept explicit offsets  
contain  _AT  
in their name (e.g.,  MPI_FILE_WRITE_AT).  
Explicit offset operations perform data access at  
the file position given directly as an argument---no  
file pointer is used nor updated.  
Note that this is not equivalent to an atomic seek-and-read  
or seek-and-write operation,  
  
as no ``seek'' is issued.  
Operations with explicit offsets are described in  
Section <a href="node276.htm#Node276">Data Access with Explicit Offsets 
</a>, page <a href="node276.htm#Node276">Data Access with Explicit Offsets 
</a>.  
<P> 
The names of the individual file pointer routines contain no  
positional qualifier (e.g.,  MPI_FILE_WRITE).  
  
Operations with individual file pointers are described in  
Section <a href="node277.htm#Node277">Data Access with Individual File Pointers 
</a>, page <a href="node277.htm#Node277">Data Access with Individual File Pointers 
</a>.  
The data access routines that use shared file pointers contain  
 _SHARED   
  
or  _ORDERED   
  
in their name (e.g.,  MPI_FILE_WRITE_SHARED).  
Operations with shared file pointers are described in  
Section <a href="node278.htm#Node278">Data Access with Shared File Pointers 
</a>, page <a href="node278.htm#Node278">Data Access with Shared File Pointers 
</a>.  
<P> 
The main semantic issues with  MPI-maintained file pointers  
are how and when they are updated by I/O operations.  
In general, each I/O operation leaves the file pointer pointing to the  
next data item after the last one that  
is accessed by the operation.  
In a nonblocking or split collective operation,  
the pointer is updated by the call that initiates the I/O,  
possibly before the access completes.  
  
<P> 
More formally,  
<P><IMG WIDTH=56 HEIGHT=10 SRC="img107.gif"><P>
  
where <I>count</I> is the number of <I>datatype</I> items to be accessed,  
  
<I>elements(X)</I> is the number of predefined datatypes in the typemap of <I>X</I>,  
and <I>it old_file_offset</I> is  
the value of the implicit offset before the call.  
  
The file position, <I>it new_file_offset</I>, is in terms  
of a count of etypes relative to the current view.  
  
<P> 
  
  
<P> 

<P>
<HR>
<A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node273"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node271.htm#Node273"> Synchronism</a>
<b>Previous: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<P>
<HR><H3><A NAME="Node273">254.2. Synchronism</a></H3>
<A HREF="node271.htm#Node272"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node274"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node271.htm#Node274"> Coordination</a>
<b>Previous: </b><A HREF="node271.htm#Node272"> Positioning</a>
<P>
  
 MPI supports blocking and nonblocking I/O routines.  
<P> 
A <em> blocking</em> I/O call will  
not return  
  
until the I/O request is completed.  
<P> 
A <em> nonblocking</em> I/O call initiates an I/O operation, but does not  
wait for it to complete.  Given suitable hardware, this allows the  
transfer of data out/in the user's buffer to proceed concurrently with  
computation.  A separate <em> request complete</em> call  
( MPI_WAIT,  MPI_TEST, or any of their variants) is  
needed to complete the I/O request,  
i.e., to confirm that the data has been read or written and that  
  
it is safe for the user to reuse the buffer.  
The nonblocking versions of the routines are named  
 MPI_FILE_IXXX, where the  I stands for immediate.  
<P> 
  
It is erroneous to access the local buffer of a nonblocking  
data access operation, or to use that buffer as the source or  
target of other communications, between the initiation and  
completion of the operation.   
  
<P> 
The split collective routines  
support a restricted form of ``nonblocking'' operations  
for collective data access  
(see Section <a href="node282.htm#Node282">Split Collective Data Access Routines 
</a>,  
page <a href="node282.htm#Node282">Split Collective Data Access Routines 
</a>).  
  
<P> 

<P>
<HR>
<A HREF="node271.htm#Node272"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node274"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node271.htm#Node274"> Coordination</a>
<b>Previous: </b><A HREF="node271.htm#Node272"> Positioning</a>
<P>
<HR><H3><A NAME="Node274">254.3. Coordination</a></H3>
<A HREF="node271.htm#Node273"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node275"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node271.htm#Node275"> Data Access Conventions</a>
<b>Previous: </b><A HREF="node271.htm#Node273"> Synchronism</a>
<P>
  
Every noncollective data access routine  MPI_FILE_XXX  
  
has a collective counterpart.  For most routines, this counterpart  
is  MPI_FILE_XXX_ALL or a pair of  MPI_FILE_XXX_BEGIN and   
 MPI_FILE_XXX_END.  
The counterparts to the  MPI_FILE_XXX_SHARED routines are   
 MPI_FILE_XXX_ORDERED.  
  
<P> 
The completion of a noncollective call only depends on the activity of  
the calling process.  
However, the completion of a collective call  
(which must be called by all members of the process group)  
may depend on the activity  
of the other processes participating in the collective call.  
See Section <a href="node294.htm#Node294">Collective File Operations 
</a>,  
page <a href="node294.htm#Node294">Collective File Operations 
</a>,  
for rules on semantics of collective calls.  
  
<P> 
Collective operations  
may perform much better than their noncollective counterparts,  
as global data accesses have significant potential for automatic optimization.  
<P> 

<P>
<HR>
<A HREF="node271.htm#Node273"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node271.htm#Node275"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node271.htm#Node275"> Data Access Conventions</a>
<b>Previous: </b><A HREF="node271.htm#Node273"> Synchronism</a>
<P>
<HR><H3><A NAME="Node275">254.4. Data Access Conventions</a></H3>
<A HREF="node271.htm#Node274"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node276.htm#Node276"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node276.htm#Node276"> Data Access with Explicit Offsets</a>
<b>Previous: </b><A HREF="node271.htm#Node274"> Coordination</a>
<P>
  
<P> 
Data is moved between files and processes  
by calling read and write routines.  
Read routines move data from a file into memory.  
Write routines move data from memory into a file.  
The file is designated by a file handle,  fh.  
The location of the file data is specified by an offset  
into the current view.  
The data in memory is specified by a triple:  
 buf,  count, and  datatype.  
Upon completion, the amount of data accessed  
by the calling process is returned in a  status.  
  
<P> 
An offset designates the starting position in the file for an access.  
The offset is always in etype units relative to the current view.  
Explicit offset routines pass  offset as an argument  
(negative values are erroneous).  
The file pointer routines use implicit offsets maintained by  MPI.  
<P> 
A data access routine attempts to transfer (read or write)  
 count data items  
of type  datatype between the user's buffer  buf  
and the file.  
  
The  datatype passed to the routine must be   
a committed datatype.  
  
The layout of data in memory corresponding to  
 buf,  count,  datatype is  
interpreted the same way as in   
 MPI   
communication functions;  
see Section <a href="node42.htm#Node42">Message Data 
</a> on page <a href="node42.htm#Node42">Message Data 
</a>   
and Section <a href="node78.htm#Node78">Use of General Datatypes in Communication 
</a> on page <a href="node78.htm#Node78">Use of General Datatypes in Communication 
</a>.   
  
The data is accessed  
from those parts of the file specified by the current view  
(Section <a href="node269.htm#Node269">File Views 
</a>, page <a href="node269.htm#Node269">File Views 
</a>).  
The type signature of  datatype must match  
the type signature of some number of contiguous copies of the  etype  
of the current view.  
As in a receive,  
it is erroneous to specify a  datatype  
for reading that contains overlapping regions  
(areas of memory which would be stored into more than once).  
<P> 
The nonblocking data access routines  
indicate that  MPI can start a data access  
and associate a request handle,  request,  
with the I/O   
  
operation.  
  
Nonblocking operations are completed via  
 MPI_TEST,  MPI_WAIT, or any of their variants.  
<P> 
Data access operations, when completed,  
return the amount of data accessed in  status.  
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
To prevent problems with the argument copying and register  
optimization done by Fortran compilers, please note the hints  
in subsections ``Problems Due to Data Copying and Sequence Association,''  
and ``A Problem with Register Optimization'' in  
Section <a href="node337.htm#Node342">A Problem with Register Optimization 
</a>,  
pages <a href="node337.htm#Node339">Problems Due to Data Copying and Sequence Association 
</a> and <a href="node337.htm#Node342">A Problem with Register Optimization 
</a>.  
 (<em> End of advice to users.</em>) <BR> 
  
<P> 
For blocking routines,  status is returned directly.  
For nonblocking routines and split collective routines,  
 status is returned when the operation is completed.  
The number of  datatype entries and predefined elements accessed  
by the calling process  
can be extracted from  status by using  
 MPI_GET_COUNT and  MPI_GET_ELEMENTS, respectively.  
The interpretation of the <tt> MPI_ERROR</tt> field is the same as for other  
operations --- normally undefined, but meaningful if an  MPI routine returns  MPI_ERR_IN_STATUS.  
The user can pass (in C and Fortran)  MPI_STATUS_IGNORE  
in the  status argument  
if the return value of this argument is not needed.  
In C++, the  status argument is optional.  
  
The  status can be passed to  MPI_TEST_CANCELLED  
to determine if the operation was cancelled.  
All other fields of  status are undefined.  
<P> 
When reading, a program can detect the end of file  
by noting that the amount of data read is less than the amount requested.  
Writing past the end of file increases the file size.  
The amount of data accessed will be the amount requested,  
unless an error is raised (or a read reaches the end of file).  
  
<P> 

<P>
<HR>
<A HREF="node271.htm#Node274"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node271.htm#Node271"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node276.htm#Node276"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node271.htm#Node271"> Data Access Routines</a>
<b>Next: </b><A HREF="node276.htm#Node276"> Data Access with Explicit Offsets</a>
<b>Previous: </b><A HREF="node271.htm#Node274"> Coordination</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
