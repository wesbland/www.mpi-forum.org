<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Scatter</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node95">94. Scatter</a></H1>
<A HREF="node94.htm#Node94"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node96.htm#Node96"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node96.htm#Node96"> Examples using  MPI_SCATTER,  MPI_SCATTERV</a>
<b>Previous: </b><A HREF="node94.htm#Node94"> Examples using  MPI_GATHER,  MPI_GATHERV</a>
<P>
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_SCATTER( sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, root, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> address of send buffer (choice, significant  
only at root)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements sent to each process (non-negative  
integer, significant only at root)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements  
(significant only at root) (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements in receive buffer (non-negative  
integer)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  root</TD><TD>  rank of sending process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Scatter(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_SCATTER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR <BR></tt>  
  
  
 <tt> void MPI::Comm::Scatter(const void* sendbuf, int sendcount,  const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount,  const MPI::Datatype&amp; recvtype, int root) const = 0 <BR></tt>  
  
  
<P> 
  MPI_SCATTER is the inverse operation to  MPI_GATHER.  
<P> 
If  comm is an intracommunicator,   
the outcome is <em> as if</em> the root executed <tt> n</tt> send operations,  
<p><I> 
MPI_Send(sendbuf+i&#183; sendcount&#183; extent(sendtype), sendcount, 
sendtype, i,...), 
</I><p>  
and each process executed a receive,  
<p><I> 
MPI_Recv(recvbuf, recvcount, recvtype, i,...). 
</I><p>  
<P> 
An alternative description is that the root sends a message with  
 MPI_Send(sendbuf, sendcount<I>&#183;</I>n, sendtype, ...). This  
message is split into <tt> n</tt> equal segments, the   
<I>i</I>-th   
segment is  
sent to the   
<I>i</I>-th   
process in the group, and each process receives  
this message as above.  
<P> 
The send buffer is ignored for all non-root processes.  
<P> 
The type signature associated with  sendcount, sendtype at the root  
must be equal to the type signature associated with  
 recvcount, recvtype at all  
processes (however, the type maps may be different).  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between each process and the root.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
All arguments to the function are significant on process  root,  
while on other processes, only arguments  recvbuf, recvcount,  
recvtype, root,  
and  
 comm are significant.  
The arguments  root and  comm  
must have identical values on all processes.  
<P> 
The specification of counts and types  
should not cause any location on the root to be read more than  
once.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
Though not needed, the last restriction is imposed so as  
to achieve symmetry with  
 MPI_GATHER, where the corresponding restriction (a multiple-write  
restriction) is necessary.  
 (<em> End of rationale.</em>) <BR> 
  
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE as   
the value of  recvbuf at the root.  In such case,  
 recvcount and  recvtype are ignored, and root  
``sends'' no data to itself. The scattered vector is still assumed to  
contain <I>n</I> segments, where <I>n</I> is the group size; the <em> root</em>-th  
segment, which root should ``send to itself,'' is not moved.  
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Data is scattered from the root to all processes in  
group B.  The receive buffer arguments of the processes in group B  
must be consistent with the send buffer argument of the root.  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_SCATTERV( sendbuf, sendcounts, displs, sendtype,  
recvbuf, recvcount, recvtype, root, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> address of send buffer (choice, significant  
only at root)</TD></TR>  
<TR><TD> IN  sendcounts</TD><TD>non-negative  
integer array (of length group size)  
specifying the number of elements to send to each processor </TD></TR>  
<TR><TD> IN  displs</TD><TD> integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement (relative to  sendbuf from  
which to take the outgoing data to process <tt> i</tt></TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements in receive buffer (non-negative  
integer)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  root</TD><TD>  rank of sending process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Scatterv(void* sendbuf, int *sendcounts, int *displs, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_SCATTERV(SENDBUF, SENDCOUNTS, DISPLS, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNTS(*), DISPLS(*), SENDTYPE, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR <BR></tt>  
  
  
 <tt> void MPI::Comm::Scatterv(const void* sendbuf, const int sendcounts[], const int displs[], const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount, const MPI::Datatype&amp; recvtype, int root) const = 0 <BR></tt>  
  
  
<P> 
  MPI_SCATTERV is the inverse operation to  MPI_GATHERV.  
<P> 
 MPI_SCATTERV extends the functionality of  MPI_SCATTER  
by allowing a varying count of data to be sent to each process,  
since  sendcounts is now an array.  
It also allows more flexibility as to where the data  
is taken from on the root, by providing   
an additional   
argument,  displs.  
<P> 
If  comm is an intracommunicator,   
the outcome is as if the root executed <tt> n</tt> send operations,  
<p><I> 
MPI_Send(sendbuf+displs[i]&#183; extent(sendtype), sendcounts[i], 
sendtype, i,...), 
</I><p>  
and each process executed a receive,  
<p><I> 
MPI_Recv(recvbuf, recvcount, recvtype, i,...). 
</I><p>  
<P> 
The send buffer is ignored for all non-root processes.  
<P> 
The type signature implied by  sendcount[i], sendtype at the root  
must be equal to the type signature implied by  
 recvcount, recvtype at process  
<tt> i</tt> (however, the type maps may be different).  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between each process and the root.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
All arguments to the function are significant on process  root,  
while on other processes, only arguments  recvbuf, recvcount,  
recvtype, root,  
and  
 comm are significant.  
The arguments  root and  comm  
must have identical values on all processes.  
<P> 
The specification of counts, types, and displacements  
should not cause any location on the root to be read more than  
once.  
  
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE as   
the value of  recvbuf at the root.  In such case,  
 recvcount and  recvtype are ignored, and root  
``sends'' no data to itself. The scattered vector is still assumed to  
contain <I>n</I> segments, where <I>n</I> is the group size; the <em> root</em>-th  
segment, which root should ``send to itself,'' is not moved.  
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Data is scattered from the root to all processes in  
group B.  The receive buffer arguments of the processes in group B  
must be consistent with the send buffer argument of the root.  
  
<menu> 
</menu> 

<P>
<HR>
<A HREF="node94.htm#Node94"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node96.htm#Node96"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node96.htm#Node96"> Examples using  MPI_SCATTER,  MPI_SCATTERV</a>
<b>Previous: </b><A HREF="node94.htm#Node94"> Examples using  MPI_GATHER,  MPI_GATHERV</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
