<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-one-side/one-side-2.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>General Active Target Synchronization</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node235">219. General Active Target Synchronization</a></H2>
<A HREF="node234.htm#Node234"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node233.htm#Node233"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node236.htm#Node236"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node233.htm#Node233"> Synchronization Calls</a>
<b>Next: </b><A HREF="node236.htm#Node236"> Lock</a>
<b>Previous: </b><A HREF="node234.htm#Node234"> Fence</a>
<P>
<TABLE><TR><TD COLSPAN=2>MPI_WIN_START(group, assert, win)</TD></TR>  
<TR><TD> IN group</TD><TD>group of target processes (handle)</TD></TR>  
<TR><TD> IN assert</TD><TD>program assertion (integer)</TD></TR>  
<TR><TD> IN win</TD><TD>window object (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Win_start(MPI_Group group, int assert, MPI_Win win) <BR></tt>  
<P> 
 <tt> MPI_WIN_START(GROUP, ASSERT, WIN, IERROR)<BR> INTEGER GROUP, ASSERT, WIN, IERROR <BR></tt>  
<P> 
 <tt> void MPI::Win::Start(const MPI::Group&amp; group, int assert) const <BR></tt>  
<P> 
Starts an  RMA access epoch for  win.   RMA calls issued on  
 win during this epoch must access only windows at  
processes in  group.  
Each process in  group must issue a matching call to   
 MPI_WIN_POST.  
 RMA accesses to each target  
window will be delayed, if necessary,  
until the target process executed the matching call to   
 MPI_WIN_POST.  
  
 MPI_WIN_START is allowed to block until the corresponding  
 MPI_WIN_POST calls are executed, but is not required to.  
  
<P> 
The  assert argument is used to provide assertions on the  
context of the call that may be used for various optimizations.  This  
is described in  
Section <a href="node237.htm#Node237">Assertions 
</a>.  A value of  assert = 0 is  
always valid.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_WIN_COMPLETE(win)</TD></TR>  
<TR><TD> IN win</TD><TD>window object (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Win_complete(MPI_Win win) <BR></tt>  
<P> 
 <tt> MPI_WIN_COMPLETE(WIN, IERROR)<BR> INTEGER WIN,  IERROR <BR></tt>  
<P> 
  
 <tt> void MPI::Win::Complete() const <BR></tt>  
  
<P> 
Completes an  RMA access epoch on  win started by a  
call to  MPI_WIN_START.  All  RMA communication  
calls issued on  win during this epoch will have completed at  
the origin when the call returns.  
<P> 
 MPI_WIN_COMPLETE enforces completion of preceding  RMA  
calls at the origin, but not at the target.  
A put or accumulate call may not have completed  
at the target when it has completed at the origin.  
<P> 
Consider the sequence of calls in the example below.  
<BR><b> Example</b>  
  
  
  
  
<P> 
   
<BR> 
<pre><tt>MPI_Win_start(group, flag, win); 
MPI_Put(...,win); 
MPI_Win_complete(win); 
</tt></pre> 
  
<P> 
The call to  MPI_WIN_COMPLETE does not return until the  
put call has completed at the origin; and  
the target window will be accessed by the put operation only after the  
call to  MPI_WIN_START has matched a call to  
 MPI_WIN_POST by the target process.  This still leaves  
much choice to implementors. The call to  
 MPI_WIN_START can block until the matching call to  
 MPI_WIN_POST occurs at all target processes.  One can also  
have implementations where the call to  MPI_WIN_START is  
nonblocking, but the call to  MPI_PUT blocks until the  
matching call to  MPI_WIN_POST occurred;  or  
implementations where the first two calls are nonblocking, but the  
call to  MPI_WIN_COMPLETE blocks until the call to  
 MPI_WIN_POST occurred; or even implementations where all  
three calls can complete before any target process called   
 MPI_WIN_POST ---   
the data put must be buffered, in this last case, so as to allow the  
put to complete at the origin ahead of its completion at the   
target.  
However, once the  
call to  MPI_WIN_POST is issued, the sequence above  
must complete, without further dependencies.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_WIN_POST(group, assert, win)</TD></TR>  
<TR><TD> IN group</TD><TD>group of origin processes (handle)</TD></TR>  
<TR><TD> IN assert</TD><TD>program assertion (integer)</TD></TR>  
<TR><TD> IN win</TD><TD>window object (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Win_post(MPI_Group group, int assert, MPI_Win win) <BR></tt>  
<P> 
 <tt> MPI_WIN_POST(GROUP, ASSERT, WIN, IERROR)<BR> INTEGER GROUP, ASSERT, WIN, IERROR <BR></tt>  
<P> 
 <tt> void MPI::Win::Post(const MPI::Group&amp; group, int assert) const <BR></tt>  
<P> 
Starts an  RMA exposure epoch for the  
local window associated with  win.  Only processes in  
 group should access the window with  RMA calls on  
 win during  this   
epoch.  
Each process in  group must issue a matching call to   
 MPI_WIN_START.  
  
 MPI_WIN_POST does not block.  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_WIN_WAIT(win)</TD></TR>  
<TR><TD> IN win</TD><TD>window object (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Win_wait(MPI_Win win) <BR></tt>  
<P> 
 <tt> MPI_WIN_WAIT(WIN, IERROR)<BR> INTEGER WIN,  IERROR <BR></tt>  
<P> 
  
 <tt> void MPI::Win::Wait() const <BR></tt>  
  
<P> 
Completes an  RMA exposure epoch started by a call to  
 MPI_WIN_POST on   
 win.  
This call matches calls to  MPI_WIN_COMPLETE(win)  
issued  
by each of the origin processes that were granted access to the window  
during this epoch.  
The call to  MPI_WIN_WAIT will  
block until all matching calls to  MPI_WIN_COMPLETE  
have occurred.  
This guarantees that all these origin processes have completed their  
 RMA accesses to the local  
window.  
When the call returns, all these  RMA accesses will  
have completed at the target window.  
<P> 
Figure <a href="node235.htm#Figure20">20 
</a> illustrates the use of these four  
functions.  
<CENTER><P><IMG WIDTH=539 HEIGHT=281 SRC="2party.gif"><P>
</CENTER>  
<BR> 
<b>Figure 20: </b><A NAME="Figure20">Active target communication.  Dashed arrows represent
synchronizations and solid arrows represent data transfer.</a><P> 
  
  
Process 0 puts data in the windows of processes 1 and 2 and process 3  
puts data in the window of process 2.  Each start call lists the ranks  
of the processes whose windows will be accessed;  
each post call lists the ranks of the  
processes that access the local window.  The figure illustrates a  
possible timing for the events, assuming strong synchronization; in a   
weak synchronization, the start, put or complete calls  
may occur ahead of the matching post calls.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_WIN_TEST(win, flag)</TD></TR>  
<TR><TD> IN win</TD><TD>window object (handle)</TD></TR>  
<TR><TD> OUT flag</TD><TD>success flag (logical)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Win_test(MPI_Win win, int *flag) <BR></tt>  
<P> 
 <tt> MPI_WIN_TEST(WIN, FLAG, IERROR)<BR> INTEGER WIN, IERROR<BR>LOGICAL FLAG <BR></tt>  
<P> 
  
 <tt> bool MPI::Win::Test() const <BR></tt>  
  
<P> 
This is the nonblocking version of  MPI_WIN_WAIT.  It  
returns  flag = true if  MPI_WIN_WAIT would return,  
 flag = false,   
otherwise.  The effect of return of  MPI_WIN_TEST with  
 flag = true is the same as the effect of a return   
of  MPI_WIN_WAIT.  If  flag = false is returned,  
then the call has no visible effect.  
<P> 
 MPI_WIN_TEST should be invoked only where  
 MPI_WIN_WAIT  
can be invoked.  
Once the call has returned  flag = true, it must not be  
invoked anew, until the window is posted anew.  
<P> 
  
Assume that window  win is associated with a ``hidden''  
communicator  wincomm, used for communication by the processes  
of  win.  The rules for matching of post and start calls and  
for matching complete and wait call can be derived from the rules for  
matching sends and receives, by considering the following (partial) model  
implementation.   
<P> 
<dl> 
 
<dt> 
<b>{</b> MPI_WIN_POST(group,0,win)}</b><dd> 
initiate a nonblocking send with tag  tag0 to each  
process in  group, using  wincomm.  No need to wait for the  
completion of these sends.  
 
<dt> 
<b>{</b> MPI_WIN_START(group,0,win)}</b><dd> 
initiate a nonblocking receive with tag  tag0 from each  
process in  group, using  wincomm.  An  RMA access to  
a window in target process  i is delayed until the receive  
from  i is completed.  
 
<dt> 
<b>{</b> MPI_WIN_COMPLETE(win)}</b><dd> 
initiate a nonblocking send with tag  tag1 to each process in  
the group of the preceding start call.  No need to wait for the  
completion of these sends.  
 
<dt> 
<b>{</b> MPI_WIN_WAIT(win)}</b><dd> 
initiate a nonblocking receive with tag  tag1 from each  
process in the group of the preceding post call.  Wait for the  
completion of all receives.   
</dl> 
<BR> 
No races can occur in a correct program: each of the sends matches a  
unique receive, and vice-versa.   
  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
The design for general active target synchronization requires the  
user to provide complete information on the communication pattern, at  
each end of a communication link: each origin specifies a list of  
targets, and each target specifies a list of origins.  This provides  
maximum flexibility (hence, efficiency) for the implementor:   
each  
synchronization can be initiated by either side, since each ``knows''  
the identity of the other.  This also provides maximum protection from  
possible races.  On the other hand, the design requires more  
information than  RMA needs, in general: in general, it is sufficient  
for the origin to know the rank of the target, but not vice  
versa.  
Users that want more ``anonymous'' communication will be required to  
use the fence or lock mechanisms.  
 (<em> End of rationale.</em>) <BR> 
  
 
<BR> 
<em> Advice to users.</em>  
<P> 
Assume a communication pattern that is represented by a directed graph  
<IMG WIDTH=124 HEIGHT=12 SRC="img100.gif">
, where <I>V = {0, ..., n-1}</I> and <IMG WIDTH=119 HEIGHT=12 SRC="img101.gif">
 if origin process  
<I>i</I> accesses the window at target process <I>j</I>.  Then each process <I>i</I>  
issues a call to   
 MPI_WIN_POST(<I>ingroup<SUB>i</SUB></I>, ...),   
followed by a call to  
 MPI_WIN_START(<I>outgroup<SUB>i</SUB></I>,...),  
where   
<IMG WIDTH=1 HEIGHT=11 SRC="img102.gif">
 and <IMG WIDTH=426 HEIGHT=106 SRC="img103.gif">
.  A call is a noop, and can be skipped, if the group argument is  
empty. After the communications calls, each process that issued  
a start will issue a complete.  Finally, each process that  
issued  a post will issue a wait.   
<P> 
Note that each process may call with a group argument that has  
different members.  
 (<em> End of advice to users.</em>) <BR> 
  
  <P> 

<P>
<HR>
<A HREF="node234.htm#Node234"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node233.htm#Node233"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node236.htm#Node236"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node233.htm#Node233"> Synchronization Calls</a>
<b>Next: </b><A HREF="node236.htm#Node236"> Lock</a>
<b>Previous: </b><A HREF="node234.htm#Node234"> Fence</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
