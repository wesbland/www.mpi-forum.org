<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>All-Reduce</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node107">105. All-Reduce</a></H2>
<A HREF="node105.htm#Node106"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node108.htm#Node108"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node108.htm#Node108"> Reduce-Scatter</a>
<b>Previous: </b><A HREF="node105.htm#Node106"> Example of User-defined Reduce</a>
<P>
  
<P> 
 MPI includes   
a variant  
of the reduce operations  
where the result is returned to all processes in  
a  
group.  
 MPI requires that all processes  
from the same group  
participating in these operations  
receive identical results.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLREDUCE( sendbuf, recvbuf, count, datatype, op, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD> starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD> number of elements in send buffer (non-negative  
integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD> data type of elements of send buffer (handle)</TD></TR>  
<TR><TD> IN op</TD><TD> operation (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Allreduce(void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_ALLREDUCE(SENDBUF, RECVBUF, COUNT, DATATYPE, OP, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER COUNT, DATATYPE, OP, COMM, IERROR <BR></tt>  
  
  
 <tt> void MPI::Comm::Allreduce(const void* sendbuf, void* recvbuf,  int count, const MPI::Datatype&amp; datatype, const MPI::Op&amp; op) const = 0 <BR></tt>  
  
  
If  comm is an intracommunicator,  
 MPI_ALLREDUCE behaves the  
same as  MPI_REDUCE except that the result  
appears in the receive buffer of all the group members.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
The all-reduce operations can be implemented as a reduce, followed by a  
broadcast.  However, a direct implementation can lead to better performance.  
 (<em> End of advice to implementors.</em>) <BR> 
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing the  
value  MPI_IN_PLACE to the argument  sendbuf  
at all processes.  
In this case,  
the input data is taken at each process from the receive buffer,  
where it will be replaced by the output data.  
<P> 
If  comm is an intercommunicator, then the result of the reduction  
of the data provided by processes in group A is stored at each process  
in group B, and vice versa.    
Both groups should  
provide  count and  datatype arguments that specify the same type  
signature.  
  
<P> 
  
The following example uses an intracommunicator.  
  
<BR><b> Example</b>   
  
  
<P> 
A routine that computes  
the product of a vector and an array that are distributed across a  
group of processes and returns the answer at all nodes (see also Example  
<a href="node102.htm#Node102">Predefined Reduction Operations 
</a>).  
<P> 
  
<BR> 
<pre><tt>SUBROUTINE PAR_BLAS2(m, n, a, b, c, comm) 
REAL a(m), b(m,n)    ! local slice of array 
REAL c(n)            ! result 
REAL sum(n) 
INTEGER n, comm, i, j, ierr 
 
! local sum 
DO j= 1, n 
  sum(j) = 0.0 
  DO i = 1, m 
    sum(j) = sum(j) + a(i)*b(i,j) 
  END DO 
END DO 
 
! global sum 
CALL MPI_ALLREDUCE(sum, c, n, MPI_REAL, MPI_SUM, comm, ierr) 
 
! return result at all nodes 
RETURN 
</tt></pre> 
  
  
  
<P> 

<P>
<HR>
<A HREF="node105.htm#Node106"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node100.htm#Node100"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node108.htm#Node108"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.htm#Node100"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node108.htm#Node108"> Reduce-Scatter</a>
<b>Previous: </b><A HREF="node105.htm#Node106"> Example of User-defined Reduce</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
