<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-pt2pt/pt2pt.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Buffer Allocation and Usage</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node53">52. Buffer Allocation and Usage</a></H1>
<A HREF="node52.htm#Node52"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node54.htm#Node54"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node54.htm#Node54"> Model Implementation of Buffered Mode</a>
<b>Previous: </b><A HREF="node52.htm#Node52"> Semantics of Point-to-Point Communication</a>
<P>
  
<P> 
A user may specify a buffer to be used for buffering messages sent in buffered  
mode.   Buffering is done by the sender.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_BUFFER_ATTACH(buffer, size)</TD></TR>  
<TR><TD> IN buffer</TD><TD>initial buffer address (choice)</TD></TR>  
<TR><TD> IN size</TD><TD>buffer size, in bytes (non-negative   
integer)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Buffer_attach(void* buffer, int size) <BR></tt>  
<P> 
 <tt> MPI_BUFFER_ATTACH(BUFFER, SIZE, IERROR)<BR> &lt;type&gt; BUFFER(*) <BR>INTEGER  SIZE, IERROR <BR></tt>  
 <tt> void MPI::Attach_buffer(void* buffer, int size) <BR></tt>  
  
Provides to  MPI a buffer in the user's memory to be used for buffering outgoing  
messages.  The buffer is used only by messages sent in buffered mode.  
Only one buffer can be attached to a process at a time.  
<P> 
  
<TABLE><TR><TD COLSPAN=2>MPI_BUFFER_DETACH(buffer_addr, size)</TD></TR>  
<TR><TD> OUT buffer_addr</TD><TD>initial buffer address (choice)</TD></TR>  
<TR><TD> OUT size</TD><TD>buffer size, in bytes (non-negative   
integer)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Buffer_detach(void* buffer_addr, int* size) <BR></tt>  
<P> 
 <tt> MPI_BUFFER_DETACH(BUFFER_ADDR, SIZE, IERROR)<BR> &lt;type&gt; BUFFER_ADDR(*) <BR>INTEGER  SIZE, IERROR <BR></tt>  
 <tt> int MPI::Detach_buffer(void*&amp; buffer) <BR></tt>  
  
Detach the buffer currently associated with  MPI.  The call returns the  
address and the size of the detached buffer.  This operation  
will block until all messages currently in the buffer have been transmitted.  
Upon return of  
this function, the user may reuse or deallocate the space taken by the buffer.  
<P> 
<BR><b> Example</b>  
  
  
  
Calls to attach and detach buffers.  
<BR> 
<pre><tt>#define BUFFSIZE 10000 
int size 
char *buff; 
MPI_Buffer_attach( malloc(BUFFSIZE), BUFFSIZE); 
/* a buffer of 10000 bytes can now be used by MPI_Bsend */ 
MPI_Buffer_detach( &amp;buff, &amp;size); 
/* Buffer size reduced to zero */ 
MPI_Buffer_attach( buff, size); 
/* Buffer of 10000 bytes available again */ 
</tt></pre> 
   
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
Even though the C functions  
 MPI_Buffer_attach and  MPI_Buffer_detach both have  
a first argument of type  void*, these arguments are used  
differently: A pointer to the buffer is passed to  MPI_Buffer_attach;  
the address of the pointer is passed to  MPI_Buffer_detach, so that  
this call can return the pointer value.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
<em> Rationale.</em>  
<P> 
Both arguments are defined to be of type  void* (rather than  
 void* and  void**, respectively), so as to avoid complex type  
casts. E.g., in the last example,  &amp;buff, which is of type  
 char**, can be passed as argument to  MPI_Buffer_detach  
without type casting.  If the formal parameter had type  void** then we  
would need a type cast before and after the call.  
 (<em> End of rationale.</em>) <BR> 
  
<P> 
The statements made in this section describe the behavior of  MPI  
for buffered-mode sends.  
When no buffer is currently associated,  MPI behaves as if a  
zero-sized buffer is associated with the process.  
<P> 
 MPI must provide as much buffering for outgoing messages <em> as if</em>  
outgoing message  
data were buffered by the sending process, in the specified buffer space,  
using a circular, contiguous-space allocation policy.  
We outline below a model implementation that defines this policy.  
 MPI may provide more buffering, and may use a better buffer allocation  
algorithm  
than described below.  On the other hand,  MPI may signal an error whenever the  
simple buffering allocator described below would run out of space.  In  
particular, if no buffer is explicitly associated with the process, then any  
buffered send may cause an error.  
<P> 
 MPI does not provide mechanisms for querying or controlling buffering done by  
standard mode sends.   It is expected that vendors will provide such  
information  
for their implementations.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
There is a wide spectrum of possible implementations of buffered communication:  
buffering can be done at sender, at receiver, or both; buffers can be dedicated  
to one sender-receiver pair, or be shared by  
all communications; buffering can be done in real or  
in virtual memory; it can use dedicated memory, or memory shared by other  
processes; buffer space may be allocated statically or be changed dynamically;  
etc.   It does not seem feasible to provide a portable mechanism for querying  
or controlling buffering that would be compatible with all these choices, yet  
provide meaningful information.  
 (<em> End of rationale.</em>) <BR> 
<menu> 
</menu> 

<P>
<HR>
<A HREF="node52.htm#Node52"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node54.htm#Node54"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node54.htm#Node54"> Model Implementation of Buffered Mode</a>
<b>Previous: </b><A HREF="node52.htm#Node52"> Semantics of Point-to-Point Communication</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
