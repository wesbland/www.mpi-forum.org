<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Reduce-Scatter</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node108">106. Reduce-Scatter</a></H1>
<A HREF="node107.htm#Node107"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node109.htm#Node109"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node109.htm#Node109"> Scan</a>
<b>Previous: </b><A HREF="node107.htm#Node107"> All-Reduce</a>
<P>
  
<P> 
 MPI includes   
a variant  
of the reduce operations  
where the result is scattered to all processes in  
a  
group on return.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_REDUCE_SCATTER( sendbuf, recvbuf, recvcounts,  
datatype, op, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD> starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcounts</TD><TD>non-negative  
integer array specifying the  
number of elements in result distributed to each process.  
Array must be identical on all calling processes.</TD></TR>  
<TR><TD> IN datatype</TD><TD> data type of elements of input buffer (handle)</TD></TR>  
<TR><TD> IN op</TD><TD> operation (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Reduce_scatter(void* sendbuf, void* recvbuf, int *recvcounts, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_REDUCE_SCATTER(SENDBUF, RECVBUF, RECVCOUNTS, DATATYPE, OP, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER RECVCOUNTS(*), DATATYPE, OP, COMM, IERROR <BR></tt>  
  
  
 <tt> void MPI::Comm::Reduce_scatter(const void* sendbuf,  void* recvbuf, int recvcounts[], const MPI::Datatype&amp; datatype,  const MPI::Op&amp; op) const = 0 <BR></tt>  
  
  
<P> 
If  comm is an intracommunicator,  
 MPI_REDUCE_SCATTER first does  
an element-wise reduction on  
vector of   
<IMG WIDTH=125 HEIGHT=29 SRC="img55.gif">
   
elements  
in the send buffer defined by  sendbuf, count and  
 datatype.  
Next, the resulting vector of results is split into <tt> n</tt> disjoint  
segments, where <tt> n</tt> is the number of members in the group.  
Segment <tt> i</tt> contains  recvcounts[i] elements.  
The   
<tt> i</tt>-th   
segment is sent to process <tt> i</tt> and stored in the  
receive buffer defined by  recvbuf, recvcounts[i] and  
 datatype.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
The  MPI_REDUCE_SCATTER  
routine is functionally equivalent to:  
an  
 MPI_REDUCE  
collective  
operation  
with  count equal to  
the sum of  recvcounts[i] followed by  
 MPI_SCATTERV with  sendcounts equal to  recvcounts.  
However, a direct implementation may run faster.  
 (<em> End of advice to implementors.</em>) <BR> 
  
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE in   
the  sendbuf argument.  
In this case, the input data is taken from the top of the receive   
buffer.  
  
If  comm is an intercommunicator, then the result of the reduction  
of the data provided by processes in group A is scattered among  
processes in group B, and vice versa.  Within each group, all  
processes provide the same  recvcounts argument, and the sum  
of the  recvcounts entries should be the same for the two groups.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
The last restriction is needed so that the length of the send  
buffer can be determined by the sum of the local  recvcounts entries.  
Otherwise, a communication is needed to figure out how many elements  
are reduced.  
 (<em> End of rationale.</em>) <BR> 
  

<P>
<HR>
<A HREF="node107.htm#Node107"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node109.htm#Node109"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node109.htm#Node109"> Scan</a>
<b>Previous: </b><A HREF="node107.htm#Node107"> All-Reduce</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
