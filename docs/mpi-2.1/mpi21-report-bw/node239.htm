<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-one-side/one-side-2.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Examples</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node239">223. Examples</a></H1>
<A HREF="node238.htm#Node238"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node240.htm#Node240"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node240.htm#Node240"> Error Handling</a>
<b>Previous: </b><A HREF="node238.htm#Node238"> Miscellaneous Clarifications</a>
<P>
<BR><b> Example</b>  
  
  
  
  
The following example shows a generic loosely synchronous, iterative  
code, using fence synchronization.  The window at each process  
consists of array <tt> A</tt>, which contains the origin and target buffers of  
the  
put calls.  
<P> 
<BR> 
<pre><tt>... 
while(!converged(A)){ 
  update(A); 
  MPI_Win_fence(MPI_MODE_NOPRECEDE, win); 
  for(i=0; i &lt; toneighbors; i++) 
    MPI_Put(&amp;frombuf[i], 1, fromtype[i], toneighbor[i], 
                         todisp[i], 1, totype[i], win); 
  MPI_Win_fence((MPI_MODE_NOSTORE | MPI_MODE_NOSUCCEED), win); 
  } 
</tt></pre> 
The same code could be written with get, rather than put.  Note that,  
during the communication phase, each  
window is concurrently read  (as origin buffer of puts) and written  
(as target buffer of puts).  This is OK, provided that there is no  
overlap between the target buffer of a put and another communication   
buffer.  
  
<P> 
<BR><b> Example</b>  
  
  
  
  
Same generic example, with more computation/communication overlap.  We  
assume that the update phase is broken in two subphases: the first,  
where the ``boundary,'' which is involved in communication, is updated, and  
the second, where the ``core,'' which neither use nor provide  
communicated data, is updated.  
<BR> 
<pre><tt>... 
while(!converged(A)){ 
  update_boundary(A); 
  MPI_Win_fence((MPI_MODE_NOPUT | MPI_MODE_NOPRECEDE), win); 
  for(i=0; i &lt; fromneighbors; i++) 
    MPI_Get(&amp;tobuf[i], 1, totype[i], fromneighbor[i], 
                    fromdisp[i], 1, fromtype[i], win); 
  update_core(A); 
  MPI_Win_fence(MPI_MODE_NOSUCCEED, win); 
  } 
</tt></pre> 
The get communication can be concurrent with the core update, since  
they do not access the same locations, and the local update of the  
origin buffer by the get call can be concurrent with the local update  
of the core by the <tt> update_core</tt> call.  In order to get similar  
overlap with put communication we would need to use separate windows  
for the core and for the boundary.  
  
This is required   
because we do not allow local stores to be concurrent with puts  
on the same, or on overlapping, windows.  
  
  
<P> 
<BR><b> Example</b>  
  
  
  
  
  
Same code as in Example <a href="node239.htm#Node239">Examples 
</a>,  
rewritten using post-start-complete-wait.  
<BR> 
<pre><tt>... 
while(!converged(A)){ 
  update(A); 
  MPI_Win_post(fromgroup, 0, win); 
  MPI_Win_start(togroup, 0, win); 
  for(i=0; i &lt; toneighbors; i++) 
    MPI_Put(&amp;frombuf[i], 1, fromtype[i], toneighbor[i], 
                         todisp[i], 1, totype[i], win); 
  MPI_Win_complete(win); 
  MPI_Win_wait(win); 
  } 
</tt></pre> 
  
<P> 
<BR><b> Example</b>  
  
  
  
  
  
Same example, with split phases, as in Example <a href="node239.htm#Node239">Examples 
</a>.  
<BR> 
<pre><tt>... 
while(!converged(A)){ 
  update_boundary(A); 
  MPI_Win_post(togroup, MPI_MODE_NOPUT, win); 
  MPI_Win_start(fromgroup, 0, win); 
  for(i=0; i &lt; fromneighbors; i++) 
    MPI_Get(&amp;tobuf[i], 1, totype[i], fromneighbor[i], 
                   fromdisp[i], 1, fromtype[i], win); 
  update_core(A); 
  MPI_Win_complete(win); 
  MPI_Win_wait(win); 
  } 
</tt></pre> 
  
<P> 
<BR><b> Example</b>  
  
  
  
  
  
  
A checkerboard, or double buffer  communication pattern, that allows  
more computation/communication overlap.  Array <tt> A0</tt> is updated  
using values of array <tt> A1</tt>, and vice versa.  We assume that communication is symmetric: if process A gets data from process B, then process B gets data from process A.  Window <tt> wini</tt> consists of array <tt> Ai</tt>.  
<BR> 
<pre><tt>... 
if (!converged(A0,A1)) 
  MPI_Win_post(neighbors, (MPI_MODE_NOCHECK | MPI_MODE_NOPUT), win0); 
MPI_Barrier(comm0); 
/* the barrier is needed because the start call inside the 
loop uses the nocheck option */ 
while(!converged(A0, A1)){ 
  /* communication on A0 and computation on A1 */ 
  update2(A1, A0); /* local update of A1 that depends on A0 (and A1) */ 
  MPI_Win_start(neighbors, MPI_MODE_NOCHECK, win0); 
  for(i=0; i &lt; neighbors; i++) 
    MPI_Get(&amp;tobuf0[i], 1, totype0[i], neighbor[i], 
               fromdisp0[i], 1, fromtype0[i], win0); 
  update1(A1); /* local update of A1 that is 
                  concurrent with communication that updates A0 */  
  MPI_Win_post(neighbors, (MPI_MODE_NOCHECK | MPI_MODE_NOPUT), win1); 
  MPI_Win_complete(win0); 
  MPI_Win_wait(win0); 
 
  /* communication on A1 and computation on A0 */ 
  update2(A0, A1); /* local update of A0 that depends on A1 (and A0)*/ 
  MPI_Win_start(neighbors, MPI_MODE_NOCHECK, win1); 
  for(i=0; i &lt; neighbors; i++) 
    MPI_Get(&amp;tobuf1[i], 1, totype1[i], neighbor[i], 
                fromdisp1[i], 1, fromtype1[i], win1); 
  update1(A0); /* local update of A0 that depends on A0 only, 
                 concurrent with communication that updates A1 */ 
  if (!converged(A0,A1)) 
    MPI_Win_post(neighbors, (MPI_MODE_NOCHECK | MPI_MODE_NOPUT), win0); 
  MPI_Win_complete(win1); 
  MPI_Win_wait(win1); 
  } 
</tt></pre> 
A process posts the local window associated with  
<tt> win0</tt> before it completes  RMA accesses to  
the remote windows associated with <tt> win1</tt>.  
When the <tt> wait(win1</tt>) call  
returns, then all neighbors of the calling process have posted the  
windows associated with <tt> win0</tt>. Conversely, when the <tt>  
wait(win0)</tt> call returns, then all neighbors of the calling process  
have posted the windows associated with <tt> win1</tt>.  
Therefore, the nocheck option can be used with the calls to  
 MPI_WIN_START.  
<P> 
Put calls can be used, instead of get calls, if the area of array  
<tt> A0</tt> (resp. <tt> A1</tt>) used by the <tt> update(A1, A0)</tt>  
(resp. <tt> update(A0, A1)</tt>) call is disjoint from the area  
modified by the  RMA communication.  On some systems, a put call may be  
more efficient than a get call, as it requires information exchange  
only in one direction.  
<P> 
  
<P> 

<P>
<HR>
<A HREF="node238.htm#Node238"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node240.htm#Node240"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node240.htm#Node240"> Error Handling</a>
<b>Previous: </b><A HREF="node238.htm#Node238"> Miscellaneous Clarifications</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
