<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-context/context.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Library Example #2</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node137">135. Library Example #2</a></H2>
<A HREF="node136.htm#Node136"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node131.htm#Node131"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node138.htm#Node138"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node131.htm#Node131"> Motivating Examples</a>
<b>Next: </b><A HREF="node138.htm#Node138"> Inter-Communication</a>
<b>Previous: </b><A HREF="node136.htm#Node136"> Library Example #1</a>
<P>
  
The main program:  
  
<BR> 
<pre><tt>   main(int argc, char **argv) 
   { 
     int ma, mb; 
     MPI_Group MPI_GROUP_WORLD, group_a, group_b; 
     MPI_Comm comm_a, comm_b; 
 
     static int list_a[] = {0, 1}; 
#if  defined(EXAMPLE_2B) | defined(EXAMPLE_2C) 
     static int list_b[] = {0, 2 ,3}; 
#else/* EXAMPLE_2A */ 
     static int list_b[] = {0, 2}; 
#endif 
     int size_list_a = sizeof(list_a)/sizeof(int); 
     int size_list_b = sizeof(list_b)/sizeof(int); 
 
     ... 
     MPI_Init(&amp;argc, &amp;argv); 
     MPI_Comm_group(MPI_COMM_WORLD, &amp;MPI_GROUP_WORLD); 
 
     MPI_Group_incl(MPI_GROUP_WORLD, size_list_a, list_a, &amp;group_a); 
     MPI_Group_incl(MPI_GROUP_WORLD, size_list_b, list_b, &amp;group_b); 
 
     MPI_Comm_create(MPI_COMM_WORLD, group_a, &amp;comm_a); 
     MPI_Comm_create(MPI_COMM_WORLD, group_b, &amp;comm_b); 
 
     if(comm_a != MPI_COMM_NULL) 
        MPI_Comm_rank(comm_a, &amp;ma); 
     if(comm_b != MPI_COMM_NULL) 
        MPI_Comm_rank(comm_b, &amp;mb); 
 
     if(comm_a != MPI_COMM_NULL) 
        lib_call(comm_a); 
 
     if(comm_b != MPI_COMM_NULL) 
     { 
       lib_call(comm_b); 
       lib_call(comm_b); 
     } 
 
     if(comm_a != MPI_COMM_NULL) 
       MPI_Comm_free(&amp;comm_a); 
     if(comm_b != MPI_COMM_NULL) 
       MPI_Comm_free(&amp;comm_b); 
     MPI_Group_free(&amp;group_a); 
     MPI_Group_free(&amp;group_b); 
     MPI_Group_free(&amp;MPI_GROUP_WORLD); 
     MPI_Finalize(); 
   } 
</tt></pre> 
 The library:  
<BR> 
<pre><tt>   void lib_call(MPI_Comm comm) 
   { 
     int me, done = 0; 
     MPI_Status status;  
     MPI_Comm_rank(comm, &amp;me); 
     if(me == 0) 
        while(!done) 
        { 
           MPI_Recv(..., MPI_ANY_SOURCE, MPI_ANY_TAG, comm, &amp;status); 
           ... 
        } 
     else 
     { 
       /* work */ 
       MPI_Send(..., 0, ARBITRARY_TAG, comm); 
       .... 
     } 
#ifdef EXAMPLE_2C 
     /* include (resp, exclude) for safety (resp, no safety): */ 
     MPI_Barrier(comm); 
#endif 
   } 
</tt></pre> 
The above example is really three examples, depending on whether or  
not one includes rank 3 in  list_b, and whether or not a  
synchronize is included in  lib_call.  This example illustrates  
that, despite contexts, subsequent calls to  lib_call with the  
same context need not be safe from one another (colloquially,  
``back-masking'').  Safety is realized if the  MPI_Barrier is  
added.  What this demonstrates is that libraries have to be written  
carefully, even with contexts.  When rank 3 is excluded, then  
the synchronize is not needed to get safety from back masking.  
<P> 
Algorithms like ``reduce'' and ``allreduce'' have strong enough source  
selectivity properties so that they are inherently okay (no backmasking),  
provided that  MPI provides basic guarantees.  So are multiple calls to a  
typical tree-broadcast algorithm with the same root or different roots  
(see [<a href="node427.htm#-Bib45">45</a>]).    
Here we rely on two guarantees of  MPI: pairwise ordering of  
messages between processes in the same context, and source selectivity ---  
deleting either feature removes the guarantee that backmasking cannot  
be required.  
<P> 
Algorithms that try to do non-deterministic broadcasts or other calls that  
include wildcard operations will not generally have the good properties of the  
deterministic implementations of ``reduce,'' ``allreduce,'' and ``broadcast.''  
Such algorithms would have to utilize the monotonically increasing tags  
(within a communicator scope) to keep things straight.  
<P> 
All of the foregoing is a supposition of ``collective calls'' implemented with  
point-to-point operations.   MPI implementations may or may not implement  
collective calls using point-to-point operations.  These algorithms are used  
to illustrate the issues of correctness and safety, independent of how  MPI  
implements its collective calls.  See also Section <a href="node153.htm#Node153">Formalizing the Loosely Synchronous Model 
</a>.  
<P> 

<P>
<HR>
<A HREF="node136.htm#Node136"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node131.htm#Node131"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node138.htm#Node138"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node131.htm#Node131"> Motivating Examples</a>
<b>Next: </b><A HREF="node138.htm#Node138"> Inter-Communication</a>
<b>Previous: </b><A HREF="node136.htm#Node136"> Library Example #1</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
