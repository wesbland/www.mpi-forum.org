<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-io/io-2.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>File Consistency</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node291">265. File Consistency</a></H2>
<A HREF="node290.htm#Node290"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node290.htm#Node290"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node292.htm#Node292"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node290.htm#Node290"> Consistency and Semantics</a>
<b>Next: </b><A HREF="node292.htm#Node292"> Random Access vs. Sequential Files</a>
<b>Previous: </b><A HREF="node290.htm#Node290"> Consistency and Semantics</a>
<P>
  
Consistency semantics define the outcome of multiple accesses  
to a single file.  
All file accesses in  MPI are relative to a specific file handle  
created from a collective open.  
 MPI provides three levels of consistency:  
sequential consistency among all accesses using a single file handle,  
sequential consistency among all accesses  
using file handles created from a single collective open  
with atomic mode enabled,  
and  
  
user-imposed consistency among accesses other than the above.  
  
Sequential consistency means the behavior of a set of operations  
will be as if the operations were performed in some serial order  
consistent with program order; each access appears atomic,  
although the exact ordering of accesses is unspecified.  
  
User-imposed consistency may be obtained using program order  
and calls to  MPI_FILE_SYNC.  
  
<P> 
Let <I>FH<SUB>1</SUB></I> be the set of file handles created  
from one particular collective open of the file <I>FOO</I>,  
and <I>FH<SUB>2</SUB></I> be the set of file handles created  
from a different collective open of <I>FOO</I>.  
Note that nothing restrictive is said about <I>FH<SUB>1</SUB></I> and <I>FH<SUB>2</SUB></I>:  
the sizes of <I>FH<SUB>1</SUB></I> and <I>FH<SUB>2</SUB></I> may be different,  
the groups of processes used for each open may or may not intersect,  
the file handles in <I>FH<SUB>1</SUB></I> may be destroyed  
before those in <I>FH<SUB>2</SUB></I> are created, etc.  
Consider the following three cases:  
a single file handle (e.g., <IMG WIDTH=55 HEIGHT=10 SRC="img108.gif">
),  
two file handles created from a single collective open  
  (e.g., <IMG WIDTH=51 HEIGHT=10 SRC="img109.gif">
 and <IMG WIDTH=51 HEIGHT=10 SRC="img110.gif">
), and  
two file handles from different collective opens  
  (e.g., <IMG WIDTH=59 HEIGHT=10 SRC="img111.gif">
 and <IMG WIDTH=6 HEIGHT=2 SRC="img112.gif">
).  
  
<P> 
  
For the purpose of consistency semantics, a matched   
pair (Section <a href="node282.htm#Node282">Split Collective Data Access Routines 
</a>,  
page <a href="node282.htm#Node282">Split Collective Data Access Routines 
</a>) of split  
collective data access operations (e.g.,  
 MPI_FILE_READ_ALL_BEGIN and  
 MPI_FILE_READ_ALL_END) compose a single data access  
operation.  
Similarly, a nonblocking data access routine  
(e.g.,  MPI_FILE_IREAD) and the  
routine which completes the request  
(e.g.,  MPI_WAIT) also compose a single data  
access operation.  
For all cases below, these data access operations   
are subject to the same constraints as blocking data access operations.  
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
For an  MPI_FILE_IREAD and  MPI_WAIT pair,  
the operation begins when  MPI_FILE_IREAD is called and  
ends when  MPI_WAIT returns.  
 (<em> End of advice to users.</em>) <BR> 
Assume that <I>A<SUB>1</SUB></I> and <I>A<SUB>2</SUB></I> are two data access operations.  
Let <I>D<SUB>1</SUB></I> (<I>D<SUB>2</SUB></I>) be the set of absolute byte displacements  
of every byte accessed in <I>A<SUB>1</SUB></I> (<I>A<SUB>2</SUB></I>).  
The two data accesses <em> overlap</em>  
if <IMG WIDTH=7 HEIGHT=9 SRC="img113.gif">
.  
The two data accesses <em> conflict</em>  
if they overlap and at least one is a write access.  
  
<P> 
Let <I>SEQ<SUB>fh</SUB></I> be a sequence of file operations on a single file handle,  
bracketed by  MPI_FILE_SYNCs on that file handle.  
  
(Both opening and closing a file implicitly perform an  MPI_FILE_SYNC.)  
  
<I>SEQ<SUB>fh</SUB></I> is a ``write sequence''  
if any of the data access operations in the sequence are writes  
or if any of the file manipulation operations in the sequence  
change the state of the file  
  
(e.g.,  MPI_FILE_SET_SIZE or  MPI_FILE_PREALLOCATE).  
  
Given two sequences, <I>SEQ<SUB>1</SUB></I> and <I>SEQ<SUB>2</SUB></I>,  
we say they  
are not <em> concurrent</em>  
if one sequence is guaranteed to completely precede the other (temporally).  
  
<P> 
  
The requirements for guaranteeing sequential consistency among all  
accesses to a particular file are divided into the three cases given  
below.  If any of these requirements are not met, then the value of  
all data in that file is implementation dependent.  
  
<P> 
<P> 
Case 1: $fh_1  FH_1$   
All operations on <I>fh<SUB>1</SUB></I> are sequentially consistent if atomic mode  
is set.  If nonatomic mode is set, then all operations on <I>fh<SUB>1</SUB></I>   
are sequentially consistent if they are either nonconcurrent, nonconflicting,  
or both.  
  
<P> 
<P> 
Case 2: $fh_1a  FH_1$ and $fh_1b  FH_1$ Assume <I>A<SUB>1</SUB></I> is a data access operation using <I>fh<SUB>1a</SUB></I>,  
and    <I>A<SUB>2</SUB></I> is a data access operation using <I>fh<SUB>1b</SUB></I>.  
  
If for any access <I>A<SUB>1</SUB></I>, there is no access <I>A<SUB>2</SUB></I> that conflicts  
with <I>A<SUB>1</SUB></I>, then  MPI guarantees sequential consistency.  
  
<P> 
However, unlike POSIX semantics, the default  MPI semantics  
for conflicting accesses  
do not guarantee sequential consistency.  
If <I>A<SUB>1</SUB></I> and <I>A<SUB>2</SUB></I> conflict,  
  
sequential consistency can be guaranteed by either  
enabling atomic mode via the  MPI_FILE_SET_ATOMICITY routine, or  
meeting the   
  
condition  
  
described in Case 3 below.  
<P> 
  
<P> 
<P> 
Case 3: $fh_1  FH_1$ and $fh_2  FH_2$   
<P> 
Consider access to a single file using file handles from distinct  
collective opens.  
  
  
In order to guarantee sequential consistency,  MPI_FILE_SYNC  
must be used (both opening and closing a file implicitly perform an  
 MPI_FILE_SYNC).  
<P> 
  
  
Sequential consistency is guaranteed among accesses to a single file if  
for any write sequence <I>SEQ<SUB>1</SUB></I> to the file,  
there is no sequence <I>SEQ<SUB>2</SUB></I> to the file which  
is <em> concurrent</em> with  
<I>SEQ<SUB>1</SUB></I>.  
  
  
To guarantee sequential consistency when there are write sequences,  
 MPI_FILE_SYNC must be used  
together with a mechanism that guarantees nonconcurrency of the sequences.  
<P> 
See the examples in Section <a href="node300.htm#Node300">Examples 
</a>,  
page <a href="node300.htm#Node300">Examples 
</a>, for  
further clarification of some of these consistency semantics.  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_SET_ATOMICITY(fh, flag)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN flag</TD><TD> true to set atomic mode,  false to set nonatomic mode (logical)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_set_atomicity(MPI_File fh, int flag) <BR></tt>  
 <tt> MPI_FILE_SET_ATOMICITY(FH, FLAG, IERROR)<BR> INTEGER FH, IERROR<BR>LOGICAL FLAG <BR></tt>  
 <tt> void MPI::File::Set_atomicity(bool flag) <BR></tt>  
<P> 
Let <I>FH</I> be the set of file handles created by one collective open.  
The consistency semantics for data access operations using <I>FH</I>  
is set by collectively calling  MPI_FILE_SET_ATOMICITY on <I>FH</I>.  
 MPI_FILE_SET_ATOMICITY is collective;  
all processes in the group must pass identical values for  
 fh and  flag.  
If  flag is  true, atomic mode is set;  
if  flag is  false, nonatomic mode is set.  
<P> 
Changing the consistency semantics for an open file only affects new  
data accesses.  All completed data accesses are guaranteed to abide  
by the consistency semantics in effect during their execution.  
Nonblocking data accesses  
and split collective operations  
that  
  
have not completed  
(e.g., via  MPI_WAIT) are only guaranteed  
to abide by nonatomic mode consistency semantics.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
Since the semantics guaranteed by atomic mode are stronger than  
those guaranteed by nonatomic mode, an implementation is  
free to adhere to the more stringent atomic mode semantics  
for outstanding requests.  
 (<em> End of advice to implementors.</em>) <BR> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_GET_ATOMICITY(fh, flag)</TD></TR>  
<TR><TD> IN fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT flag</TD><TD> true if atomic mode,  false if nonatomic mode (logical)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_get_atomicity(MPI_File fh, int *flag) <BR></tt>  
 <tt> MPI_FILE_GET_ATOMICITY(FH, FLAG, IERROR)<BR> INTEGER FH, IERROR<BR>LOGICAL FLAG <BR></tt>  
  
 <tt> bool MPI::File::Get_atomicity() const <BR></tt>  
  
<P> 
 MPI_FILE_GET_ATOMICITY returns  
the current consistency semantics for  
data access operations on  
the set of file handles created by one collective open.  
If  flag is  true, atomic mode is enabled;  
if  flag is  false, nonatomic mode is enabled.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_SYNC(fh)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_sync(MPI_File fh) <BR></tt>  
 <tt> MPI_FILE_SYNC(FH, IERROR)<BR> INTEGER FH, IERROR <BR></tt>  
  
 <tt> void MPI::File::Sync() <BR></tt>  
  
<P> 
Calling  MPI_FILE_SYNC with  fh causes  
all previous writes to  fh by the calling process   
to be transferred to the storage device.  
If other processes have made updates to the storage device,  
then all such updates become visible to  
subsequent reads of  fh by the calling process.  
  
 MPI_FILE_SYNC   
  
may be necessary to ensure sequential consistency in certain  
cases (see above).  
  
  
<P> 
 MPI_FILE_SYNC is a collective operation.  
<P> 
The user is responsible for ensuring that all nonblocking requests  
and split collective operations  
on  fh have been completed before calling  
 MPI_FILE_SYNC---otherwise, the call to  
 MPI_FILE_SYNC is erroneous.  
<P> 

<P>
<HR>
<A HREF="node290.htm#Node290"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node290.htm#Node290"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node292.htm#Node292"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node290.htm#Node290"> Consistency and Semantics</a>
<b>Next: </b><A HREF="node292.htm#Node292"> Random Access vs. Sequential Files</a>
<b>Previous: </b><A HREF="node290.htm#Node290"> Consistency and Semantics</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
