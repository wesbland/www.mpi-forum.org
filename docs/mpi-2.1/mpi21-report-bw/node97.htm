<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Gather-to-all</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node97">96. Gather-to-all</a></H1>
<A HREF="node96.htm#Node96"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node98.htm#Node98"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node98.htm#Node98"> Examples using  MPI_ALLGATHER,  MPI_ALLGATHERV</a>
<b>Previous: </b><A HREF="node96.htm#Node96"> Examples using  MPI_SCATTER,  MPI_SCATTERV</a>
<P>
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLGATHER( sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements in send buffer (non-negative  
integer)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements received from any process (non-negative  
integer)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  comm</TD><TD>  communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Allgather(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_ALLGATHER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, COMM, IERROR <BR></tt>  
  
  
 <tt> void MPI::Comm::Allgather(const void* sendbuf, int sendcount, const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount, const MPI::Datatype&amp; recvtype) const = 0 <BR></tt>  
  
  
 MPI_ALLGATHER can be thought of as  MPI_GATHER, but  
where all processes receive the result, instead of just the root.  
The block of data sent from the   
<tt> j</tt>-th   
process is received by every process and placed in the   
<tt> j</tt>-th   
block of the buffer  recvbuf.  
<P> 
The type signature associated with  sendcount, sendtype,  
at a process must be equal to the type signature associated with  
 recvcount, recvtype at any other process.  
<P> 
If  comm is an intracommunicator,   
the outcome of a call to  MPI_ALLGATHER(...) is as if  
all processes executed <tt> n</tt> calls to  
<BR> 
<pre><tt>   MPI_GATHER(sendbuf,sendcount,sendtype,recvbuf,recvcount, 
                                                 recvtype,root,comm), 
</tt></pre> 
for <tt> root = 0 , ..., n-1</tt>.  The rules for correct usage of  
 MPI_ALLGATHER are easily found from the corresponding rules  
for  MPI_GATHER.  
  
The ``in place'' option  for intracommunicators is specified by passing the  
value   
 MPI_IN_PLACE to the argument  sendbuf at all processes.  
 sendcount and  sendtype are ignored.  Then the input data  
of each process is assumed to be in the area where that  
process would receive its own contribution to the receive  
buffer.  
  
If  comm is an intercommunicator, then each process in group A  
contributes a data item; these items are concatenated and the result  
is stored at each process in group B.  Conversely the concatenation of the  
contributions of the processes in group B is stored at each process in  
group A.   The send buffer arguments in group A must be consistent  
with the receive buffer arguments in group B, and vice versa.  
<P> 
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
The communication pattern of  MPI_ALLGATHER executed on an  
intercommunication domain need not be symmetric.  The number of items  
sent by processes in group A (as specified by the arguments  
 sendcount, sendtype in group A and the arguments  
 recvcount, recvtype in group B), need not equal the number of  
items sent by processes in group B (as specified by the arguments  
 sendcount, sendtype in group B and the arguments  
 recvcount, recvtype in group A).  In particular, one can move  
data in only one direction by specifying  sendcount = 0 for  
the communication in the reverse direction.  
<P> 
 (<em> End of advice to users.</em>) <BR> 
  
<TABLE><TR><TD COLSPAN=2>MPI_ALLGATHERV( sendbuf, sendcount, sendtype, recvbuf,  
recvcounts, displs, recvtype, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements in send buffer (non-negative  
integer)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcounts</TD><TD>non-negative  
integer array (of length group size)  
containing the number of elements that are received from each process</TD></TR>  
<TR><TD> IN  displs</TD><TD> integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement (relative to  recvbuf) at  
which to place the incoming data from process <tt> i</tt></TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  comm</TD><TD>  communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Allgatherv(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int *recvcounts, int *displs, MPI_Datatype recvtype, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_ALLGATHERV(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNTS, DISPLS, RECVTYPE, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNTS(*), DISPLS(*), RECVTYPE, COMM, IERROR <BR></tt>  
  
  
 <tt> void MPI::Comm::Allgatherv(const void* sendbuf, int sendcount,  const MPI::Datatype&amp; sendtype, void* recvbuf,  const int recvcounts[], const int displs[],  const MPI::Datatype&amp; recvtype) const = 0 <BR></tt>  
  
  
 MPI_ALLGATHERV can be thought of as  MPI_GATHERV, but  
where all processes receive the result, instead of just the root.  
The block of data sent from the   
<tt> j</tt>-th   
process is received by every process and placed in the   
<tt> j</tt>-th   
block of the buffer  recvbuf.  
These blocks need not all be the same size.  
<P> 
The type signature associated with  sendcount, sendtype,  
at process <tt> j</tt> must be equal to the type signature associated with  
 recvcounts[j], recvtype at any other process.  
<P> 
If  comm is an intracommunicator,   
the outcome is as if all processes executed calls to  
<BR> 
<pre><tt>    MPI_GATHERV(sendbuf,sendcount,sendtype,recvbuf,recvcounts,displs, 
                                                   recvtype,root,comm), 
</tt></pre> 
for <tt> root = 0 , ..., n-1</tt>.  The rules for correct usage of  
 MPI_ALLGATHERV are easily found from the corresponding rules  
for  MPI_GATHERV.  
  
The ``in place'' option  for intracommunicators is specified by passing the  
value   
 MPI_IN_PLACE to the argument  sendbuf at all processes.  
 sendcount and  sendtype are ignored.  Then the input data  
of each process is assumed to be in the area where that  
process would receive its own contribution to the receive   
buffer.  
  
If  comm is an intercommunicator, then each process in group A  
contributes a data item; these items are concatenated and the result  
is stored at each process in group B.  Conversely the concatenation of the  
contributions of the processes in group B is stored at each process in  
group A.   The send buffer arguments in group A must be consistent  
with the receive buffer arguments in group B, and vice versa.  
<P> 
  
<menu> 
</menu> 

<P>
<HR>
<A HREF="node96.htm#Node96"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi21-report-bw.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node98.htm#Node98"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi21-report-bw.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node98.htm#Node98"> Examples using  MPI_ALLGATHER,  MPI_ALLGATHERV</a>
<b>Previous: </b><A HREF="node96.htm#Node96"> Examples using  MPI_SCATTER,  MPI_SCATTERV</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
