<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-context/context.tex -->
<!-- with the command
tohtml -default -basedef ../mpi2defs-bw.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi2-forum-tail.htm -Wnoredef -o mpi21-report-bw.tex mpi-report.tex 
-->
<TITLE>Communicator Constructors</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node129">127. Communicator Constructors</a></H2>
<A HREF="node128.htm#Node128"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node127.htm#Node127"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node130.htm#Node130"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node127.htm#Node127"> Communicator Management</a>
<b>Next: </b><A HREF="node130.htm#Node130"> Communicator Destructors</a>
<b>Previous: </b><A HREF="node128.htm#Node128"> Communicator Accessors</a>
<P>
  
The following are collective functions that are invoked by all processes in the  
group or groups associated with  comm.  
  
 
<BR> 
<em> Rationale.</em>  
<P> 
Note that there is a chicken-and-egg aspect to  MPI in that a  
communicator is needed to create a new communicator.  The base  
communicator for all  MPI communicators is predefined outside of  MPI,  
and is  MPI_COMM_WORLD.  This model was arrived at after  
considerable debate, and was chosen to increase ``safety'' of programs  
written in  MPI.  
 (<em> End of rationale.</em>) <BR> 
  
<P> 
The  MPI interface provides four communicator construction routines that  
apply to both intracommunicators and intercommunicators. The construction routine  
 MPI_INTERCOMM_CREATE (discussed later) applies only to intercommunicators.  
<P> 
An intracommunicator involves a single group while an intercommunicator   
involves two groups.  
Where the following discussions address intercommunicator semantics,   
the two groups in an intercommunicator are  
called the <em> left</em> and <em> right</em> groups.  A process in an  
intercommunicator is a member of either the left or the right group.  From the  
point of view of that process, the  
group that the process is a member of is called the <em> local</em> group; the  
other group (relative to that process) is the <em> remote</em> group.  
The left and right group labels give us a way to describe the two groups in  
an intercommunicator that is not relative to any particular process (as the  
local and remote groups are).  
  
<TABLE><TR><TD COLSPAN=2>MPI_COMM_DUP(comm, newcomm)</TD></TR>  
<TR><TD> IN comm</TD><TD> communicator (handle)</TD></TR>  
<TR><TD> OUT newcomm</TD><TD> copy of  comm (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Comm_dup(MPI_Comm comm, MPI_Comm *newcomm) <BR></tt>  
<P> 
 <tt> MPI_COMM_DUP(COMM, NEWCOMM, IERROR)<BR> INTEGER COMM, NEWCOMM, IERROR <BR></tt>  
 <tt> MPI::Intracomm MPI::Intracomm::Dup() const <BR></tt>  
 <tt> MPI::Intercomm MPI::Intercomm::Dup() const <BR></tt>  
 <tt> MPI::Cartcomm MPI::Cartcomm::Dup() const <BR></tt>  
 <tt> MPI::Graphcomm MPI::Graphcomm::Dup() const <BR></tt>  
 <tt> MPI::Comm&amp; MPI::Comm::Clone() const = 0 <BR></tt>  
 <tt> MPI::Intracomm&amp; MPI::Intracomm::Clone() const <BR></tt>  
 <tt> MPI::Intercomm&amp; MPI::Intercomm::Clone() const <BR></tt>  
 <tt> MPI::Cartcomm&amp; MPI::Cartcomm::Clone() const <BR></tt>  
 <tt> MPI::Graphcomm&amp; MPI::Graphcomm::Clone() const <BR></tt>  
  
 MPI_COMM_DUP Duplicates the existing communicator  comm with  
associated key values.  For each key value, the respective copy callback  
function determines the attribute value associated with this key in the  
new communicator; one particular action that a copy callback may take  
is to delete the attribute from the new communicator.  
Returns in  newcomm a new  
communicator with the same group or groups, any copied cached information,  
but a new context (see Section <a href="node146.htm#Node146">Functionality 
</a>).  
Please see Section <a href="node331.htm#Node331">Communicators 
</a> on page <a href="node331.htm#Node331">Communicators 
</a> for further discussion   
about the C++ bindings for  Dup() and  Clone().   
  
 
<BR> 
<em> Advice to users.</em>  
<P> 
This operation is used to provide a parallel library call with a duplicate  
communication  
space that has the same properties as the original communicator.  This  
includes any attributes (see below), and topologies (see  
Chapter <a href="node159.htm#Node159">Process Topologies 
</a>).  This call is valid even if there are  
pending point-to-point communications involving the communicator  
 comm.  A typical call might involve a  MPI_COMM_DUP  
at the beginning of the parallel call, and an  MPI_COMM_FREE of  
that duplicated communicator at the end of the call.  Other models  
of communicator management are also possible.  
<P> 
This call applies to both intra- and inter-communicators.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
One need not actually copy the group information, but only add a new reference  
and increment the reference count.  Copy on write can be used for the cached  
information. (<em> End of advice to implementors.</em>) <BR> 
  
<TABLE><TR><TD COLSPAN=2>MPI_COMM_CREATE(comm, group, newcomm)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator (handle)</TD></TR>  
<TR><TD> IN group</TD><TD> Group, which is a subset of the group of  
 comm (handle)</TD></TR>  
<TR><TD> OUT newcomm</TD><TD> new communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm *newcomm) <BR></tt>  
<P> 
 <tt> MPI_COMM_CREATE(COMM, GROUP, NEWCOMM, IERROR)<BR> INTEGER COMM, GROUP, NEWCOMM, IERROR <BR></tt>  
  
 <tt> MPI::Intercomm MPI::Intercomm::Create(const MPI::Group&amp; group) const <BR></tt>  
  
 <tt> MPI::Intracomm MPI::Intracomm::Create(const MPI::Group&amp; group) const <BR></tt>  
  
  
 If  comm is an intra-communicator, this function creates a new communicator  newcomm with  
communication group defined by  group and a new context.  No cached  
information propagates from  comm to  newcomm.  The function  
returns  MPI_COMM_NULL to processes that are not in  group.  
The call is erroneous if not all  group arguments have the same value,  
or if  group is not a subset of the group associated with  
 comm.  Note that the call is to be executed by all processes in  
 comm, even if they do not belong to the new group.  
  
 
<BR> 
<em> Rationale.</em>  
<P> 
The requirement that the entire group of  comm participate in the call  
stems from the following considerations:  
<ul> 
 
<li>It allows the implementation to layer  MPI_COMM_CREATE on top of  
regular collective communications.  
 
<li>It provides additional safety, in particular in the case where partially  
overlapping groups are used to create new communicators.  
 
<li>It permits implementations sometimes to avoid communication related to context  
creation.  
</ul> 
<BR> 
 (<em> End of rationale.</em>) <BR> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
 MPI_COMM_CREATE provides a means to subset a group of processes for the  
purpose of separate MIMD computation, with separate communication space.  
 newcomm, which emerges from  MPI_COMM_CREATE can be used in  
subsequent calls to  MPI_COMM_CREATE (or other communicator  
constructors) further to subdivide a computation into parallel  
sub-computations.  A more general service is provided by  
 MPI_COMM_SPLIT, below.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
Since all processes calling  MPI_COMM_DUP or  
<BR>  
 MPI_COMM_CREATE provide the same  group argument, it  
is theoretically possible to agree on a group-wide unique context with  
no communication.  However, local execution of these functions requires  
use of a larger context name space and reduces error checking.  
Implementations may strike various compromises between these  
conflicting goals, such as bulk allocation of multiple contexts in one  
collective operation.  
<P> 
Important: If new communicators are created without synchronizing the  
processes involved then the communication system should be able to cope with  
messages arriving in a context that has not yet been allocated at the  
receiving process.  
 (<em> End of advice to implementors.</em>) <BR> 
  
 If  comm is an  
intercommunicator, then the output communicator is also an intercommunicator  
where the local group consists only of those processes contained in  
 group (see Figure <a href="node129.htm#Figure13">13 
</a>).  The  group   
argument should only contain those processes in the local group of the input  
intercommunicator that are to be a part of  newcomm.  If either  
 group does not specify at least one process in the local group of  
the intercommunicator, or if the calling process is not included in the  
 group,  MPI_COMM_NULL is returned.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
In the case where either the left or right group is empty, a null communicator  
is returned instead of an intercommunicator with  MPI_GROUP_EMPTY  
because the side with the empty group must return  MPI_COMM_NULL.  
 (<em> End of rationale.</em>) <BR> 
  
  <CENTER><P><IMG WIDTH=481 HEIGHT=459 SRC="collective-create.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 13: </b><A NAME="Figure13">Intercommunicator create using  MPI_COMM_CREATE
extended to intercommunicators.  The input groups are those in the grey
circle.</a><P> 
  
    
<BR><b> Example</b>  
The following example illustrates how the first node in the left  
side of an intercommunicator could be joined with all members on the  
right side of an intercommunicator to form a new  
intercommunicator.  
  
  
  
  
  
<BR> 
<pre><tt>        MPI_Comm  inter_comm, new_inter_comm; 
        MPI_Group local_group, group; 
        int       rank = 0; /* rank on left side to include in  
                               new inter-comm */ 
 
        /* Construct the original intercommunicator: "inter_comm" */ 
        ... 
 
        /* Construct the group of processes to be in new  
           intercommunicator */ 
        if (/* I'm on the left side of the intercommunicator */) { 
          MPI_Comm_group ( inter_comm, &amp;local_group ); 
          MPI_Group_incl ( local_group, 1, &amp;rank, &amp;group ); 
          MPI_Group_free ( &amp;local_group ); 
        } 
        else  
          MPI_Comm_group ( inter_comm, &amp;group ); 
 
        MPI_Comm_create ( inter_comm, group, &amp;new_inter_comm ); 
        MPI_Group_free( &amp;group ); 
</tt></pre> 
  
  
<TABLE><TR><TD COLSPAN=2>MPI_COMM_SPLIT(comm, color, key, newcomm)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator (handle)</TD></TR>  
<TR><TD> IN color</TD><TD>control of subset assignment (integer)</TD></TR>  
<TR><TD> IN key</TD><TD> control of rank assigment (integer)</TD></TR>  
<TR><TD> OUT newcomm</TD><TD> new communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *newcomm) <BR></tt>  
<P> 
 <tt> MPI_COMM_SPLIT(COMM, COLOR, KEY, NEWCOMM, IERROR)<BR> INTEGER COMM, COLOR, KEY, NEWCOMM, IERROR <BR></tt>  
  
 <tt> MPI::Intercomm MPI::Intercomm::Split(int color, int key) const <BR></tt>  
  
 <tt> MPI::Intracomm MPI::Intracomm::Split(int color, int key) const <BR></tt>  
  
  
 This function partitions the group associated with  comm  
into disjoint subgroups, one for each value of  color.  Each subgroup  
contains all processes of the same color.  Within each subgroup, the processes  
are ranked in the order defined by the value of the argument  key,  
with ties broken according to their rank in the old group.  A new communicator  
is created for each subgroup and returned in  newcomm.  A process may  
supply the color value  MPI_UNDEFINED, in which case  newcomm  
returns  MPI_COMM_NULL.  This is a collective call, but each process  
is permitted to provide different values for  color and  key.  
<P> 
A call to  MPI_COMM_CREATE(comm, group, newcomm) is equivalent to  
<BR>  
a call to  
 MPI_COMM_SPLIT(comm, color, key, newcomm),  
where all  
members of  group provide  color<I>~ =~0</I> and  
 key<I>~=~</I> rank in  
 group, and  all processes that are not members of  group provide  
 color<I>~ =~</I>  MPI_UNDEFINED.  
The function  MPI_COMM_SPLIT allows  
more general partitioning of a group  
into one or more subgroups with optional reordering.  
  
  
The value of  color must be nonnegative.  
  
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
This is an extremely powerful mechanism for dividing a single  
communicating  group of processes into <I>k</I> subgroups, with <I>k</I> chosen  
implicitly by the user (by the number of  
colors asserted over all the processes).  Each resulting communicator will be  
non-overlapping.  Such a division could be useful for defining a hierarchy  
of computations, such as for multigrid, or linear algebra.  
<P> 
Multiple calls to  MPI_COMM_SPLIT can be used to overcome the  
requirement that any call have no overlap of the resulting communicators (each  
process is of only one color per call).  In this way, multiple overlapping  
communication structures can be created.  Creative use of the  color  
and  key in such splitting operations is encouraged.  
<P> 
Note that, for a fixed color, the keys need not  
be unique.  It is  MPI_COMM_SPLIT's responsibility to sort processes  
in ascending order according to this key, and to break ties in a consistent  
way.  If all the keys are specified in the same way, then all the processes  
in a given color will have the relative rank order as they did in their  
parent group.    
  
Essentially, making the key value zero for all processes of a given color  
means that one doesn't really care about the rank-order of the processes in  
the new communicator.  
 (<em> End of advice to users.</em>) <BR> 
  
 
<BR> 
<em> Rationale.</em>  
<P> 
 color is restricted to be nonnegative, so as not to confict with the value assigned to  MPI_UNDEFINED.  
 (<em> End of rationale.</em>) <BR> 
  
  
 The result of  MPI_COMM_SPLIT on an intercommunicator is that those  
processes on the left with the same  color as those processes on  
the right combine to create a new intercommunicator.  The  key  
argument describes the relative rank of processes on each side of the  
intercommunicator (see Figure <a href="node129.htm#Figure14">14 
</a>).  For those colors  
that are specified only on one side of the intercommunicator,  
 MPI_COMM_NULL is returned.   MPI_COMM_NULL   
is also returned to those processes that specify  MPI_UNDEFINED  
as the color.  
<P> 
<P> 
  <CENTER><P><IMG WIDTH=293 HEIGHT=431 SRC="collective-split2.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 14: </b><A NAME="Figure14">Intercommunicator construction achieved by splitting an
    existing intercommunicator with  MPI_COMM_SPLIT
extended to intercommunicators.</a><P> 
  
    
<BR><b> Example</b>(Parallel client-server model).  
The following client code illustrates how clients on the left side of an  
intercommunicator could be assigned to a single server from a pool of  
servers on the right side of an intercommunicator.  
  
  
  
<BR> 
<pre><tt>        /* Client code */ 
        MPI_Comm  multiple_server_comm; 
        MPI_Comm  single_server_comm; 
        int       color, rank, num_servers; 
         
        /* Create intercommunicator with clients and servers:  
           multiple_server_comm */ 
        ... 
         
        /* Find out the number of servers available */ 
        MPI_Comm_remote_size ( multiple_server_comm, &amp;num_servers ); 
         
        /* Determine my color */ 
        MPI_Comm_rank ( multiple_server_comm, &amp;rank ); 
        color = rank % num_servers; 
         
        /* Split the intercommunicator */ 
        MPI_Comm_split ( multiple_server_comm, color, rank,  
                         &amp;single_server_comm ); 
</tt></pre> 
 The following is the corresponding server code:  
<BR> 
<pre><tt>        /* Server code */ 
        MPI_Comm  multiple_client_comm; 
        MPI_Comm  single_server_comm; 
        int       rank; 
 
        /* Create intercommunicator with clients and servers:  
           multiple_client_comm */ 
        ... 
         
        /* Split the intercommunicator for a single server per group 
           of clients */ 
        MPI_Comm_rank ( multiple_client_comm, &amp;rank ); 
        MPI_Comm_split ( multiple_client_comm, rank, 0,  
                         &amp;single_server_comm );   
</tt></pre> 
  
  

<P>
<HR>
<A HREF="node128.htm#Node128"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node127.htm#Node127"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node130.htm#Node130"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node127.htm#Node127"> Communicator Management</a>
<b>Next: </b><A HREF="node130.htm#Node130"> Communicator Destructors</a>
<b>Previous: </b><A HREF="node128.htm#Node128"> Communicator Accessors</a>
<P>
<HR>
Return to <A HREF="node428.htm">MPI-2.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 1, 2008<BR>
HTML Generated on July 6, 2008
</FONT>
</BODY>
</HTML>
