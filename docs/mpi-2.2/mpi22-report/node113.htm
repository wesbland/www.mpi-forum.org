<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml erif"> MPI-2.0</font>
-->
<TITLE><font face="sans-serif"> MPI_REDUCE_SCATTER</font></TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node113">111.  MPI_REDUCE_SCATTER</a></H2>
<A HREF="node112.htm#Node112"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node111.htm#Node111"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node114.htm#Node114"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node111.htm#Node111"> Reduce-Scatter</a>
<b>Next: </b><A HREF="node114.htm#Node114"> Scan</a>
<b>Previous: </b><A HREF="node112.htm#Node112"> <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font></a>
<P>
<font face="sans-serif"> MPI_REDUCE_SCATTER</font> extends the functionality of <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font>  
such that the scattered blocks can vary in size.  
Block sizes are determined by the <font face="sans-serif"> recvcounts</font> array,  
such that the <tt> i</tt>-th block contains <font face="sans-serif"> recvcounts[i]</font> elements.  
<font color="black">  
<TABLE><TR><TD COLSPAN=2>MPI_REDUCE_SCATTER( sendbuf, recvbuf, recvcounts,  
datatype, op, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD> starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcounts</TD><TD>non-negative  
integer array <font color="red">(of length group size) <font color="black">specifying the number of elements <font color="red">of the<font color="black"> result distributed to each process.  
</TD></TR>  
<TR><TD> IN datatype</TD><TD> data type of elements of <font color="red">send and receive buffers<font color="black"> (handle)</TD></TR>  
<TR><TD> IN op</TD><TD> operation (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Reduce_scatter(void* sendbuf, void* recvbuf, int *recvcounts, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_REDUCE_SCATTER(SENDBUF, RECVBUF, RECVCOUNTS, DATATYPE, OP, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER RECVCOUNTS(*), DATATYPE, OP, COMM, IERROR <BR></tt>  
  
  
 <font color="red">{<font color="black"> void MPI::Comm::Reduce_scatter(const void* sendbuf,  void* recvbuf, int recvcounts[], const MPI::Datatype&amp; datatype,  const MPI::Op&amp; op) const = 0 <font color="red"><em> (binding deprecated, see Section <a href="node328.htm#Node328">Deprecated since <font face="sans-serif"> MPI-2.2</font> 
</a>)</em> }<font color="black"><BR>  
  
  
<P> 
If <font face="sans-serif"> comm</font> is an intracommunicator,  
<font face="sans-serif"> MPI_REDUCE_SCATTER</font> first  
<font color="red">performs a global, element-wise reduction on vectors of   
<IMG WIDTH=123 HEIGHT=13 SRC="img81.gif">
 elements in the send  
buffers defined by <font face="sans-serif"> sendbuf</font>, <font face="sans-serif"> count</font> and  
<font face="sans-serif"> datatype</font>, using the operation <font face="sans-serif"> op</font>, where <tt> n</tt> is the number of  
processes in the group of <font face="sans-serif"> comm</font>. The routine is called by all group  
members using the same arguments for <font face="sans-serif"> recvcounts</font>,  
<font face="sans-serif"> datatype</font>, <font face="sans-serif"> op</font> and <font face="sans-serif"> comm</font>.  
The resulting vector is treated as n consecutive blocks where the number  
of elements of the <tt> i</tt>-th block is <font face="sans-serif"> recvcounts[i]</font>. The blocks are scattered  
to the processes of the group. The <tt> i</tt>-th block   
<font color="black">is sent to process <tt> i</tt> and stored in the  
receive buffer defined by <font face="sans-serif"> recvbuf, recvcounts[i]</font> and  
<font face="sans-serif"> datatype</font>.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
The <font face="sans-serif"> MPI_REDUCE_SCATTER</font>  
routine is functionally equivalent to:  
an  
<font face="sans-serif"> MPI_REDUCE</font>  
collective  
operation  
with <font face="sans-serif"> count</font> equal to  
the sum of <font face="sans-serif"> recvcounts[i]</font> followed by  
<font face="sans-serif"> MPI_SCATTERV</font> with <font face="sans-serif"> sendcounts</font> equal to <font face="sans-serif"> recvcounts</font>.  
However, a direct implementation may run faster.  
 (<em> End of advice to implementors.</em>) <BR> 
  
The ``in place'' option  for intracommunicators is specified by passing  
<font face="sans-serif">  MPI_IN_PLACE</font> in   
the <font face="sans-serif"> sendbuf</font> argument.  
In this case, the input data is taken from the <font color="red">receive<font color="black">buffer. <font color="red">It is not required to specify the ``in  
place'' option on all processes, since the processes for which  
<font face="sans-serif"> recvcounts[i]</font><font face="sans-serif"> ==0</font> may not have allocated a receive buffer.<font color="black">  
  
If <font face="sans-serif"> comm</font> is an intercommunicator, then the result of the reduction  
of the data provided by processes in <font color="red">one  
group (group A)<font color="black"> is scattered among processes in  
<font color="red">the other group (group B)<font color="black">, and vice  
versa.  Within each group, all processes provide the same  
<font face="sans-serif"> recvcounts</font> argument, and <font color="red">provide input vectors of <IMG WIDTH=123 HEIGHT=13 SRC="img82.gif">
  
elements stored in the send buffers, where <tt> n</tt> is the size of the group.  
The resulting vector from the other group is scattered in blocks of  
<font face="sans-serif"> recvcounts[i]</font> elements among the processes in the group. The number of  
elements <font face="sans-serif"> count</font> must<font color="black"> be the same for the two groups.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
The last restriction is needed so that the length of the send  
buffer can be determined by the sum of the local <font face="sans-serif"> recvcounts</font> entries.  
Otherwise, a communication is needed to figure out how many elements  
are reduced.  
 (<em> End of rationale.</em>) <BR> 
  

<P>
<HR>
<A HREF="node112.htm#Node112"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node111.htm#Node111"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node114.htm#Node114"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node111.htm#Node111"> Reduce-Scatter</a>
<b>Next: </b><A HREF="node114.htm#Node114"> Scan</a>
<b>Previous: </b><A HREF="node112.htm#Node112"> <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font></a>
<P>
<HR>
Return to <A HREF="node434.htm">MPI-2.2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-2.2 of September 4, 2009<BR>
HTML Generated on September 10, 2009
</FONT>
</BODY>
</HTML>
