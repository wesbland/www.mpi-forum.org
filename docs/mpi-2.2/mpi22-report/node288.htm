<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-io/io-2.tex -->
<!-- with the command
tohtml erif"> MPI-2.0</font>
-->
<TITLE>File Interoperability</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node288">265. File Interoperability</a></H1>
<A HREF="node287.htm#Node287"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi22-report.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node289.htm#Node289"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi22-report.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node289.htm#Node289"> Datatypes for File Interoperability</a>
<b>Previous: </b><A HREF="node287.htm#Node287"> Split Collective Data Access Routines</a>
<P>
  
<P> 
At the most basic level, file interoperability is the ability to  
read the information previously written to a file---not just the  
bits of data, but the actual information the bits represent.  
<font face="sans-serif"> MPI</font> guarantees full interoperability within a single <font face="sans-serif"> MPI</font> environment,  
  
and supports increased interoperability outside that environment  
through the external data representation (Section <a href="node290.htm#Node290">External Data Representation: ``external32'' 
</a>, page <a href="node290.htm#Node290">External Data Representation: ``external32'' 
</a>) as  
well as the data conversion functions (Section <a href="node291.htm#Node291">User-Defined Data Representations 
</a>,  
page <a href="node291.htm#Node291">User-Defined Data Representations 
</a>).   
  
<P> 
Interoperability within a single <font face="sans-serif"> MPI</font> environment  
(which could be considered ``operability'') ensures that file  
data written by one <font face="sans-serif"> MPI</font> process can be read by any other <font face="sans-serif"> MPI</font> process,  
subject to the consistency constraints  
(see Section <a href="node296.htm#Node296">File Consistency 
</a>,  
page <a href="node296.htm#Node296">File Consistency 
</a>),  
provided that it would have been possible to start the two processes  
simultaneously and have them reside in a single <font face="sans-serif">  MPI_COMM_WORLD</font>.  
Furthermore, both processes must see the same data values at every absolute  
byte offset in the file for which data was written.  
<P> 
This single environment file interoperability  
implies that file data is accessible  
regardless of the number of processes.  
  
  
There are three aspects to file interoperability:  
  
<ul> 
 
<li>transferring the bits,  
 
<li>converting between different file structures, and  
 
<li>converting between different machine representations.  
</ul> 
<BR> 
The first two aspects of file interoperability are beyond the  
scope of this standard, as both are highly machine dependent.  
However, transferring the bits of a file  
into and out of the <font face="sans-serif"> MPI</font> environment (e.g., by writing a file to tape)  
is required to be supported by all <font face="sans-serif"> MPI</font> implementations.  
  
In particular, an implementation must specify how familiar operations  
similar to POSIX <tt> cp</tt>, <tt> rm</tt>, and <tt> mv</tt> can  
  
be performed on the file.  
Furthermore, it is expected that the facility provided maintains  
the correspondence between absolute byte offsets (e.g., after  
possible file structure conversion, the data bits at byte offset 102  
in the <font face="sans-serif"> MPI</font> environment are at byte offset 102 outside  
the <font face="sans-serif"> MPI</font> environment).  
As an example, a simple off-line conversion utility that transfers  
and converts files between the native file system and the <font face="sans-serif"> MPI</font>   
environment would suffice, provided it maintained the offset  
coherence mentioned above.  
In a high-quality implementation of <font face="sans-serif"> MPI</font>,  
users will be able to manipulate <font face="sans-serif"> MPI</font> files  
using the same or similar tools that the native file system offers  
for manipulating its files.  
  
<P> 
The remaining aspect of file interoperability,  
converting between different machine representations,  
  
is supported  
  
by the typing information specified   
in the etype and filetype.  
This facility allows the information in files to be  
shared between any two applications,  
regardless of whether they use <font face="sans-serif"> MPI</font>,  
and regardless of the machine architectures on which they run.  
<P> 
<font face="sans-serif"> MPI</font> supports multiple data representations:  
``native,'' ``internal,'' and ``external32.''  
An implementation may support additional data representations.  
<font face="sans-serif"> MPI</font> also supports user-defined data representations  
(see Section <a href="node291.htm#Node291">User-Defined Data Representations 
</a>, page <a href="node291.htm#Node291">User-Defined Data Representations 
</a>).  
  
The   
``native'' and ``internal''   
data representations are implementation dependent,  
while the   
``external32''   
representation is  
common to all <font face="sans-serif"> MPI</font> implementations  
and facilitates file interoperability.  
The data representation is specified in the <em> datarep</em> argument  
to <font face="sans-serif"> MPI_FILE_SET_VIEW</font>.  
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
  
<font face="sans-serif"> MPI</font> is not  
guaranteed to retain knowledge of what data  
representation was used when a file is written.  
Therefore, to correctly retrieve file data, an <font face="sans-serif"> MPI</font>  
application is responsible for specifying the same data  
representation as was used to create the file.   
  
 (<em> End of advice to users.</em>) <BR> 
<dl> 
 
<dt> 
<b>``native''</b><dd> 
  
Data in this representation is stored in a file exactly  
as it is in memory.  
The advantage of this data representation is that  
  
data precision and I/O performance are not lost in type conversions  
with a purely homogeneous environment.  
The disadvantage is the loss of transparent interoperability within a  
heterogeneous <font face="sans-serif"> MPI</font> environment.  
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
This data representation should only be used in a homogeneous  
<font face="sans-serif"> MPI</font> environment, or when the <font face="sans-serif"> MPI</font> application is capable of performing  
the data type conversions itself.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
When implementing read and write operations  
on top of <font face="sans-serif"> MPI</font> message-passing, the message data should be typed as  
<font face="sans-serif"> MPI_BYTE</font> to ensure that the message routines do not perform any  
type conversions on the data.  
 (<em> End of advice to implementors.</em>) <BR> 
 
<dt> 
<b>``internal''</b><dd> 
  
This data representation can be used for I/O operations in a homogeneous or  
heterogeneous environment; the implementation will perform type  
conversions if necessary. The implementation is free to store data in  
any format of its choice,  
  
with the restriction that it will maintain constant extents  
for all predefined datatypes in any one file.  
  
The environment in which the resulting file   
can be reused is implementation-defined  
and must be documented by the implementation.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
This data representation allows the implementation  
to perform I/O efficiently in a  
heterogeneous environment, though with implementation-defined  
restrictions on how the file can be reused.  
 (<em> End of rationale.</em>) <BR> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
Since ``external32'' is a superset of the  
functionality provided by ``internal,''  
an implementation may choose to implement ``internal''  
as ``external32.''  
 (<em> End of advice to implementors.</em>) <BR> 
 
<dt> 
<b>``external32''</b><dd> 
  
This data representation states that read and write operations  
convert all data from  
and to the ``external32''  
representation defined in Section <a href="node290.htm#Node290">External Data Representation: ``external32'' 
</a>,  
page <a href="node290.htm#Node290">External Data Representation: ``external32'' 
</a>.  
The data conversion rules for communication also apply to these  
conversions (see Section 3.3.2, page 25-27, of the <font face="sans-serif"> MPI-1</font>  
document).  
The data on the storage  
medium is always in this canonical representation, and  
the data in memory  
is always in the local process's native representation.  
  
<P> 
This data representation has several advantages.  
First, all processes reading the  
file in a heterogeneous <font face="sans-serif"> MPI</font> environment will automatically have the  
data converted to their respective native representations.  
Second, the file can be exported from one <font face="sans-serif"> MPI</font> environment  
and imported into any other <font face="sans-serif"> MPI</font> environment  
with the guarantee that the second environment will be able  
to read all the data in the file.  
  
<P> 
The disadvantage of this data representation is that data precision and I/O  
performance may be lost in data type conversions.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
When implementing read and write operations  
on top of <font face="sans-serif"> MPI</font> message-passing, the message data should be converted  
to and from the ``external32'' representation in the client,  
and sent as type <font face="sans-serif"> MPI_BYTE</font>.  
This will avoid possible double data type conversions  
and the associated further loss of precision and performance.  
 (<em> End of advice to implementors.</em>) <BR> 
</dl> 
<BR> 
<menu> 
</menu> 

<P>
<HR>
<A HREF="node287.htm#Node287"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi22-report.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node289.htm#Node289"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi22-report.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node289.htm#Node289"> Datatypes for File Interoperability</a>
<b>Previous: </b><A HREF="node287.htm#Node287"> Split Collective Data Access Routines</a>
<P>
<HR>
Return to <A HREF="node434.htm">MPI-2.2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-2.2 of September 4, 2009<BR>
HTML Generated on September 10, 2009
</FONT>
</BODY>
</HTML>
