<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml erif"> MPI-2.0</font>
-->
<TITLE>All-Reduce</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node109">107. All-Reduce</a></H2>
<A HREF="node107.htm#Node108"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node102.htm#Node102"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node110.htm#Node110"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node102.htm#Node102"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node110.htm#Node110"> Process-local reduction</a>
<b>Previous: </b><A HREF="node107.htm#Node108"> Example of User-defined Reduce</a>
<P>
  
<P> 
<font face="sans-serif"> MPI</font> includes   
a variant  
of the reduce operations  
where the result is returned to all processes in  
a  
group.  
<font face="sans-serif"> MPI</font> requires that all processes  
from the same group  
participating in these operations  
receive identical results.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLREDUCE( sendbuf, recvbuf, count, datatype, op, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD> starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD> number of elements in send buffer (non-negative  
integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD> data type of elements of send buffer (handle)</TD></TR>  
<TR><TD> IN op</TD><TD> operation (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Allreduce(void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_ALLREDUCE(SENDBUF, RECVBUF, COUNT, DATATYPE, OP, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER COUNT, DATATYPE, OP, COMM, IERROR <BR></tt>  
  
  
 <font color="red">{<font color="black"> void MPI::Comm::Allreduce(const void* sendbuf, void* recvbuf,  int count, const MPI::Datatype&amp; datatype, const MPI::Op&amp; op) const = 0 <font color="red"><em> (binding deprecated, see Section <a href="node328.htm#Node328">Deprecated since <font face="sans-serif"> MPI-2.2</font> 
</a>)</em> }<font color="black"><BR>  
  
  
If <font face="sans-serif"> comm</font> is an intracommunicator,  
<font face="sans-serif"> MPI_ALLREDUCE</font> behaves the  
same as <font face="sans-serif"> MPI_REDUCE</font> except that the result  
appears in the receive buffer of all the group members.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
The all-reduce operations can be implemented as a reduce, followed by a  
broadcast.  However, a direct implementation can lead to better performance.  
 (<em> End of advice to implementors.</em>) <BR> 
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing the  
value <font face="sans-serif">  MPI_IN_PLACE</font> to the argument <font face="sans-serif"> sendbuf</font>  
at all processes.  
In this case,  
the input data is taken at each process from the receive buffer,  
where it will be replaced by the output data.  
<P> 
If <font face="sans-serif"> comm</font> is an intercommunicator, then the result of the reduction  
of the data provided by processes in group A is stored at each process  
in group B, and vice versa.    
Both groups should  
provide <font face="sans-serif"> count</font> and <font face="sans-serif"> datatype</font> arguments that specify the same type  
signature.  
  
<P> 
  
The following example uses an intracommunicator.  
  
<BR><b> Example</b>   
  
  
A routine that computes  
the product of a vector and an array that are distributed across a  
group of processes and returns the answer at all nodes (see also Example  
<a href="node104.htm#Node104">Predefined Reduction Operations 
</a>).  
<P> 
  
<BR> 
<pre><tt>SUBROUTINE PAR_BLAS2(m, n, a, b, c, comm) 
REAL a(m), b(m,n)    ! local slice of array 
REAL c(n)            ! result 
REAL sum(n) 
INTEGER n, comm, i, j, ierr 
 
! local sum 
DO j= 1, n 
  sum(j) = 0.0 
  DO i = 1, m 
    sum(j) = sum(j) + a(i)*b(i,j) 
  END DO 
END DO 
 
! global sum 
CALL MPI_ALLREDUCE(sum, c, n, MPI_REAL, MPI_SUM, comm, ierr) 
 
! return result at all nodes 
RETURN 
</tt></pre> 
  
  
  
<font color="red">
<P>
<HR>
<A HREF="node107.htm#Node108"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node102.htm#Node102"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node110.htm#Node110"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node102.htm#Node102"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node110.htm#Node110"> Process-local reduction</a>
<b>Previous: </b><A HREF="node107.htm#Node108"> Example of User-defined Reduce</a>
<P>
<HR>
Return to <A HREF="node434.htm">MPI-2.2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-2.2 of September 4, 2009<BR>
HTML Generated on September 10, 2009
</FONT>
</BODY>
</HTML>
