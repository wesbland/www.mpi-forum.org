<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml erif"> MPI-2.0</font>
-->
<TITLE>Correctness</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node118">116. Correctness</a></H1>
<A HREF="node117.htm#Node117"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi22-report.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node119.htm#Node119"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi22-report.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node119.htm#Node119"> Groups, Contexts, Communicators, and Caching</a>
<b>Previous: </b><A HREF="node117.htm#Node117"> Example using <font face="sans-serif"> MPI_SCAN</font></a>
<P>
  
<P> 
A correct, portable program must invoke collective communications so  
that deadlock will  
not occur, whether collective communications are synchronizing or not.  
The following examples illustrate dangerous use of collective routines  
on intracommunicators.  
  
<BR><b> Example</b>   
  
  
The following is erroneous.  
<P> 
<BR> 
<pre><tt>switch(rank) { 
    case 0: 
        MPI_Bcast(buf1, count, type, 0, comm); 
        MPI_Bcast(buf2, count, type, 1, comm); 
        break; 
    case 1: 
        MPI_Bcast(buf2, count, type, 1, comm); 
        MPI_Bcast(buf1, count, type, 0, comm); 
        break; 
} 
</tt></pre> 
We assume that the group of <font face="sans-serif"> comm</font> is {0,1}.  
Two processes execute two broadcast operations in reverse order.  If  
the operation is synchronizing then a deadlock will occur.  
<P> 
Collective  
operations must be executed in the same order at all members of the  
communication group.  
  
  
<P> 
<BR><b> Example</b>   
  
  
The following is erroneous.  
<P> 
<BR> 
<pre><tt>switch(rank) { 
    case 0: 
        MPI_Bcast(buf1, count, type, 0, comm0); 
        MPI_Bcast(buf2, count, type, 2, comm2); 
        break; 
    case 1: 
        MPI_Bcast(buf1, count, type, 1, comm1); 
        MPI_Bcast(buf2, count, type, 0, comm0); 
        break; 
    case 2: 
        MPI_Bcast(buf1, count, type, 2, comm2); 
        MPI_Bcast(buf2, count, type, 1, comm1); 
        break; 
} 
</tt></pre> 
Assume that the group of  
<font face="sans-serif"> comm0</font> is {0,1}, of <font face="sans-serif"> comm1</font> is {1, 2} and of <font face="sans-serif"> comm2</font>  
is {2,0}.  If the broadcast is a synchronizing operation, then there  
is a cyclic dependency: the broadcast in <font face="sans-serif"> comm2</font> completes only  
after the broadcast in <font face="sans-serif"> comm0</font>; the broadcast in <font face="sans-serif"> comm0</font>  
completes only after the broadcast in <font face="sans-serif"> comm1</font>; and the broadcast  
in <font face="sans-serif"> comm1</font> completes only after the broadcast in <font face="sans-serif"> comm2</font>.  
Thus, the code will deadlock.  
<P> 
Collective operations must be executed in an order so that  
no cyclic dependences occur.  
  
  
<P> 
<BR><b> Example</b>   
  
  
The following is erroneous.  
<P> 
<BR> 
<pre><tt>switch(rank) { 
    case 0: 
        MPI_Bcast(buf1, count, type, 0, comm); 
        MPI_Send(buf2, count, type, 1, tag, comm); 
        break; 
    case 1: 
        MPI_Recv(buf2, count, type, 0, tag, comm, status); 
        MPI_Bcast(buf1, count, type, 0, comm); 
        break; 
} 
</tt></pre> 
Process zero executes a broadcast, followed by a blocking send operation.  
Process one first executes a blocking receive that matches the send,  
followed by broadcast call that matches the broadcast of process zero.  
This program may deadlock.  The broadcast call on process zero  
<em> may</em> block until process one executes the matching  
broadcast call, so that the  
send is not executed.  Process one will definitely block on the  
receive and so, in this case, never executes the  
broadcast.  
<P> 
The relative order of execution of collective operations and point-to-point  
operations should be such, so that even if the collective  
operations and the point-to-point operations are synchronizing, no  
deadlock will occur.  
  
  
<P> 
<BR><b> Example</b>   
  
  
An unsafe,   
non-deterministic program.  
<P> 
<BR> 
<pre><tt>switch(rank) { 
    case 0: 
        MPI_Bcast(buf1, count, type, 0, comm); 
        MPI_Send(buf2, count, type, 1, tag, comm); 
        break; 
    case 1: 
        MPI_Recv(buf2, count, type, MPI_ANY_SOURCE, tag, comm, status); 
        MPI_Bcast(buf1, count, type, 0, comm); 
        MPI_Recv(buf2, count, type, MPI_ANY_SOURCE, tag, comm, status); 
        break; 
    case 2: 
        MPI_Send(buf2, count, type, 1, tag, comm); 
        MPI_Bcast(buf1, count, type, 0, comm); 
        break; 
} 
</tt></pre> 
All three processes participate in a broadcast.  Process 0 sends a message to  
process 1 after the broadcast, and process 2 sends a message  
to process 1 before  
the broadcast.  Process 1 receives before and after the broadcast, with a  
wildcard source argument.  
<P> 
Two possible executions of this program, with different matchings  
of sends and receives, are  
illustrated in Figure <a href="node118.htm#Figure12">12 
</a>.  
Note that the second execution has the peculiar effect that a send executed  
after the broadcast is received at another node before the broadcast.  
This example illustrates the fact that one should not rely on  
collective communication functions to have particular synchronization  
effects.  
A program that works correctly only when the first execution occurs  
(only when broadcast is synchronizing) is erroneous.  
<P> 
  
  
<P> 
<P> 
  <CENTER>  
</CENTER>  <P><IMG WIDTH=383 HEIGHT=394 SRC="coll-matchings.gif"><P>
  
    
  <BR> 
<b>Figure 12: </b><A NAME="Figure12">A race condition causes non-deterministic matching of sends
  and receives.  One cannot rely on synchronization from a broadcast
  to make the program deterministic.</a><P> 
  
    
Finally, in multithreaded implementations, one can have more than one,  
concurrently executing, collective communication call at a process.  In these  
situations, it is the user's responsibility to ensure that  
the same communicator is not used concurrently by two different  
collective communication calls at the same process.  
<P> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
Assume that broadcast is implemented using point-to-point <font face="sans-serif"> MPI</font> communication.  
Suppose the following two rules are followed.  
<ol> 
 
1. All receives specify their source explicitly (no wildcards).  
 
<BR> 
2. Each process sends all messages that pertain to one collective call before  
sending any message that pertain to a subsequent collective call.  
</ol> 
Then, messages belonging to successive broadcasts cannot be confused,  
as the order of point-to-point messages is preserved.  
<P> 
It is the implementor's responsibility to  
ensure that point-to-point messages are not confused with collective  
messages.  One way to accomplish this is, whenever a communicator is  
created, to also create a ``hidden communicator'' for collective communication.  
One could achieve a similar  
effect more cheaply, for example, by using a hidden  
tag or context bit to indicate  
whether the communicator is used for point-to-point or collective  
communication.  
 (<em> End of advice to implementors.</em>) <BR> 
  
<P> 
  

<P>
<HR>
<A HREF="node117.htm#Node117"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi22-report.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node119.htm#Node119"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi22-report.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node119.htm#Node119"> Groups, Contexts, Communicators, and Caching</a>
<b>Previous: </b><A HREF="node117.htm#Node117"> Example using <font face="sans-serif"> MPI_SCAN</font></a>
<P>
<HR>
Return to <A HREF="node434.htm">MPI-2.2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-2.2 of September 4, 2009<BR>
HTML Generated on September 10, 2009
</FONT>
</BODY>
</HTML>
