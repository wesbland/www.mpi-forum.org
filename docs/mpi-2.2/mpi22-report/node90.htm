<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml erif"> MPI-2.0</font>
-->
<TITLE>Applying Collective Operations to Intercommunicators</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node90">89. Applying Collective Operations to Intercommunicators</a></H2>
<A HREF="node89.htm#Node89"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node88.htm#Node88"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node91.htm#Node91"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node88.htm#Node88"> Communicator Argument</a>
<b>Next: </b><A HREF="node91.htm#Node91"> Specifics for Intercommunicator Collective Operations</a>
<b>Previous: </b><A HREF="node89.htm#Node89"> Specifics for Intracommunicator Collective Operations</a>
<P>
  
  
  
  
  
<P> 
  
<P> 
<P> 
  
  
  
<P> 
<P> 
To understand how collective operations apply to intercommunicators,  
we can view most <font face="sans-serif"> MPI</font> intracommunicator  
collective operations as fitting one of the following categories (see, for  
instance,   
[<a href="node433.htm#-Bib43">43</a>]):  
<dl> 
 
<dt> 
<b>All-To-All</b><dd> 
All processes contribute to the result.  All processes  
  receive the result.    
  <ul> 
   
<li><font face="sans-serif"> MPI_ALLGATHER</font>, <font face="sans-serif"> MPI_ALLGATHERV</font>  
   
<li><font face="sans-serif"> MPI_ALLTOALL</font>, <font face="sans-serif"> MPI_ALLTOALLV</font>,  
        <font face="sans-serif"> MPI_ALLTOALLW</font>  
   
<li><font face="sans-serif"> MPI_ALLREDUCE</font>, <font face="sans-serif"> MPI_REDUCE_SCATTER</font>  
<font color="red">   
<li><font face="sans-serif"> MPI_BARRIER</font>  
<font color="black">  </ul> 
<BR> 
 
<dt> 
<b>All-To-One</b><dd> 
All processes contribute to the result.  One process  
  receives the result.  
  <ul> 
   
<li><font face="sans-serif"> MPI_GATHER</font>, <font face="sans-serif"> MPI_GATHERV</font>  
   
<li><font face="sans-serif"> MPI_REDUCE</font>  
  </ul> 
<BR> 
 
<dt> 
<b>One-To-All</b><dd> 
One process contributes to the result.  All processes  
  receive the result.  
  <ul> 
   
<li><font face="sans-serif"> MPI_BCAST</font>  
   
<li><font face="sans-serif"> MPI_SCATTER</font>, <font face="sans-serif"> MPI_SCATTERV</font>  
  </ul> 
<BR> 
 
<dt> 
<b>Other</b><dd> 
Collective operations that do not fit into one of the above  
  categories.  
  <ul> 
   
<li><font face="sans-serif"> MPI_SCAN</font>, <font face="sans-serif"> MPI_EXSCAN</font>  
  
2.2  </ul> 
<BR> 
</dl> 
<BR> 
  
2.2The data movement patterns of <font face="sans-serif"> MPI_SCAN</font>   
and <font face="sans-serif"> MPI_EXSCAN</font>  
do not fit this taxonomy.  
<P> 
<P> 
The application of collective communication to  
intercommunicators is best described in terms of two groups.  
For example, an all-to-all  
<font face="sans-serif"> MPI_ALLGATHER</font> operation can be described as collecting data  
from all members of one group with the result appearing in all members  
of the other group (see Figure <a href="node90.htm#Figure2">2 
</a>).  As another  
example, a one-to-all <font face="sans-serif"> MPI_BCAST</font> operation sends data from one  
member of one group to all members of the other group.  
Collective computation operations such as <font face="sans-serif"> MPI_REDUCE_SCATTER</font> have  
a   
similar interpretation (see Figure <a href="node90.htm#Figure3">3 
</a>).  
For intracommunicators, these two groups  
are the same.  For intercommunicators, these two groups are distinct.  
For the all-to-all operations, each such operation is described in two phases,  
so that it   
has a symmetric, full-duplex behavior.  
<P> 
The following collective operations also apply to intercommunicators:  
<ul> 
 
<li><font face="sans-serif"> MPI_BARRIER</font>,  
 
<li><font face="sans-serif"> MPI_BCAST</font>,  
 
<li><font face="sans-serif"> MPI_GATHER</font>, <font face="sans-serif"> MPI_GATHERV</font>,  
 
<li><font face="sans-serif"> MPI_SCATTER</font>, <font face="sans-serif"> MPI_SCATTERV</font>,  
 
<li><font face="sans-serif"> MPI_ALLGATHER</font>, <font face="sans-serif"> MPI_ALLGATHERV</font>,  
 
<li><font face="sans-serif"> MPI_ALLTOALL</font>, <font face="sans-serif"> MPI_ALLTOALLV</font>, <font face="sans-serif"> MPI_ALLTOALLW</font>,  
 
<li><font face="sans-serif"> MPI_ALLREDUCE</font>, <font face="sans-serif"> MPI_REDUCE</font>,  
 
<li><font face="sans-serif"> MPI_REDUCE_SCATTER</font>.  
</ul> 
<BR> 
In C++, the bindings for these functions are in the <tt> MPI::Comm</tt> class.  
However,  
since the collective operations do not make  
sense on a C++ <tt> MPI::Comm</tt>  
(as  
it is neither an intercommunicator nor an intracommunicator),  
the functions are all pure virtual.  
<P> 
<P> 
  <CENTER><P><IMG WIDTH=533 HEIGHT=406 SRC="collective-allgather.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 2: </b><A NAME="Figure2">Intercommunicator allgather.  The focus of data to one process is
    represented, not mandated by the semantics. 
    The two phases do allgathers in both directions.</a><P> 
  
    
  
  <CENTER><P><IMG WIDTH=533 HEIGHT=406 SRC="collective-reduce_scatter.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 3: </b><A NAME="Figure3">Intercommunicator reduce-scatter.  The focus of data to one process
    is represented, not mandated by the semantics.
    The two phases do reduce-scatters in both directions.</a><P> 
  
    
  

<P>
<HR>
<A HREF="node89.htm#Node89"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node88.htm#Node88"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node91.htm#Node91"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node88.htm#Node88"> Communicator Argument</a>
<b>Next: </b><A HREF="node91.htm#Node91"> Specifics for Intercommunicator Collective Operations</a>
<b>Previous: </b><A HREF="node89.htm#Node89"> Specifics for Intracommunicator Collective Operations</a>
<P>
<HR>
Return to <A HREF="node434.htm">MPI-2.2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-2.2 of September 4, 2009<BR>
HTML Generated on September 10, 2009
</FONT>
</BODY>
</HTML>
