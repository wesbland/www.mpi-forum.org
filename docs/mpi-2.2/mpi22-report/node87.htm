<HTML>
<HEAD>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml erif"> MPI-2.0</font>
-->
<TITLE>Introduction and Overview</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node87">86. Introduction and Overview</a></H1>
<A HREF="node86.htm#Node86"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi22-report.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node88.htm#Node88"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi22-report.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node88.htm#Node88"> Communicator Argument</a>
<b>Previous: </b><A HREF="node86.htm#Node86"> Collective Communication</a>
<P>
  
Collective communication is defined as communication that involves  
a group  
or groups  
of processes.  
The functions of this type provided by <font face="sans-serif"> MPI</font> are the following:  
<ul> 
 
<li><font face="sans-serif"> MPI_BARRIER</font>:  
Barrier synchronization across  
all members of a group  
(Section <a href="node92.htm#Node92">Barrier Synchronization 
</a>).  
 
<li><font face="sans-serif"> MPI_BCAST</font>:  
Broadcast from one member to all members of a group  
(Section <a href="node93.htm#Node93">Broadcast 
</a>).  
This is shown  
as ``broadcast''  
in Figure <a href="node87.htm#Figure1">1 
</a>.  
 
<li><font face="sans-serif"> MPI_GATHER</font>, <font face="sans-serif"> MPI_GATHERV</font>:  
Gather data from  
all members of a group  
to one member  
(Section <a href="node95.htm#Node95">Gather 
</a>).  
This is shown  
as ``gather''  
in Figure <a href="node87.htm#Figure1">1 
</a>.  
 
<li><font face="sans-serif"> MPI_SCATTER</font>, <font face="sans-serif"> MPI_SCATTERV</font>:  
Scatter data from one member to all members of a group  
(Section <a href="node97.htm#Node97">Scatter 
</a>).  
This is shown  
as ``scatter''  
in Figure <a href="node87.htm#Figure1">1 
</a>.  
 
<li><font face="sans-serif"> MPI_ALLGATHER</font>, <font face="sans-serif"> MPI_ALLGATHERV</font>:  
A variation on Gather where all members of  
a  
group receive the result  
(Section <a href="node99.htm#Node99">Gather-to-all 
</a>).  
This is shown as ``allgather'' in Figure <a href="node87.htm#Figure1">1 
</a>.  
 
<li><font face="sans-serif"> MPI_ALLTOALL</font>, <font face="sans-serif"> MPI_ALLTOALLV</font>, <font face="sans-serif"> MPI_ALLTOALLW</font>:  
Scatter/Gather data from all members to all members of a group  
(also called complete exchange)(Section <a href="node101.htm#Node101">All-to-All Scatter/Gather 
</a>).  
This is shown as <font color="red">``complete exchange''<font color="black"> in Figure <a href="node87.htm#Figure1">1 
</a>. 
<li><font face="sans-serif"> MPI_ALLREDUCE</font>, <font face="sans-serif"> MPI_REDUCE</font>:  
Global reduction operations such as sum, max, min, or user-defined functions,  
where the result  
is returned to  
all members of a group  
and a variation where the result is  
returned to only one member  
(Section <a href="node102.htm#Node102">Global Reduction Operations 
</a>).  
 
<li><font face="sans-serif"> MPI_REDUCE_SCATTER</font>:  
A combined reduction and scatter operation  
(Section <a href="node111.htm#Node111">Reduce-Scatter 
</a>).  
 
<li><font face="sans-serif"> MPI_SCAN</font>, <font face="sans-serif"> MPI_EXSCAN</font>:  
Scan across all members of a group (also called prefix)  
(Section <a href="node114.htm#Node114">Scan 
</a>).  
</ul> 
<BR> 
  <CENTER>  
</CENTER>  <font color="red"><P><IMG WIDTH=352 HEIGHT=584 SRC="coll-fig1-22.gif"><P>
<font color="black">    
  <BR> 
<b>Figure 1: </b><A NAME="Figure1">Collective move functions illustrated
  for a group of six processes. In each case, each row of boxes
  represents data locations in one process. Thus, in the broadcast,
  initially just the first process contains the data $A_0$, but after the
  broadcast all processes contain it.</a><P> 
  
    
One of the key arguments  
in a call to a collective routine  
is a communicator that defines the group  
or groups  
of participating processes and provides a context for the operation.  
This is discussed further in Section <a href="node88.htm#Node88">Communicator Argument 
</a>.  
The syntax and semantics of the collective operations are  
defined to be consistent with the syntax and semantics of the  
point-to-point operations. Thus, general datatypes are allowed  
and must match between sending and receiving processes as specified  
in  
Chapter <a href="node68.htm#Node68">Datatypes 
</a>.  
Several collective routines such as broadcast and gather have  
a single originating or receiving process.   
Such a process is  
called the <em> root</em>.  
Some arguments in the collective functions are specified as  
``significant only at root,'' and are ignored for all  
participants except the root.  
The reader is referred to Chapter <a href="node68.htm#Node68">Datatypes 
</a>  
for information concerning communication buffers,  
general datatypes and type matching rules, and to  
Chapter <a href="node119.htm#Node119">Groups, Contexts, Communicators, and Caching 
</a> for information on how to define groups and  
create communicators.  
<P> 
The type-matching conditions for the collective operations are more  
strict than the corresponding conditions between sender and receiver  
in point-to-point.  Namely, for collective operations,  
the amount of data sent must exactly  
match the amount of data specified by the receiver.  
Different  
type maps (the layout in memory, see Section <a href="node69.htm#Node69">Derived Datatypes 
</a>)  
between sender and receiver are still allowed.  
<P> 
Collective routine calls can (but are not required to) return as soon as their  
participation in the collective communication is complete.  The completion  
of a call indicates that the caller is now free to <font color="red">modify<font color="black"> locations in thecommunication buffer.  It does not indicate that other processes in  
the group have completed or even  
started the operation (unless otherwise  
implied bythe description of the operation).Thus, a collective communication call may, or  
may not, have the effect of synchronizing all calling processes.  
This statement excludes, of course, the barrier function.  
<P> 
Collective communication calls may use the same  
communicators as point-to-point communication; <font face="sans-serif"> MPI</font> guarantees that  
messages generated on behalf of collective communication calls will not  
be confused with messages generated by point-to-point communication.  
A more detailed discussion of correct use of collective  
routines is found in Section <a href="node118.htm#Node118">Correctness 
</a>.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
The equal-data restriction (on type matching) was made so  
as to avoid the complexity  
of providing a facility analogous to the status  
argument of <font face="sans-serif"> MPI_RECV</font> for discovering the amount of data sent.  
Some of the collective routines would require an array of status values.  
<P> 
The statements about synchronization are made so as to allow a variety  
of implementations of the collective functions.  
<P> 
The collective operations do not accept a message tag argument.  
If future revisions of <font face="sans-serif"> MPI</font> define <font color="red">nonblocking<font color="black"> collective functions,then tags (or a similar mechanism)   
might   
need to be added so as  
to allow the dis-ambiguation of multiple, pending, collective operations.  
 (<em> End of rationale.</em>) <BR> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
It is dangerous to rely on synchronization  
side-effects of the collective operations for program correctness.  
For example, even though a particular implementation may  
provide a broadcast routine  
with a side-effect of synchronization, the standard does not require  
this, and a program that relies on this will not be portable.  
<P> 
On the other hand, a correct, portable program must allow for the fact  
that a collective call <em> may</em> be synchronizing.  Though one cannot  
rely on any synchronization side-effect, one must program so as to allow  
it.  These issues are discussed further in Section <a href="node118.htm#Node118">Correctness 
</a>.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
While vendors may write optimized collective routines matched to  
their architectures, a complete library of the collective communication  
routines can be written entirely using the <font face="sans-serif"> MPI</font> point-to-point communication  
functions and a few auxiliary functions.  If implementing on top of  
point-to-point, a hidden, special communicator   
might   
be created for the  
collective operation so as to avoid interference with any on-going  
point-to-point communication at the time of the collective call.  This  
is discussed further in Section <a href="node118.htm#Node118">Correctness 
</a>.  
 (<em> End of advice to implementors.</em>) <BR> 
Many of the descriptions of the collective routines provide illustrations in  
terms of blocking <font face="sans-serif"> MPI</font> point-to-point routines.  These are intended solely to   
indicate what data is sent or received by what process.  Many of these  
examples    
are <em> not</em> correct <font face="sans-serif"> MPI</font> programs; for purposes of simplicity, they often  
assume infinite buffering.  
   
  

<P>
<HR>
<A HREF="node86.htm#Node86"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="mpi22-report.htm#Node0"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node88.htm#Node88"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="mpi22-report.htm#Node0">Contents</a>
<b>Next: </b><A HREF="node88.htm#Node88"> Communicator Argument</a>
<b>Previous: </b><A HREF="node86.htm#Node86"> Collective Communication</a>
<P>
<HR>
Return to <A HREF="node434.htm">MPI-2.2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-2.2 of September 4, 2009<BR>
HTML Generated on September 10, 2009
</FONT>
</BODY>
</HTML>
