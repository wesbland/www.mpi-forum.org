<HTML>
<!-- This file was generated by tohtml from coll.tex -->
<TITLE>All-to-All Scatter/Gather</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node75">4.8. All-to-All Scatter/Gather</a></H1>
<A HREF="node74.html#Node74"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node76.html#Node76"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node76.html#Node76"> Global Reduction Operations</a>
<b>Previous: </b><A HREF="node74.html#Node74"> Examples using  MPI_ALLGATHER,  MPI_ALLGATHERV</a>
<P>
  
<P> 
    
      
      
      
      
     MPI_ALLTOALL(sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, comm)  
     
<BR> 
[  IN    sendbuf]  starting address of send buffer (choice)  
 
<BR> 
[  IN    sendcount]  number of elements sent to each process (integer)  
 
<BR> 
[  IN    sendtype]  data type of send buffer elements (handle)  
 
<BR> 
[  OUT    recvbuf]  address of receive buffer (choice)  
 
<BR> 
[  IN    recvcount]  number of elements received from any  
process (integer)  
 
<BR> 
[  IN    recvtype]  data type of receive buffer elements (handle)  
 
<BR> 
[  IN    comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Alltoall(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_ALLTOALL(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, COMM, IERROR <BR></tt>  
<P> 
 MPI_ALLTOALL is an extension of  MPI_ALLGATHER to the case  
where each process sends distinct data to each of the receivers.  
The <tt> j</tt>th block sent from process <tt> i</tt> is received by process <tt> j</tt>  
and is placed in the <tt> i</tt>th block of  recvbuf.  
<P> 
The type signature associated with  sendcount, sendtype,  
at a process must be equal to the type signature associated with  
 recvcount, recvtype at any other process.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of processes.  
As usual, however, the type maps may be different.  
<P> 
The outcome is as if each process executed a send to each  
process (itself included)  
with a call to,  
<P><IMG SRC="img119.gif"><P>
and a receive from every other process  
with a call to,  
<P><IMG SRC="img120.gif"><P>
All arguments  
on all processes are significant.  The argument  comm  
must have identical values on all processes.  
<P> 
    
      
      
      
      
     MPI_ALLTOALLV(sendbuf, sendcounts, sdispls, sendtype,  
recvbuf, recvcounts, rdispls, recvtype, comm)  
     
<BR> 
[  IN    sendbuf]  starting address of send buffer (choice)  
 
<BR> 
[  IN    sendcounts]  integer array equal to the group size  
specifying the number of elements to send to each processor  
 
<BR> 
[  IN    sdispls]  integer array (of length group size).  Entry  
<tt> j</tt> specifies the displacement (relative to  sendbuf from  
which to take the outgoing data destined for process <tt> j</tt>  
 
<BR> 
[  IN    sendtype]  data type of send buffer elements (handle)  
 
<BR> 
[  OUT    recvbuf]  address of receive buffer (choice)  
 
<BR> 
[  IN    recvcounts]  integer array equal to the group size  
specifying the number of elements that can be received from  
each processor  
 
<BR> 
[  IN    rdispls]  integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement (relative to  recvbuf at  
which to place the incoming data from process <tt> i</tt>  
 
<BR> 
[  IN    recvtype]  data type of receive buffer elements (handle)  
 
<BR> 
[  IN    comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Alltoallv(void* sendbuf, int *sendcounts, int *sdispls, MPI_Datatype sendtype, void* recvbuf, int *recvcounts, int *rdispls, MPI_Datatype recvtype, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_ALLTOALLV(SENDBUF, SENDCOUNTS, SDISPLS, SENDTYPE, RECVBUF, RECVCOUNTS, RDISPLS, RECVTYPE, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNTS(*), SDISPLS(*), SENDTYPE, RECVCOUNTS(*), RDISPLS(*), RECVTYPE, COMM, IERROR <BR></tt>  
<P> 
 MPI_ALLTOALLV adds flexibility to  MPI_ALLTOALL in that  
the location of data for the send is specified by  sdispls  
and the location of the placement of the data on the receive side  
is specified by  rdispls.  
<P> 
The <tt> j</tt>th block sent from process <tt> i</tt> is received by process <tt> j</tt>  
and is placed in the <tt> i</tt>th block of  recvbuf.  These blocks  
need not all have the same size.  
<P> 
The type signature associated with  
 sendcount[j], sendtype at process <tt> i</tt> must be equal  
to the type signature  
associated with  recvcount[i], recvtype at process <tt> j</tt>.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of processes.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
The outcome is as if each process sent a message to every other process  
with,  
<P><IMG SRC="img121.gif"><P>
and received a message from every other process with  
a call to  
<P><IMG SRC="img122.gif"><P>
All arguments  
on all processes are significant.  The argument  comm  
must have identical values on all processes.  
<P> 
 
<BR> 
[]<em> Rationale.</em>  
<P> 
The definitions of  MPI_ALLTOALL and  MPI_ALLTOALLV give as much  
flexibility as one would achieve by specifying <tt> n</tt> independent,  
point-to-point communications, with two exceptions: all messages use the same  
datatype, and messages are scattered from (or gathered to) sequential  
storage.  
 (<em> End of rationale.</em>) <BR> 
 
<BR> 
[]<em> Advice  
 to implementors.</em>  
<P> 
Although the discussion of collective communication in terms of  
point-to-point operation implies that each message is transferred directly  
from sender to receiver, implementations may use a tree communication  
pattern. Messages can be forwarded by intermediate nodes where they  
are split (for scatter) or concatenated (for gather), if this  
is more efficient.  
 (<em> End of advice to implementors.</em>) <BR> 
  

<P>
<HR>
<A HREF="node74.html#Node74"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node76.html#Node76"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node76.html#Node76"> Global Reduction Operations</a>
<b>Previous: </b><A HREF="node74.html#Node74"> Examples using  MPI_ALLGATHER,  MPI_ALLGATHERV</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
