<HTML>
<!-- This file was generated by tohtml from context.tex -->
<TITLE>Communicator Constructors</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node102">5.4.2. Communicator Constructors</a></H2>
<A HREF="node101.html#Node101"><IMG SRC="previous.gif"></A><A HREF="node100.html#Node100"><IMG SRC="up.gif"></A><A HREF="node103.html#Node103"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.html#Node100"> Communicator Management</a>
<b>Next: </b><A HREF="node103.html#Node103"> Communicator Destructors</a>
<b>Previous: </b><A HREF="node101.html#Node101"> Communicator Accessors</a>
<P>
  
The following are collective functions that are invoked by all processes in the  
group associated with  comm.  
<P> 
 
<BR> 
[]<em> Rationale.</em>  
<P> 
Note that there is a chicken-and-egg aspect to  MPI in that a  
communicator is needed to create a new communicator.  The base  
communicator for all  MPI communicators is predefined outside of  MPI,  
and is  MPI_COMM_WORLD.  This model was arrived at after  
considerable debate, and was chosen to increase ``safety'' of programs  
written in  MPI.  
 (<em> End of rationale.</em>) <BR> 
    
      
      
      
      
     MPI_COMM_DUP(comm, newcomm)  
     
<BR> 
[  IN   comm]  communicator (handle)  
 
<BR> 
[  OUT   newcomm]  copy of  comm (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Comm_dup(MPI_Comm comm, MPI_Comm *newcomm) <BR></tt>  
<P> 
 <tt> MPI_COMM_DUP(COMM, NEWCOMM, IERROR)<BR> INTEGER COMM, NEWCOMM, IERROR <BR></tt>  
<P> 
 MPI_COMM_DUP Duplicates the existing communicator  comm with  
associated key values.  For each key value, the respective copy callback  
function determines the attribute value associated with this key in the  
new communicator; one particular action that a copy callback may take  
is to delete the attribute from the new communicator.  
Returns in  newcomm a new  
communicator with the same group, any copied cached information,  
but a new context (see section <a href="node119.html#Node119">Functionality
</a>).  
<P> 
 
<BR> 
[]<em> Advice to users.</em>  
<P> 
This operation is used to provide a parallel library call with a duplicate  
communication  
space that has the same properties as the original communicator.  This  
includes any attributes (see below), and topologies (see  
chapter <a href="node127.html#Node127">Process Topologies
</a>).  This call is valid even if there are  
pending point-to-point communications involving the communicator  
 comm.  A typical call might involve a  MPI_COMM_DUP  
at the beginning of the parallel call, and an  MPI_COMM_FREE of  
that duplicated communicator at the end of the call.  Other models  
of communicator management are also possible.  
<P> 
This call applies to both intra- and inter-communicators.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
[]<em> Advice  
 to implementors.</em>  
<P> 
One need not actually copy the group information, but only add a new reference  
and increment the reference count.  Copy on write can be used for the cached  
information. (<em> End of advice to implementors.</em>) <BR> 
    
      
      
      
      
     MPI_COMM_CREATE(comm, group, newcomm)  
     
<BR> 
[  IN   comm] communicator (handle)  
 
<BR> 
[  IN   group]  Group, which is a subset of the group of  
 comm (handle)  
 
<BR> 
[  OUT   newcomm]  new communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm *newcomm) <BR></tt>  
<P> 
 <tt> MPI_COMM_CREATE(COMM, GROUP, NEWCOMM, IERROR)<BR> INTEGER COMM, GROUP, NEWCOMM, IERROR <BR></tt>  
<P> 
 This function creates a new communicator  newcomm with  
communication group defined by  group and a new context.  No cached  
information propagates from  comm to  newcomm.  The function  
returns  MPI_COMM_NULL to processes that are not in  group.  
The call is erroneous if not all  group arguments have the same value,  
or if  group is not a subset of the group associated with  
 comm.  Note that the call is to be executed by all processes in  
 comm, even if they do not belong to the new group.  
This call applies only to intra-communicators.  
<P> 
 
<BR> 
[]<em> Rationale.</em>  
<P> 
The requirement that the entire group of  comm participate in the call  
stems from the following considerations:  
<ul> 
 
<li>It allows the implementation to layer  MPI_COMM_CREATE on top of  
regular collective communications.  
 
<li>It provides additional safety, in particular in the case where partially  
overlapping groups are used to create new communicators.  
 
<li>It permits implementations sometimes to avoid communication related to context  
creation.  
</ul> 
<BR> 
 (<em> End of rationale.</em>) <BR> 
 
<BR> 
[]<em> Advice to users.</em>  
<P> 
 MPI_COMM_CREATE provides a means to subset a group of processes for the  
purpose of separate MIMD computation, with separate communication space.  
 newcomm, which emerges from  MPI_COMM_CREATE can be used in  
subsequent calls to  MPI_COMM_CREATE (or other communicator  
constructors) further to subdivide a computation into parallel  
sub-computations.  A more general service is provided by  
 MPI_COMM_SPLIT, below.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
[]<em> Advice  
 to implementors.</em>  
<P> 
Since all processes calling  MPI_COMM_DUP or  
<BR>  
 MPI_COMM_CREATE provide the same  group argument, it  
is theoretically possible to agree on a group-wide unique context with  
no communication.  However, local execution of these functions requires  
use of a larger context name space and reduces error checking.  
Implementations may strike various compromises between these  
conflicting goals, such as bulk allocation of multiple contexts in one  
collective operation.  
<P> 
Important: If new communicators are created without synchronizing the  
processes involved then the communication system should be able to cope with  
messages arriving in a context that has not yet been allocated at the  
receiving process.  
 (<em> End of advice to implementors.</em>) <BR> 
    
      
      
      
      
     MPI_COMM_SPLIT(comm, color, key, newcomm)  
     
<BR> 
[  IN   comm] communicator (handle)  
 
<BR> 
[  IN   color] control of subset assignment (integer)  
 
<BR> 
[  IN   key]  control of rank assigment (integer)  
 
<BR> 
[  OUT   newcomm]  new communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *newcomm) <BR></tt>  
<P> 
 <tt> MPI_COMM_SPLIT(COMM, COLOR, KEY, NEWCOMM, IERROR)<BR> INTEGER COMM, COLOR, KEY, NEWCOMM, IERROR <BR></tt>  
<P> 
 This function partitions the group associated with  comm  
into disjoint subgroups, one for each value of  color.  Each subgroup  
contains all processes of the same color.  Within each subgroup, the processes  
are ranked in the order defined by the value of the argument  key,  
with ties broken according to their rank in the old group.  A new communicator  
is created for each subgroup and returned in  newcomm.  A process may  
supply the color value  MPI_UNDEFINED, in which case  newcomm  
returns  MPI_COMM_NULL.  This is a collective call, but each process  
is permitted to provide different values for  color and  key.  
<P> 
A call to  MPI_COMM_CREATE(comm, group, newcomm) is equivalent to  
<BR>  
a call to  
 MPI_COMM_SPLIT(comm, color, key, newcomm),  
where all  
members of  group provide  color<I>~ =~0</i> and  
 key<I>~=~</i> rank in  
 group, and  all processes that are not members of  group provide  
 color<I>~ =~</i>  MPI_UNDEFINED.  
The function  MPI_COMM_SPLIT allows  
more general partitioning of a group  
into one or more subgroups with optional reordering.  
This call applies only intra-communicators.  
<P> 
  
The value of  color must be nonnegative.  
  
<P> 
 
<BR> 
[]<em> Advice to users.</em>  
<P> 
This is an extremely powerful mechanism for dividing a single  
communicating  group of processes into <I>k</i> subgroups, with <I>k</i> chosen  
implicitly by the user (by the number of  
colors asserted over all the processes).  Each resulting communicator will be  
non-overlapping.  Such a division could be useful for defining a hierarchy  
of computations, such as for multigrid, or linear algebra.  
<P> 
Multiple calls to  MPI_COMM_SPLIT can be used to overcome the  
requirement that any call have no overlap of the resulting communicators (each  
process is of only one color per call).  In this way, multiple overlapping  
communication structures can be created.  Creative use of the  color  
and  key in such splitting operations is encouraged.  
<P> 
Note that, for a fixed color, the keys need not  
be unique.  It is  MPI_COMM_SPLIT's responsibility to sort processes  
in ascending order according to this key, and to break ties in a consistent  
way.  If all the keys are specified in the same way, then all the processes  
in a given color will have the relative rank order as they did in their  
parent group.  (In general, they will have different ranks.)  
<P> 
Essentially, making the key value zero for all processes of a given color  
means that one doesn't really care about the rank-order of the processes in  
the new communicator.  
 (<em> End of advice to users.</em>) <BR> 
  
 
<BR> 
[]<em> Rationale.</em>  
<P> 
 color is restricted to be nonnegative, so as not to confict with the value assigned to  MPI_UNDEFINED.  
 (<em> End of rationale.</em>) <BR> 
  
<P> 

<P>
<HR>
<A HREF="node101.html#Node101"><IMG SRC="previous.gif"></A><A HREF="node100.html#Node100"><IMG SRC="up.gif"></A><A HREF="node103.html#Node103"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.html#Node100"> Communicator Management</a>
<b>Next: </b><A HREF="node103.html#Node103"> Communicator Destructors</a>
<b>Previous: </b><A HREF="node101.html#Node101"> Communicator Accessors</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
