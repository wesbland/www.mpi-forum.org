<HTML>
<!-- This file was generated by tohtml from context.tex -->
<TITLE>Communicator Accessors</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node101">5.4.1. Communicator Accessors</a></H2>
<A HREF="node100.html#Node100"><IMG SRC="previous.gif"></A><A HREF="node100.html#Node100"><IMG SRC="up.gif"></A><A HREF="node102.html#Node102"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.html#Node100"> Communicator Management</a>
<b>Next: </b><A HREF="node102.html#Node102"> Communicator Constructors</a>
<b>Previous: </b><A HREF="node100.html#Node100"> Communicator Management</a>
<P>
  
<P> 
The following are all local operations.  
<P> 
    
      
      
      
      
     MPI_COMM_SIZE(comm, size)  
     
<BR> 
[  IN   comm]  communicator (handle)  
 
<BR> 
[  OUT   size]   number of processes in the group of    
comm (integer)  
<BR> 
  
<P> 
 <tt> int MPI_Comm_size(MPI_Comm comm, int *size) <BR></tt>  
<P> 
 <tt> MPI_COMM_SIZE(COMM, SIZE, IERROR)<BR> INTEGER COMM, SIZE, IERROR <BR></tt>  
<P> 
 
<BR> 
[]<em> Rationale.</em>  
<P> 
This function is equivalent to accessing the communicator's group with  
 MPI_COMM_GROUP (see above), computing the size using  
 MPI_GROUP_SIZE,  
and then freeing the temporary group via  MPI_GROUP_FREE.  However,  
this function is so commonly used, that this shortcut was introduced.  
 (<em> End of rationale.</em>) <BR> 
 
<BR> 
[]<em> Advice to users.</em>  
<P> 
This function indicates the number of processes involved in a communicator.  
For  MPI_COMM_WORLD, it indicates the total number of processes  
available (for this version of  MPI, there is no standard way to change  
the number of processes once initialization has taken place).  
<P> 
This call is often used with the next call to determine the amount of  
concurrency available for a specific library or program.  The following  
call,  MPI_COMM_RANK indicates the rank of the process  
that calls it in the range from <IMG SRC="img147.gif">
 size<I>-1</i>, where  size  
is the return value of  MPI_COMM_SIZE. (<em> End of advice to users.</em>) <BR> 
    
      
      
      
      
     MPI_COMM_RANK(comm, rank)  
     
<BR> 
[  IN   comm]  communicator (handle)  
 
<BR> 
[  OUT   rank]   rank of the calling process in group of  
  comm (integer)  
<BR> 
  
<P> 
 <tt> int MPI_Comm_rank(MPI_Comm comm, int *rank) <BR></tt>  
<P> 
 <tt> MPI_COMM_RANK(COMM, RANK, IERROR)<BR> INTEGER COMM, RANK, IERROR <BR></tt>  
<P> 
  
 
<BR> 
[]<em> Rationale.</em>  
<P> 
This function is equivalent to accessing the communicator's group with  
 MPI_COMM_GROUP (see above), computing the rank using  
 MPI_GROUP_RANK,  
and then freeing the temporary group via  MPI_GROUP_FREE.  However,  
this function is so commonly used, that this shortcut was introduced.  
 (<em> End of rationale.</em>) <BR> 
  
<P> 
 
<BR> 
[]<em> Advice to users.</em>  
<P> 
This function gives the rank of the process in the particular communicator's  
group.  It is useful, as noted above, in conjunction with  
 MPI_COMM_SIZE.  
<P> 
Many programs will be written with the master-slave model, where one process  
(such as the rank-zero process) will play a supervisory role, and the other  
processes will serve as compute nodes.  In this framework, the two preceding  
calls are useful for determining the roles of the various processes of a  
communicator.  
 (<em> End of advice to users.</em>) <BR> 
    
      
      
      
      
     MPI_COMM_COMPARE(comm1, comm2, result)  
     
<BR> 
[  IN   comm1]  first communicator (handle)  
 
<BR> 
[  IN   comm2]  second communicator (handle)  
 
<BR> 
[  OUT   result]  result (integer)  
<BR> 
  
<P> 
 <tt> int MPI_Comm_compare(MPI_Comm comm1,MPI_Comm comm2, int *result) <BR></tt>  
<P> 
 <tt> MPI_COMM_COMPARE(COMM1, COMM2, RESULT, IERROR)<BR> INTEGER COMM1, COMM2, RESULT, IERROR <BR></tt>  
<P> 
 MPI_IDENT results if and only if  
 comm1 and  comm2 are handles for the same object  
(identical groups and same contexts).  
 MPI_CONGRUENT results if the underlying groups are identical  
in constituents and rank order; these communicators differ only by context.  
 MPI_SIMILAR results if the group members of both  
communicators are the same but the rank order  
differs.  MPI_UNEQUAL results otherwise.  
<P> 

<P>
<HR>
<A HREF="node100.html#Node100"><IMG SRC="previous.gif"></A><A HREF="node100.html#Node100"><IMG SRC="up.gif"></A><A HREF="node102.html#Node102"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node100.html#Node100"> Communicator Management</a>
<b>Next: </b><A HREF="node102.html#Node102"> Communicator Constructors</a>
<b>Previous: </b><A HREF="node100.html#Node100"> Communicator Management</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
