<HTML>
<!-- This file was generated by tohtml from coll.tex -->
<TITLE>Reduce-Scatter</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node83">4.10. Reduce-Scatter</a></H1>
<A HREF="node82.html#Node82"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node84.html#Node84"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node84.html#Node84"> Scan</a>
<b>Previous: </b><A HREF="node82.html#Node82"> All-Reduce</a>
<P>
  
<P> 
 MPI includes variants of each of the reduce operations  
where the result is scattered to all processes in the group on return.  
<P> 
    
      
      
      
      
     MPI_REDUCE_SCATTER( sendbuf, recvbuf, recvcounts,  
datatype, op, comm)  
     
<BR> 
[  IN   sendbuf]  starting address of send buffer (choice)  
 
<BR> 
[  OUT   recvbuf]  starting address of receive buffer (choice)  
 
<BR> 
[  IN   recvcounts]  integer array specifying the  
number of elements in result distributed to each process.  
Array must be identical on all calling processes.  
 
<BR> 
[  IN   datatype]  data type of elements of input buffer (handle)  
 
<BR> 
[  IN   op]  operation (handle)  
 
<BR> 
[  IN   comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Reduce_scatter(void* sendbuf, void* recvbuf, int *recvcounts, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_REDUCE_SCATTER(SENDBUF, RECVBUF, RECVCOUNTS, DATATYPE, OP, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER RECVCOUNTS(*), DATATYPE, OP, COMM, IERROR <BR></tt>  
<P> 
 MPI_REDUCE_SCATTER first does  
an element-wise reduction on  
vector of <IMG SRC="img138.gif">
 elements  
in the send buffer defined by  sendbuf, count and  
 datatype.  
Next, the resulting vector of results is split into <tt> n</tt> disjoint  
segments, where <tt> n</tt> is the number of members in the group.  
Segment <tt> i</tt> contains  recvcounts[i] elements.  
The <tt> i</tt>th segment is sent to process <tt> i</tt> and stored in the  
receive buffer defined by  recvbuf, recvcounts[i] and  
 datatype.  
<P> 
 
<BR> 
[]<em> Advice  
 to implementors.</em>  
<P> 
The  MPI_REDUCE_SCATTER  
routine is functionally equivalent to:  
A  MPI_REDUCE operation function with  count equal to  
the sum of  recvcounts[i] followed by  
 MPI_SCATTERV with  sendcounts equal to  recvcounts.  
However, a direct implementation may run faster.  
 (<em> End of advice to implementors.</em>) <BR> 

<P>
<HR>
<A HREF="node82.html#Node82"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node84.html#Node84"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node84.html#Node84"> Scan</a>
<b>Previous: </b><A HREF="node82.html#Node82"> All-Reduce</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
