<HTML>
<!-- This file was generated by tohtml from pt2pt.tex -->
<TITLE>Pack and unpack</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node62">3.13. Pack and unpack</a></H1>
<A HREF="node61.html#Node61"><IMG SRC="previous.gif"></A><A HREF="node28.html#Node28"><IMG SRC="up.gif"></A><A HREF="node63.html#Node63"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node28.html#Node28"> Point-to-Point Communication</a>
<b>Next: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Previous: </b><A HREF="node61.html#Node61"> Examples</a>
<P>
  
<P> 
Some existing communication libraries provide pack/unpack functions for sending  
noncontiguous data. In these, the user explicitly packs  
data into a contiguous buffer  
before sending it, and unpacks it from a contiguous buffer after receiving it.  
Derived datatypes, which are described in  
Section <a href="node54.html#Node54">Derived datatypes
</a>, allow one, in most cases, to avoid  
explicit packing and unpacking.  The user specifies the layout of  
the data to be sent or received, and the communication library directly  
accesses a noncontiguous buffer.  The pack/unpack routines are provided for  
compatibility with previous libraries.   Also, they provide some functionality  
that is not otherwise available in  MPI.  
For instance, a message can be received in several  
parts, where the receive operation done on  
a later part may depend on the content of a former part.  
Another use is that outgoing messages  
may be explicitly buffered in user supplied space, thus overriding the system  
buffering policy.   Finally, the availability of pack and unpack operations  
facilitates the development of additional communication libraries layered on top  
of  MPI.  
<P> 
    
      
      
      
      
     MPI_PACK(inbuf, incount, datatype, outbuf, outsize,  
position,  comm)  
     
<BR> 
[  IN   inbuf] input buffer start (choice)  
 
<BR> 
[  IN   incount] number of input data items (integer)  
 
<BR> 
[  IN   datatype] datatype of each input data item (handle)  
 
<BR> 
[  OUT   outbuf] output buffer start (choice)  
 
<BR> 
[  IN   outsize] output buffer size, in bytes (integer)  
 
<BR> 
[  INOUT   position] current position in buffer, in bytes (integer)  
 
<BR> 
[  IN   comm] communicator for packed message (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Pack(void* inbuf, int incount, MPI_Datatype datatype, void *outbuf, int outsize, int *position,  MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_PACK(INBUF, INCOUNT, DATATYPE, OUTBUF, OUTSIZE, POSITION, COMM, IERROR)<BR> &lt;type&gt; INBUF(*), OUTBUF(*)<BR>INTEGER INCOUNT, DATATYPE, OUTSIZE, POSITION,  COMM, IERROR <BR></tt>  
<P> 
Packs the message in the send buffer specified by  inbuf, incount,  
datatype into the buffer  
space specified by  outbuf and  outcount.  The  
input buffer can  
be any communication buffer allowed in  MPI_SEND.  The output buffer  
is a contiguous storage area containing  outsize bytes, starting at  
the address  outbuf (length is counted in  bytes, not elements,  
as if it were a communication buffer for a message of type  MPI_PACKED).  
<P> 
The input value of  position is the first  
location in the output buffer to be used for packing.   position is  
incremented by the size of the packed message, and the output value of  
 position is the first location in the output buffer  
following the locations occupied by the packed message.  
The  comm argument  
is the communicator that will be subsequently used for sending the packed  
message.  
<P> 
    
      
      
      
      
     MPI_UNPACK(inbuf, insize, position, outbuf, outcount,  
datatype, comm)  
     
<BR> 
[  IN   inbuf] input buffer start (choice)  
 
<BR> 
[  IN   insize] size of input buffer, in bytes (integer)  
 
<BR> 
[  INOUT   position] current position in bytes (integer)  
 
<BR> 
[  OUT   outbuf] output buffer start (choice)  
 
<BR> 
[  IN   outcount] number of items to be unpacked (integer)  
 
<BR> 
[  IN   datatype] datatype of each output data item (handle)  
 
<BR> 
[  IN   comm] communicator for packed message (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Unpack(void* inbuf, int insize, int *position, void *outbuf, int outcount, MPI_Datatype datatype, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_UNPACK(INBUF, INSIZE, POSITION, OUTBUF, OUTCOUNT, DATATYPE, COMM, IERROR)<BR>  &lt;type&gt; INBUF(*), OUTBUF(*) <BR>INTEGER INSIZE, POSITION, OUTCOUNT, DATATYPE, COMM, IERROR <BR></tt>  
<P> 
Unpacks a message into the receive buffer specified by  outbuf,  
outcount,  
datatype from the buffer  
space specified by  inbuf and  insize.  The output buffer can  
be any communication buffer allowed in  MPI_RECV.  The input  
buffer is a contiguous storage area containing  insize bytes,  
starting at address  inbuf.  
  
The input value of  position is the first location in  
the input buffer occupied by the packed message.  
 position is incremented  
by the size of the packed message, so that the output value of  
 position is the first location in the input buffer  
  
after the locations occupied by the message that was unpacked.  
 comm is the communicator used to receive the packed message.  
<P> 
 
<BR> 
[]<em> Advice to users.</em>  
<P> 
Note the difference between  MPI_RECV and  MPI_UNPACK:  in  
 MPI_RECV, the  count argument specifies the maximum  
number of  
items that can be received.  The actual number of items received is determined  
by the length of the incoming message.  In  MPI_UNPACK, the  
 count argument specifies the actual number  
of items that are unpacked;  
the ``size'' of the corresponding message is the increment in  
 position.  
The reason for this change is that the ``incoming message size'' is not  
predetermined since the user decides how much to unpack; nor is it easy to  
determine  
the ``message size'' from the number of items to be unpacked.  In fact, in a  
heterogeneous system, this number may not be determined <em> a priori</em>.  
 (<em> End of advice to users.</em>) <BR> 
To understand the behavior of pack and unpack, it is convenient to think of the  
data part of a message as being the sequence obtained by concatenating the  
successive values sent in that message.   The pack operation stores this  
sequence in the buffer space, as if sending the message to that buffer.  The  
unpack operation retrieves this sequence from buffer space, as if receiving a  
message from that buffer.  (It is helpful to think of internal Fortran files or  
 sscanf in C, for a similar function.)  
<P> 
Several messages can be successively packed into one <b> packing unit</b>.  This  
is effected by several successive <b> related</b> calls to  MPI_PACK,  
where the first  
call provides  position = 0, and each successive call inputs the value  
of  position that was output by the previous call, and the same values  
for  outbuf, outcount and  comm.   This packing unit  
now contains  
the equivalent information that would have been stored in a message by one send  
call with a send buffer that is the ``concatenation'' of the individual send  
buffers.  
<P> 
A  packing unit can  
be sent using type  MPI_PACKED.  Any point to point  
or collective communication function can be used to move the sequence of bytes  
that forms the packing unit from one process to another.  This packing unit  
can now be  
received using any receive operation, with any datatype:  the  
type matching rules are relaxed for messages sent with type  MPI_PACKED.  
<P> 
A message sent with any type (including  MPI_PACKED) can be  
received using the type  MPI_PACKED.  Such a message can then be  
unpacked by calls to  MPI_UNPACK.  
<P> 
A packing unit (or a message created by a regular, ``typed'' send)  
can be unpacked into several successive messages.  This is  
effected by several successive related calls to  MPI_UNPACK, where  
the first  
call provides  position = 0, and each successive call inputs the value  
of  position that was output by the previous call, and the same values  
for  inbuf, insize and  comm.  
<P> 
The concatenation of two packing units is not necessarily a packing unit; nor is  
a substring of a packing unit necessarily a packing unit.  Thus, one cannot  
concatenate two packing units and then unpack the result as one packing  
unit; nor can one unpack a substring of a packing unit as a separate  
packing unit.  Each packing unit, that was created by a related  
sequence of pack  
calls, or by a regular send,  
must be unpacked as a unit, by a sequence of related unpack calls.  
<P> 
 
<BR> 
[]<em> Rationale.</em>  
<P> 
The restriction on ``atomic'' packing and unpacking of packing units allows  
the implementation to add at the head of packing units additional  
information, such as  
a description of the sender architecture (to be used for type conversion, in a  
heterogeneous environment)  
 (<em> End of rationale.</em>) <BR> 
The following call allows the user to find out how much space is  
needed to pack a message and, thus, manage space allocation for  
buffers.  
<P> 
    
      
      
      
      
     MPI_PACK_SIZE(incount, datatype, comm, size)  
     
<BR> 
[  IN   incount] count argument to packing call (integer)  
 
<BR> 
[  IN   datatype] datatype argument to packing call (handle)  
 
<BR> 
[  IN   comm] communicator argument to packing call (handle)  
 
<BR> 
[  OUT   size] upper bound on size of packed message, in bytes (integer)  
<BR> 
  
<P> 
 <tt> int MPI_Pack_size(int incount, MPI_Datatype datatype, MPI_Comm comm, int *size) <BR></tt>  
<P> 
 <tt> MPI_PACK_SIZE(INCOUNT, DATATYPE, COMM, SIZE, IERROR)<BR> INTEGER INCOUNT, DATATYPE, COMM, SIZE, IERROR <BR></tt>  
<P> 
A call to  MPI_PACK_SIZE(incount, datatype, comm, size)  
returns in  size an upper bound on the increment in  position  
that is effected by a call to  MPI_PACK(inbuf, incount, datatype,  
outbuf, outcount, position, comm).  
<P> 
 
<BR> 
[]<em> Rationale.</em>  
<P> 
The call returns an upper bound, rather than an exact bound, since the  
exact amount of space needed to pack the message may depend on the  
context (e.g., first message packed in a packing unit may take more  
space).  
 (<em> End of rationale.</em>) <BR> 
<BR><b> Example</b>   
  
An example using  MPI_PACK.  
<BR> 
<pre><tt>int position, i, j, a[2]; 
char buff[1000]; 
<P> 
.... 
<P> 
MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank); 
if (myrank == 0) 
{ 
   / * SENDER CODE */ 
<P> 
position = 0; 
  MPI_Pack(&amp;i, 1, MPI_INT, buff, 1000, &amp;position, MPI_COMM_WORLD); 
  MPI_Pack(&amp;j, 1, MPI_INT, buff, 1000, &amp;position, MPI_COMM_WORLD); 
  MPI_Send( buff, position, MPI_PACKED, 1, 0, MPI_COMM_WORLD); 
} 
else  /* RECEIVER CODE */ 
  MPI_Recv( a, 2, MPI_INT, 0, 0, MPI_COMM_WORLD) 
<P> 
} 
</tt></pre> 
   
<P> 
<BR><b> Example</b>   
  
A elaborate example.  
<BR> 
<pre><tt>int position, i; 
float a[1000]; 
char buff[1000] 
<P> 
.... 
<P> 
MPI_Comm_rank(MPI_Comm_world, &amp;myrank); 
if (myrank == 0) 
{ 
  / * SENDER CODE */ 
<P> 
int len[2]; 
  MPI_Aint disp[2]; 
  MPI_Datatype type[2], newtype; 
<P> 
/* build datatype for i followed by a[0]...a[i-1] */ 
<P> 
len[0] = 1; 
  len[1] = i; 
  MPI_Address( &amp;i, disp); 
  MPI_Address( a, disp+1); 
  type[0] = MPI_INT; 
  type[1] = MPI_FLOAT; 
  MPI_Type_struct( 2, len, disp, type, &amp;newtype); 
  MPI_Type_commit( &amp;newtype); 
<P> 
/* Pack i followed by a[0]...a[i-1]*/ 
<P> 
position = 0; 
  MPI_Pack( MPI_BOTTOM, 1, newtype, buff, 1000, &amp;position, MPI_COMM_WORLD); 
<P> 
/* Send */ 
<P> 
MPI_Send( buff, position, MPI_PACKED, 1, 0, 
            MPI_COMM_WORLD) 
<P> 
/* ***** 
   One can replace the last three lines with 
   MPI_Send( MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD); 
   ***** */ 
} 
else /* myrank == 1 */ 
{ 
   /* RECEIVER CODE */ 
<P> 
MPI_Status status; 
<P> 
/* Receive */ 
<P> 
MPI_Recv( buff, 1000, MPI_PACKED, 0, 0, &amp;status); 
<P> 
/* Unpack i */ 
<P> 
position = 0; 
 MPI_Unpack(buff, 1000, &amp;position, &amp;i, 1, MPI_INT, MPI_COMM_WORLD); 
<P> 
/* Unpack a[0]...a[i-1] */ 
 MPI_Unpack(buff, 1000, &amp;position, a, i, MPI_FLOAT, MPI_COMM_WORLD); 
} 
</tt></pre> 
   
<P> 
<BR><b> Example</b>   
  
Each process sends a count, followed by count characters to the root;  
the root concatenate all characters into one string.  
<BR> 
<pre><tt>int count, gsize, counts[64], totalcount, k1, k2, k, 
    displs[64], position, concat_pos; 
char chr[100], *lbuf, *rbuf, *cbuf; 
... 
MPI_Comm_size(comm, &amp;gsize); 
MPI_Comm_rank(comm, &amp;myrank); 
<P> 
/* allocate local pack buffer */ 
MPI_Pack_size(1, MPI_INT, comm, &amp;k1); 
MPI_Pack_size(count, MPI_CHAR, &amp;k2); 
k = k1+k2; 
lbuf = (char *)malloc(k); 
<P> 
/* pack count, followed by count characters */ 
position = 0; 
MPI_Pack(&amp;count, 1, MPI_INT, lbuf, k, &amp;position, comm); 
MPI_Pack(chr, count, MPI_CHAR, &amp;lbuf, k, &amp;position, comm); 
<P> 
if (myrank != root) 
      /* gather at root sizes of all packed messages */ 
   MPI_Gather( &amp;position, 1, MPI_INT, NULL, NULL, 
             NULL, root, comm); 
<P> 
/* gather at root packed messages */ 
  MPI_Gatherv( &amp;buf, position, MPI_PACKED, NULL, 
           NULL, NULL, NULL, root, comm); 
<P> 
else {   /* root code */ 
      /* gather sizes of all packed messages */ 
   MPI_Gather( &amp;position, 1, MPI_INT, counts, 1, 
             MPI_INT, root, comm); 
<P> 
/* gather all packed messages */ 
  displs[0] = 0; 
  for (i=1; i &lt; gsize; i++) 
    displs[i] = displs[i-1] + counts[i-1]; 
  totalcount = dipls[gsize-1] + counts[gsize-1]; 
  rbuf = (char *)malloc(totalcount); 
  cbuf = (char *)malloc(totalcount); 
  MPI_Gatherv( lbuf, position, MPI_PACKED, rbuf, 
           counts, displs, MPI_PACKED, root, comm); 
<P> 
/* unpack all messages and concatenate strings */ 
  concat_pos = 0; 
  for (i=0; i &lt; gsize; i++) { 
    position = 0; 
    MPI_Unpack( rbuf+displs[i], totalcount-displs[i], 
          &amp;position, &amp;count, 1, MPI_INT, comm); 
    MPI_Unpack( rbuf+displs[i], totalcount-displs[i], 
          &amp;position, cbuf+concat_pos, count, MPI_CHAR, comm); 
    concat_pos += count; 
    } 
  cbuf[concat_pos] = `\0'; 
  } 
</tt></pre> 
  
  
<P> 
<P> 

<P>
<HR>
<A HREF="node61.html#Node61"><IMG SRC="previous.gif"></A><A HREF="node28.html#Node28"><IMG SRC="up.gif"></A><A HREF="node63.html#Node63"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node28.html#Node28"> Point-to-Point Communication</a>
<b>Next: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Previous: </b><A HREF="node61.html#Node61"> Examples</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
