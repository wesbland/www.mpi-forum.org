<HTML>
<!-- This file was generated by tohtml from coll.tex -->
<TITLE>Examples using  MPI_SCATTER,  MPI_SCATTERV</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node72">4.6.1. Examples using  MPI_SCATTER,  MPI_SCATTERV</a></H2>
<A HREF="node71.html#Node71"><IMG SRC="previous.gif"></A><A HREF="node71.html#Node71"><IMG SRC="up.gif"></A><A HREF="node73.html#Node73"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node71.html#Node71"> Scatter</a>
<b>Next: </b><A HREF="node73.html#Node73"> Gather-to-all</a>
<b>Previous: </b><A HREF="node71.html#Node71"> Scatter</a>
<P>
<BR><b> Example</b>   
  
<P> 
The reverse of Example <a href="node70.html#Node70">Examples using  MPI_GATHER,  MPI_GATHERV
</a>.  
Scatter sets of 100 ints from the root to each process in the group.  
See figure <a href="node72.html#Figure8">8
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,*sendbuf; 
    int root, rbuf[100]; 
    ... 
    MPI_Comm_size( comm, &amp;gsize); 
    sendbuf = (int *)malloc(gsize*100*sizeof(int)); 
    ... 
    MPI_Scatter( sendbuf, 100, MPI_INT, rbuf, 100, MPI_INT, root, comm); 
</tt></pre> 
   
<P> 
<P><IMG SRC="mycoll-fig7.gif"><P>
 
  
    
  <BR> 
<b>Figure 8: </b><A NAME="Figure8">The root process scatters sets of 100 <tt> int</tt>s to each process
  in the group.
  </a><P> 
  
    
<BR><b> Example</b>   
  
<P> 
The reverse of Example <a href="node70.html#Node70">Examples using  MPI_GATHER,  MPI_GATHERV
</a>.  
The root process scatters sets of 100 ints to the other processes,  
but the sets of 100 are <em> stride</em> ints apart in the sending buffer.  
Requires use of  MPI_SCATTERV.  
Assume <IMG SRC="img117.gif">
.  See figure <a href="node72.html#Figure9">9
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,*sendbuf; 
    int root, rbuf[100], i, *displs, *scounts; 
<P> 
... 
<P> 
MPI_Comm_size( comm, &amp;gsize); 
    sendbuf = (int *)malloc(gsize*stride*sizeof(int)); 
    ... 
    displs = (int *)malloc(gsize*sizeof(int)); 
    scounts = (int *)malloc(gsize*sizeof(int)); 
    for (i=0; i&lt;gsize; ++i) { 
        displs[i] = i*stride; 
        scounts[i] = 100; 
    } 
    MPI_Scatterv( sendbuf, scounts, displs, MPI_INT, rbuf, 100, MPI_INT, 
                                                              root, comm); 
</tt></pre> 
   
<P> 
<P><IMG SRC="mycoll-fig8.gif"><P>
 
  
    
  <BR> 
<b>Figure 9: </b><A NAME="Figure9">The root process scatters sets of 100 <tt> int</tt>s, moving by
  <tt> stride</tt> ints from send to send in the scatter.
  </a><P> 
  
    
<BR><b> Example</b>   
  
<P> 
The reverse of Example <a href="node70.html#Node70">Examples using  MPI_GATHER,  MPI_GATHERV
</a>.  
We have a varying stride between blocks at sending (root) side,  
at the receiving side we receive into the ith column of a 100<IMG SRC="img118.gif">
150  
C array.  
See figure <a href="node72.html#Figure10">10
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,recvarray[100][150],*rptr; 
    int root, *sendbuf, myrank, bufsize, *stride; 
    MPI_Datatype rtype; 
    int i, *displs, *scounts, offset; 
    ... 
    MPI_Comm_size( comm, &amp;gsize); 
    MPI_Comm_rank( comm, &amp;myrank ); 
<P> 
stride = (int *)malloc(gsize*sizeof(int)); 
    ... 
    /* stride[i] for i = 0 to gsize-1 is set somehow 
     * sendbuf comes from elsewhere 
     */ 
    ... 
    displs = (int *)malloc(gsize*sizeof(int)); 
    scounts = (int *)malloc(gsize*sizeof(int)); 
    offset = 0; 
    for (i=0; i&lt;gsize; ++i) { 
        displs[i] = offset; 
        offset += stride[i]; 
        scounts[i] = 100 - i; 
    } 
    /* Create datatype for the column we are receiving 
     */ 
    MPI_Type_vector( 100-myrank, 1, 150, MPI_INT, &amp;rtype); 
    MPI_Type_commit( &amp;rtype ); 
    rptr = &amp;recvarray[0][myrank]; 
    MPI_Scatterv( sendbuf, scounts, displs, MPI_INT, rptr, 1, rtype, 
                                                            root, comm); 
<P> 
</tt></pre> 
   
<P> 
<P><IMG SRC="mycoll-fig9.gif"><P>
 
  
    
  <BR> 
<b>Figure 10: </b><A NAME="Figure10">The root scatters blocks of 100-i ints into
  column <tt> i</tt> of a 100$ x $150
  C array.  At the sending side, the blocks are <tt> stride[i]</tt> ints apart.
  </a><P> 
  
    

<P>
<HR>
<A HREF="node71.html#Node71"><IMG SRC="previous.gif"></A><A HREF="node71.html#Node71"><IMG SRC="up.gif"></A><A HREF="node73.html#Node73"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node71.html#Node71"> Scatter</a>
<b>Next: </b><A HREF="node73.html#Node73"> Gather-to-all</a>
<b>Previous: </b><A HREF="node71.html#Node71"> Scatter</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
