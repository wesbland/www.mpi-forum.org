<HTML>
<!-- This file was generated by tohtml from intro.tex -->
<TITLE>Overview and Goals</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node2">1.1. Overview and Goals</a></H1>
<A HREF="node1.html#Node1"><IMG SRC="previous.gif"></A><A HREF="node1.html#Node1"><IMG SRC="up.gif"></A><A HREF="node3.html#Node3"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node1.html#Node1"> Introduction to MPI</a>
<b>Next: </b><A HREF="node3.html#Node3"> Who Should Use This Standard?</a>
<b>Previous: </b><A HREF="node1.html#Node1"> Introduction to MPI</a>
<P>
Message passing is a paradigm used  
widely on certain classes of parallel machines, especially those with  
distributed memory.  
Although there are many variations, the basic  
concept of processes communicating through messages is well understood.  
Over the last ten years, substantial progress has been made in casting  
significant applications in this paradigm.  Each vendor  
has implemented its own variant.  
More recently, several systems have demonstrated  
that a message passing system can be efficiently and  
portably implemented.  
It is thus an appropriate time to try to define both the syntax  
and semantics of a core of library routines that will be useful to a wide  
range of users and efficiently implementable on a wide range of computers.  
<P> 
In designing  MPI we have sought to make use of the most attractive features  
of a number of existing message passing systems, rather than selecting one of  
them and adopting it as the standard. Thus,  MPI has been strongly influenced  
by work at the IBM T. J. Watson Research Center  
[<a href="node166.html#-Bib1">1</a>,<a href="node166.html#-Bib2">2</a>], Intel's NX/2 [<a href="node166.html#-Bib23">23</a>], Express  
[<a href="node166.html#-Bib22">22</a>], nCUBE's Vertex [<a href="node166.html#-Bib21">21</a>], p4 [<a href="node166.html#-Bib7">7</a>,<a href="node166.html#-Bib6">6</a>], and  
PARMACS [<a href="node166.html#-Bib5">5</a>,<a href="node166.html#-Bib8">8</a>].  Other important contributions have come  
from Zipcode [<a href="node166.html#-Bib24">24</a>,<a href="node166.html#-Bib25">25</a>], Chimp [<a href="node166.html#-Bib14">14</a>,<a href="node166.html#-Bib15">15</a>], PVM  
[<a href="node166.html#-Bib4">4</a>,<a href="node166.html#-Bib11">11</a>], Chameleon [<a href="node166.html#-Bib19">19</a>], and PICL [<a href="node166.html#-Bib18">18</a>].  
<P> 
The  MPI standardization effort involved about 60 people from 40 organizations  
mainly from the United States and Europe. Most of the major vendors of  
concurrent computers were involved in  MPI, along with researchers from  
universities, government laboratories, and industry. The standardization  
process began with the Workshop on  
Standards for Message Passing in a Distributed Memory Environment,   
sponsored by the Center for Research on Parallel Computing, held April 29-30,  
1992, in Williamsburg, Virginia  
[<a href="node166.html#-Bib29">29</a>]. At this workshop the basic features essential  
to a standard message passing interface were discussed, and a working group  
established to continue the standardization process.  
<P> 
A preliminary draft proposal, known as MPI1, was put forward by Dongarra,  
Hempel, Hey, and Walker in November 1992, and a revised version was completed  
in February 1993 [<a href="node166.html#-Bib12">12</a>]. MPI1 embodied the main features that were  
identified at the Williamsburg workshop as being necessary in a message  
passing standard.  Since MPI1 was primarily intended to promote discussion  
and ``get the ball rolling,'' it focused mainly on point-to-point  
communications.  MPI1 brought to the forefront a number of important  
standardization issues, but did not include any collective communication  
routines and was not thread-safe.  
<P> 
In November 1992, a  
meeting of the  MPI working group was held in Minneapolis, at which it was  
decided to place the standardization process on a more formal footing, and  
to generally adopt the procedures and organization of the High Performance  
Fortran Forum. Subcommittees were formed for the major  
component areas of the standard, and an email discussion service established  
for each. In addition, the goal of producing a draft  MPI standard by the Fall   
of 1993 was set.   
To achieve this goal the  MPI working group met every  
6 weeks for two days throughout the first 9 months of 1993, and   
presented the draft  MPI standard at the  
Supercomputing 93 conference in November 1993. These meetings and the email  
discussion together constituted the  MPI Forum, membership of which has been open  
to all members of the high performance computing community.  
<P> 
The main advantages of establishing a message-passing standard are portability  
and ease-of-use. In a distributed memory communication environment in which  
the higher level routines and/or abstractions are build upon lower level  
message passing routines the benefits of standardization are particularly  
apparent. Furthermore, the definition of a message passing standard, such  
as that proposed here, provides vendors with a clearly defined base set of  
routines that they can implement efficiently, or in some cases provide  
hardware support for, thereby enhancing scalability.  
<P> 
The goal of the Message Passing Interface simply stated is to  
develop a widely used  
standard for writing message-passing programs.  
As such the interface should  
establish a practical, portable, efficient, and flexible standard for message  
passing.  
<P> 
A complete list of goals follows.  
<P> 
<ul> 
 
<li>Design an application programming interface (not necessarily for compilers  
or a system implementation library).  
<P> 
 
<li>Allow efficient communication: Avoid memory-to-memory copying  
and allow overlap of computation and communication and offload to communication  
co-processor, where available.  
<P> 
 
<li>Allow for implementations that can be used in a heterogeneous environment.  
<P> 
 
<li>Allow convenient C and Fortran 77 bindings for the interface.  
<P> 
 
<li>Assume a reliable communication interface:  
the user need not cope with communication failures.  
Such failures are dealt with by the underlying communication subsystem.  
<P> 
 
<li>Define an interface that is not too different from current practice,  
such as PVM, NX, Express, p4, etc., and provides extensions that allow   
greater flexibility.  
<P> 
 
<li>Define an interface that can be implemented on many  
vendor's platforms, with no significant changes in the underlying  
communication and system software.  
<P> 
 
<li>Semantics of the interface should be language independent.  
<P> 
 
<li>The interface should be designed to allow for thread-safety.  
</ul> 
<BR> 

<P>
<HR>
<A HREF="node1.html#Node1"><IMG SRC="previous.gif"></A><A HREF="node1.html#Node1"><IMG SRC="up.gif"></A><A HREF="node3.html#Node3"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node1.html#Node1"> Introduction to MPI</a>
<b>Next: </b><A HREF="node3.html#Node3"> Who Should Use This Standard?</a>
<b>Previous: </b><A HREF="node1.html#Node1"> Introduction to MPI</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
