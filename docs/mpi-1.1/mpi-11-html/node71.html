<HTML>
<!-- This file was generated by tohtml from coll.tex -->
<TITLE>Scatter</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node71">4.6. Scatter</a></H1>
<A HREF="node70.html#Node70"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node72.html#Node72"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node72.html#Node72"> Examples using  MPI_SCATTER,  MPI_SCATTERV</a>
<b>Previous: </b><A HREF="node70.html#Node70"> Examples using  MPI_GATHER,  MPI_GATHERV</a>
<P>
  
<P> 
    
      
      
      
      
     MPI_SCATTER( sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, root, comm)  
     
<BR> 
[  IN    sendbuf]  address of send buffer (choice, significant  
only at root)  
 
<BR> 
[  IN    sendcount]  number of elements sent to each process  
(integer, significant only at root)  
 
<BR> 
[  IN    sendtype]  data type of send buffer elements  
(significant only at root) (handle)  
 
<BR> 
[  OUT    recvbuf]  address of receive buffer (choice)  
 
<BR> 
[  IN    recvcount]  number of elements in receive buffer (integer)  
 
<BR> 
[  IN    recvtype]  data type of receive buffer elements (handle)  
 
<BR> 
[  IN    root]   rank of sending process (integer)  
 
<BR> 
[  IN    comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Scatter(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_SCATTER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR <BR></tt>  
<P> 
  MPI_SCATTER is the inverse operation to  MPI_GATHER.  
<P> 
The outcome is <em> as if</em> the root executed <tt> n</tt> send operations,  
<P><IMG SRC="img112.gif"><P>
and each process executed a receive,  
<P><IMG SRC="img113.gif"><P>
An alternative description is that the root sends a message with  
 MPI_Send(sendbuf, sendcount<IMG SRC="img114.gif">
n, sendtype, ...). This  
message is split into <tt> n</tt> equal segments, the <I>i</i>th segment is  
sent to the <I>i</i>th process in the group, and each process receives  
this message as above.  
<P> 
The send buffer is ignored for all non-root processes.  
<P> 
The type signature associated with  sendcount, sendtype at the root  
must be equal to the type signature associated with  
 recvcount, recvtype at all  
processes (however, the type maps may be different).  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between each process and the root.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
All arguments to the function are significant on process  root,  
while on other processes, only arguments  recvbuf, recvcount,  
recvtype, root, comm are significant.  
The arguments  root and  comm  
must have identical values on all processes.  
<P> 
The specification of counts and types  
should not cause any location on the root to be read more than  
once.  
<P> 
 
<BR> 
[]<em> Rationale.</em>  
<P> 
Though not needed, the last restriction is imposed so as  
to achieve symmetry with  
 MPI_GATHER, where the corresponding restriction (a multiple-write  
restriction) is necessary.  
 (<em> End of rationale.</em>) <BR> 
    
      
      
      
      
     MPI_SCATTERV( sendbuf, sendcounts, displs, sendtype,  
recvbuf, recvcount, recvtype, root, comm)  
     
<BR> 
[  IN    sendbuf]  address of send buffer (choice, significant  
only at root)  
 
<BR> 
[  IN    sendcounts]  integer array (of length group size)  
specifying the number of elements to send to each processor   
 
<BR> 
[  IN    displs]  integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement (relative to  sendbuf from  
which to take the outgoing data to process <tt> i</tt>  
 
<BR> 
[  IN    sendtype]  data type of send buffer elements (handle)  
 
<BR> 
[  OUT    recvbuf]  address of receive buffer (choice)  
 
<BR> 
[  IN    recvcount]  number of elements in receive buffer (integer)  
 
<BR> 
[  IN    recvtype]  data type of receive buffer elements (handle)  
 
<BR> 
[  IN    root]   rank of sending process (integer)  
 
<BR> 
[  IN    comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Scatterv(void* sendbuf, int *sendcounts, int *displs, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_SCATTERV(SENDBUF, SENDCOUNTS, DISPLS, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNTS(*), DISPLS(*), SENDTYPE, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR <BR></tt>  
<P> 
  MPI_SCATTERV is the inverse operation to  MPI_GATHERV.  
<P> 
 MPI_SCATTERV extends the functionality of  MPI_SCATTER  
by allowing a varying count of data to be sent to each process,  
since  sendcounts is now an array.  
It also allows more flexibility as to where the data  
is taken from on the root, by providing the new argument,  displs.  
<P> 
The outcome is as if the root executed <tt> n</tt> send operations,  
<P><IMG SRC="img115.gif"><P>
and each process executed a receive,  
<P><IMG SRC="img116.gif"><P>
The send buffer is ignored for all non-root processes.  
<P> 
The type signature implied by  sendcount[i], sendtype at the root  
must be equal to the type signature implied by  
 recvcount, recvtype at process  
<tt> i</tt> (however, the type maps may be different).  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between each process and the root.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
All arguments to the function are significant on process  root,  
while on other processes, only arguments  recvbuf, recvcount,  
recvtype, root, comm are significant.  
The arguments  root and  comm  
must have identical values on all processes.  
<P> 
The specification of counts, types, and displacements  
should not cause any location on the root to be read more than  
once.  
<P> 
<menu> 
</menu> 

<P>
<HR>
<A HREF="node70.html#Node70"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node72.html#Node72"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node72.html#Node72"> Examples using  MPI_SCATTER,  MPI_SCATTERV</a>
<b>Previous: </b><A HREF="node70.html#Node70"> Examples using  MPI_GATHER,  MPI_GATHERV</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
