<HTML>
<!-- This file was generated by tohtml from coll.tex -->
<TITLE>Examples using  MPI_GATHER,  MPI_GATHERV</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node70">4.5.1. Examples using  MPI_GATHER,  MPI_GATHERV</a></H2>
<A HREF="node69.html#Node69"><IMG SRC="previous.gif"></A><A HREF="node69.html#Node69"><IMG SRC="up.gif"></A><A HREF="node71.html#Node71"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node69.html#Node69"> Gather</a>
<b>Next: </b><A HREF="node71.html#Node71"> Scatter</a>
<b>Previous: </b><A HREF="node69.html#Node69"> Gather</a>
<P>
<BR><b> Example</b>   
  
<P> 
Gather 100 ints from every process in group to root. See figure  
<a href="node70.html#Figure3">3
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100]; 
    int root, *rbuf; 
    ... 
    MPI_Comm_size( comm, &amp;gsize); 
    rbuf = (int *)malloc(gsize*100*sizeof(int)); 
    MPI_Gather( sendarray, 100, MPI_INT, rbuf, 100, MPI_INT, root, comm); 
</tt></pre> 
   
<P> 
<BR><b> Example</b>   
  
<P> 
Previous example modified -- only the root allocates memory for the  
receive buffer.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100]; 
    int root, myrank, *rbuf; 
    ... 
    MPI_Comm_rank( comm, myrank); 
    if ( myrank == root) { 
       MPI_Comm_size( comm, &amp;gsize); 
       rbuf = (int *)malloc(gsize*100*sizeof(int)); 
       } 
    MPI_Gather( sendarray, 100, MPI_INT, rbuf, 100, MPI_INT, root, comm); 
</tt></pre> 
   
<P> 
<P><IMG SRC="mycoll-fig2.gif"><P>
 
  
    
  <BR> 
<b>Figure 3: </b><A NAME="Figure3">The root process gathers 100 <tt> int</tt>s from each process
  in the group.
  </a><P> 
  
    
<BR><b> Example</b>   
  
<P> 
Do the same as the previous example, but use a derived datatype.  Note that  
the type cannot be the entire set of <tt> gsize*100 int</tt>s since type matching  
is defined pairwise between the root and each process in the gather.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100]; 
    int root, *rbuf; 
    MPI_Datatype rtype; 
    ... 
    MPI_Comm_size( comm, &amp;gsize); 
    MPI_Type_contiguous( 100, MPI_INT, &amp;rtype ); 
    MPI_Type_commit( &amp;rtype ); 
    rbuf = (int *)malloc(gsize*100*sizeof(int)); 
    MPI_Gather( sendarray, 100, MPI_INT, rbuf, 1, rtype, root, comm); 
</tt></pre> 
   
<P> 
<BR><b> Example</b>   
  
<P> 
Now have each process send 100 ints to root, but place each set (of 100)  
<em> stride</em> ints apart at receiving end. Use  MPI_GATHERV  
and the  displs  
argument to achieve this effect. Assume <IMG SRC="img108.gif">
.  
See figure <a href="node70.html#Figure4">4
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100]; 
    int root, *rbuf, stride; 
    int *displs,i,*rcounts; 
<P> 
... 
<P> 
MPI_Comm_size( comm, &amp;gsize); 
    rbuf = (int *)malloc(gsize*stride*sizeof(int)); 
    displs = (int *)malloc(gsize*sizeof(int)); 
    rcounts = (int *)malloc(gsize*sizeof(int)); 
    for (i=0; i&lt;gsize; ++i) { 
        displs[i] = i*stride; 
        rcounts[i] = 100; 
    } 
    MPI_Gatherv( sendarray, 100, MPI_INT, rbuf, rcounts, displs, MPI_INT, 
                                                               root, comm); 
</tt></pre> 
Note that the program is erroneous if <I>stride &lt; 100</i>.  
   
<P> 
<P><IMG SRC="mycoll-fig3.gif"><P>
 
  
    
  <BR> 
<b>Figure 4: </b><A NAME="Figure4">The root process gathers 100 <tt> int</tt>s from each process
  in the group, each set is placed <tt> stride</tt> ints apart.
  </a><P> 
  
    
<BR><b> Example</b>   
  
<P> 
Same as Example <a href="node70.html#Node70">Examples using  MPI_GATHER,  MPI_GATHERV
</a> on the receiving side, but send the  
100 ints from the 0th column of a  
100<IMG SRC="img109.gif">
150 int array, in C.  See figure <a href="node70.html#Figure5">5
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100][150]; 
    int root, *rbuf, stride; 
    MPI_Datatype stype; 
    int *displs,i,*rcounts; 
<P> 
... 
<P> 
MPI_Comm_size( comm, &amp;gsize); 
    rbuf = (int *)malloc(gsize*stride*sizeof(int)); 
    displs = (int *)malloc(gsize*sizeof(int)); 
    rcounts = (int *)malloc(gsize*sizeof(int)); 
    for (i=0; i&lt;gsize; ++i) { 
        displs[i] = i*stride; 
        rcounts[i] = 100; 
    } 
    /* Create datatype for 1 column of array 
     */ 
    MPI_Type_vector( 100, 1, 150, MPI_INT, &amp;stype); 
    MPI_Type_commit( &amp;stype ); 
    MPI_Gatherv( sendarray, 1, stype, rbuf, rcounts, displs, MPI_INT, 
                                                             root, comm); 
</tt></pre> 
   
<P> 
<P><IMG SRC="mycoll-fig4.gif"><P>
 
  
    
  <BR> 
<b>Figure 5: </b><A NAME="Figure5">The root process gathers column <tt> 0</tt> of a 100$ x $150
  C array, and each set is placed <tt> stride</tt> ints apart.
  </a><P> 
  
    
<BR><b> Example</b>   
  
<P> 
Process i sends (100-i) ints from the ith column of a  
100 <IMG SRC="img110.gif">
 150 int array, in C.  It is received into a buffer with stride,  
as in the previous two examples. See figure <a href="node70.html#Figure6">6
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100][150],*sptr; 
    int root, *rbuf, stride, myrank; 
    MPI_Datatype stype; 
    int *displs,i,*rcounts; 
<P> 
... 
<P> 
MPI_Comm_size( comm, &amp;gsize); 
    MPI_Comm_rank( comm, &amp;myrank ); 
    rbuf = (int *)malloc(gsize*stride*sizeof(int)); 
    displs = (int *)malloc(gsize*sizeof(int)); 
    rcounts = (int *)malloc(gsize*sizeof(int)); 
    for (i=0; i&lt;gsize; ++i) { 
        displs[i] = i*stride; 
        rcounts[i] = 100-i;     /* note change from previous example */ 
    } 
    /* Create datatype for the column we are sending 
     */ 
    MPI_Type_vector( 100-myrank, 1, 150, MPI_INT, &amp;stype); 
    MPI_Type_commit( &amp;stype ); 
    /* sptr is the address of start of "myrank" column 
     */ 
    sptr = &amp;sendarray[0][myrank]; 
    MPI_Gatherv( sptr, 1, stype, rbuf, rcounts, displs, MPI_INT, 
       root, comm); 
</tt></pre> 
Note that a different amount of data is received from each process.  
   
<P> 
<P><IMG SRC="mycoll-fig5.gif"><P>
 
  
    
  <BR> 
<b>Figure 6: </b><A NAME="Figure6">The root process gathers 100-i ints from
  column <tt> i</tt> of a 100$ x $150
  C array, and each set is placed <tt> stride</tt> ints apart.
  </a><P> 
  
    
<BR><b> Example</b>   
  
<P> 
Same as Example <a href="node70.html#Node70">Examples using  MPI_GATHER,  MPI_GATHERV
</a>, but done in a different way at the sending end.  
We create a datatype that causes the correct striding at the  
sending end so that that we read a column of a C array.  
A similar thing was done in Example <a href="node61.html#Node61">Examples
</a>,  
Section <a href="node61.html#Node61">Examples
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100][150],*sptr; 
    int root, *rbuf, stride, myrank, disp[2], blocklen[2]; 
    MPI_Datatype stype,type[2]; 
    int *displs,i,*rcounts; 
<P> 
... 
<P> 
MPI_Comm_size( comm, &amp;gsize); 
    MPI_Comm_rank( comm, &amp;myrank ); 
    rbuf = (int *)malloc(gsize*stride*sizeof(int)); 
    displs = (int *)malloc(gsize*sizeof(int)); 
    rcounts = (int *)malloc(gsize*sizeof(int)); 
    for (i=0; i&lt;gsize; ++i) { 
        displs[i] = i*stride; 
        rcounts[i] = 100-i; 
    } 
    /* Create datatype for one int, with extent of entire row 
     */ 
    disp[0] = 0;       disp[1] = 150*sizeof(int); 
    type[0] = MPI_INT; type[1] = MPI_UB; 
    blocklen[0] = 1;   blocklen[1] = 1; 
    MPI_Type_struct( 2, blocklen, disp, type, &amp;stype ); 
    MPI_Type_commit( &amp;stype ); 
    sptr = &amp;sendarray[0][myrank]; 
    MPI_Gatherv( sptr, 100-myrank, stype, rbuf, rcounts, displs, MPI_INT, 
                                                               root, comm); 
</tt></pre> 
   
<P> 
<BR><b> Example</b>   
  
<P> 
Same as Example <a href="node70.html#Node70">Examples using  MPI_GATHER,  MPI_GATHERV
</a> at sending side, but  
at receiving side we make the  
stride between received blocks vary from block to block.  
See figure <a href="node70.html#Figure7">7
</a>.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100][150],*sptr; 
    int root, *rbuf, *stride, myrank, bufsize; 
    MPI_Datatype stype; 
    int *displs,i,*rcounts,offset; 
<P> 
... 
<P> 
MPI_Comm_size( comm, &amp;gsize); 
    MPI_Comm_rank( comm, &amp;myrank ); 
<P> 
stride = (int *)malloc(gsize*sizeof(int)); 
    ... 
    /* stride[i] for i = 0 to gsize-1 is set somehow 
     */ 
<P> 
/* set up displs and rcounts vectors first 
     */ 
    displs = (int *)malloc(gsize*sizeof(int)); 
    rcounts = (int *)malloc(gsize*sizeof(int)); 
    offset = 0; 
    for (i=0; i&lt;gsize; ++i) { 
        displs[i] = offset; 
        offset += stride[i]; 
        rcounts[i] = 100-i; 
    } 
    /* the required buffer size for rbuf is now easily obtained 
     */ 
    bufsize = displs[gsize-1]+rcounts[gsize-1]; 
    rbuf = (int *)malloc(bufsize*sizeof(int)); 
    /* Create datatype for the column we are sending 
     */ 
    MPI_Type_vector( 100-myrank, 1, 150, MPI_INT, &amp;stype); 
    MPI_Type_commit( &amp;stype ); 
    sptr = &amp;sendarray[0][myrank]; 
    MPI_Gatherv( sptr, 1, stype, rbuf, rcounts, displs, MPI_INT, 
                                                        root, comm); 
</tt></pre> 
   
<P> 
<P><IMG SRC="mycoll-fig6.gif"><P>
 
  
    
  <BR> 
<b>Figure 7: </b><A NAME="Figure7">The root process gathers 100-i ints from
  column <tt> i</tt> of a 100$ x $150
  C array, and each set is placed <tt> stride[i]</tt> ints apart (a varying
stride).
  </a><P> 
  
    
<BR><b> Example</b>   
  
<P> 
Process i sends <tt> num</tt> ints from the ith column of a  
100 <IMG SRC="img111.gif">
 150 int array, in C.  The complicating factor is that  
the various values of <tt> num</tt> are not known to <tt> root</tt>, so a  
separate gather must first be run to find these out.  The data is  
placed contiguously at the receiving end.  
<P> 
<BR> 
<pre><tt>MPI_Comm comm; 
    int gsize,sendarray[100][150],*sptr; 
    int root, *rbuf, stride, myrank, disp[2], blocklen[2]; 
    MPI_Datatype stype,types[2]; 
    int *displs,i,*rcounts,num; 
<P> 
... 
<P> 
MPI_Comm_size( comm, &amp;gsize); 
    MPI_Comm_rank( comm, &amp;myrank ); 
<P> 
/* First, gather nums to root 
     */ 
    rcounts = (int *)malloc(gsize*sizeof(int)); 
    MPI_Gather( &amp;num, 1, MPI_INT, rcounts, 1, MPI_INT, root, comm); 
    /* root now has correct rcounts, using these we set displs[] so 
     * that data is placed contiguously (or concatenated) at receive end 
     */ 
    displs = (int *)malloc(gsize*sizeof(int)); 
    displs[0] = 0; 
    for (i=1; i&lt;gsize; ++i) { 
        displs[i] = displs[i-1]+rcounts[i-1]; 
    } 
    /* And, create receive buffer 
     */ 
    rbuf = (int *)malloc(gsize*(displs[gsize-1]+rcounts[gsize-1]) 
            *sizeof(int)); 
    /* Create datatype for one int, with extent of entire row 
     */ 
    disp[0] = 0;       disp[1] = 150*sizeof(int); 
    type[0] = MPI_INT; type[1] = MPI_UB; 
    blocklen[0] = 1;   blocklen[1] = 1; 
    MPI_Type_struct( 2, blocklen, disp, type, &amp;stype ); 
    MPI_Type_commit( &amp;stype ); 
    sptr = &amp;sendarray[0][myrank]; 
    MPI_Gatherv( sptr, num, stype, rbuf, rcounts, displs, MPI_INT, 
                                                               root, comm); 
</tt></pre> 
   
<P> 

<P>
<HR>
<A HREF="node69.html#Node69"><IMG SRC="previous.gif"></A><A HREF="node69.html#Node69"><IMG SRC="up.gif"></A><A HREF="node71.html#Node71"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node69.html#Node69"> Gather</a>
<b>Next: </b><A HREF="node71.html#Node71"> Scatter</a>
<b>Previous: </b><A HREF="node69.html#Node69"> Gather</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
