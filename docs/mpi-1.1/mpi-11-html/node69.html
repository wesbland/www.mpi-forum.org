<HTML>
<!-- This file was generated by tohtml from coll.tex -->
<TITLE>Gather</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H1><A NAME="Node69">4.5. Gather</a></H1>
<A HREF="node68.html#Node68"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node70.html#Node70"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node70.html#Node70"> Examples using  MPI_GATHER,  MPI_GATHERV</a>
<b>Previous: </b><A HREF="node68.html#Node68"> Example using  MPI_BCAST</a>
<P>
  
<P> 
    
      
      
      
      
     MPI_GATHER( sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, root, comm)   
     
<BR> 
[  IN    sendbuf]  starting address of send buffer (choice)  
 
<BR> 
[  IN    sendcount]  number of elements in send buffer (integer)  
 
<BR> 
[  IN    sendtype]  data type of send buffer elements (handle)  
 
<BR> 
[  OUT    recvbuf]  address of receive buffer (choice,  
significant only at root)  
 
<BR> 
[  IN    recvcount]  number of elements for any single receive (integer,  
significant only at root)  
 
<BR> 
[  IN    recvtype]  data type of recv buffer elements  
(significant only at root) (handle)  
 
<BR> 
[  IN    root]  rank of receiving process (integer)  
 
<BR> 
[  IN    comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Gather(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)  <BR></tt>  
<P> 
 <tt> MPI_GATHER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, ROOT, COMM, IERROR <BR></tt>  
<P> 
Each process (root process included) sends the contents of its send  
buffer to the root process.  The root process receives the messages and  
stores them in rank order.  
The outcome is <em> as if</em> each of the <tt> n</tt> processes in the group  
(including the root process) had executed a call to  
<P><IMG SRC="img103.gif"><P>
and the  
root had executed <tt> n</tt> calls to  
<P><IMG SRC="img104.gif"><P>
where <tt> extent(recvtype)</tt> is the type extent obtained from a call to  
<tt> MPI_Type_extent()</tt>.  
<P> 
An alternative description is that the <tt> n</tt> messages sent by the  
processes in the group are concatenated in rank order, and the  
resulting message is received by the root as if by a call to  
 MPI_RECV(recvbuf, recvcount<IMG SRC="img105.gif">
n, recvtype, ...).  
<P> 
The receive buffer is ignored for all non-root processes.  
<P> 
General, derived datatypes are allowed for both  sendtype  
and  recvtype.  
The type signature of  sendcount, sendtype on process <tt> i</tt>  
must be equal to the type signature of  
 recvcount, recvtype at the root.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between each process and the root.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
All arguments to the function are significant on process  root,  
while on other processes, only arguments  sendbuf, sendcount,  
sendtype, root, comm are significant.  
The arguments  root and  comm  
must have identical values on all processes.  
<P> 
The specification of counts and types  
should not cause any location on the root to be written more than  
once.  Such a call is erroneous.  
<P> 
Note that the  recvcount argument at the root indicates  
the number of items it receives from <em> each</em> process, not the total number  
of items it receives.  
<P> 
    
      
      
      
      
     MPI_GATHERV( sendbuf, sendcount, sendtype, recvbuf,  
recvcounts, displs, recvtype, root, comm)   
     
<BR> 
[  IN    sendbuf]  starting address of send buffer (choice)  
 
<BR> 
[  IN    sendcount]  number of elements in send buffer (integer)  
 
<BR> 
[  IN    sendtype]  data type of send buffer elements (handle)  
 
<BR> 
[  OUT    recvbuf]  address of receive buffer (choice,  
significant only at root)  
 
<BR> 
[  IN    recvcounts]  integer array (of length group size)  
containing the number of elements that are received from each process  
(significant only at root)  
 
<BR> 
[  IN    displs]  integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement relative to  recvbuf at  
which to place the incoming data from process <tt> i</tt> (significant only  
at root)  
 
<BR> 
[  IN    recvtype]  data type of recv buffer elements  
(significant only at root) (handle)  
 
<BR> 
[  IN    root]  rank of receiving process (integer)  
 
<BR> 
[  IN    comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Gatherv(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int *recvcounts, int *displs, MPI_Datatype recvtype, int root, MPI_Comm comm)  <BR></tt>  
<P> 
 <tt> MPI_GATHERV(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNTS, DISPLS, RECVTYPE, ROOT, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNTS(*), DISPLS(*), RECVTYPE, ROOT, COMM, IERROR <BR></tt>  
<P> 
 MPI_GATHERV extends the functionality of  MPI_GATHER  
by allowing a varying count of data from each process, since  
 recvcounts  
is now an array.  It also allows more flexibility as to where the data  
is placed on the root, by providing the new argument,  displs.  
<P> 
The outcome is <em> as if</em> each process, including the root process,  
sends a message to the root,  
<P><IMG SRC="img106.gif"><P>
and the root executes <tt> n</tt> receives,  
<P><IMG SRC="img107.gif"><P>
Messages are placed in the receive buffer of the root process  
in rank order, that is, the data sent from process <tt> j</tt> is  
placed in the <tt> j</tt>th portion of the receive buffer  recvbuf  
on process  root.  The <tt> j</tt>th portion of  recvbuf  
begins at offset  displs[j] elements (in terms of  
 recvtype) into  recvbuf.  
<P> 
The receive buffer is ignored for all non-root processes.  
<P> 
The type signature implied by  sendcount, sendtype on process <tt> i</tt>  
must be equal to the type signature implied by  recvcounts[i], recvtype  
at the root.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between each process and the root.  
Distinct type maps between sender and receiver are still allowed,  
as illustrated in Example <a href="node70.html#Node70">Examples using  MPI_GATHER,  MPI_GATHERV
</a>.  
<P> 
All arguments to the function are significant on process  root,  
while on other processes, only arguments  sendbuf, sendcount,  
sendtype, root, comm are significant.  
The arguments  root and  comm  
must have identical values on all processes.  
<P> 
The specification of counts, types, and displacements  
should not cause any location on the root to be written more than  
once.  Such a call is erroneous.  
<P> 
<menu> 
</menu> 

<P>
<HR>
<A HREF="node68.html#Node68"><IMG SRC="previous.gif"></A><A HREF="node63.html#Node63"><IMG SRC="up.gif"></A><A HREF="node70.html#Node70"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node63.html#Node63"> Collective Communication</a>
<b>Next: </b><A HREF="node70.html#Node70"> Examples using  MPI_GATHER,  MPI_GATHERV</a>
<b>Previous: </b><A HREF="node68.html#Node68"> Example using  MPI_BCAST</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
