<HTML>
<!-- This file was generated by tohtml from coll.tex -->
<TITLE>All-Reduce</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node82">4.9.5. All-Reduce</a></H2>
<A HREF="node80.html#Node81"><IMG SRC="previous.gif"></A><A HREF="node76.html#Node76"><IMG SRC="up.gif"></A><A HREF="node83.html#Node83"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node76.html#Node76"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node83.html#Node83"> Reduce-Scatter</a>
<b>Previous: </b><A HREF="node80.html#Node81"> Example of User-defined Reduce</a>
<P>
  
<P> 
 MPI includes variants of each of the reduce operations  
where the result is returned to all processes in the group.  
 MPI requires that all processes participating in these  
operations receive identical results.  
<P> 
    
      
      
      
      
     MPI_ALLREDUCE( sendbuf, recvbuf, count, datatype, op, comm)  
     
<BR> 
[  IN   sendbuf]  starting address of send buffer (choice)  
 
<BR> 
[  OUT   recvbuf]  starting address of receive buffer (choice)  
 
<BR> 
[  IN   count]  number of elements in send buffer (integer)  
 
<BR> 
[  IN   datatype]  data type of elements of send buffer (handle)  
 
<BR> 
[  IN   op]  operation (handle)  
 
<BR> 
[  IN   comm]  communicator (handle)  
<BR> 
  
<P> 
 <tt> int MPI_Allreduce(void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <BR></tt>  
<P> 
 <tt> MPI_ALLREDUCE(SENDBUF, RECVBUF, COUNT, DATATYPE, OP, COMM, IERROR) <BR> &lt;type&gt; SENDBUF(*), RECVBUF(*) <BR>INTEGER COUNT, DATATYPE, OP, COMM, IERROR <BR></tt>  
<P> 
Same as  MPI_REDUCE except that the result  
appears in the receive buffer of all the group members.  
<P> 
 
<BR> 
[]<em> Advice  
 to implementors.</em>  
<P> 
The all-reduce operations can be implemented as a reduce, followed by a  
broadcast.  However, a direct implementation can lead to better performance.  
 (<em> End of advice to implementors.</em>) <BR> 
<BR><b> Example</b>   
  
<P> 
A routine that computes  
the product of a vector and an array that are distributed across a  
group of processes and returns the answer at all nodes (see also Example  
<a href="node78.html#Node78">Predefined reduce operations
</a>).  
<P> 
<BR> 
<pre><tt>SUBROUTINE PAR_BLAS2(m, n, a, b, c, comm) 
REAL a(m), b(m,n)    ! local slice of array 
REAL c(n)            ! result 
REAL sum(n) 
INTEGER n, comm, i, j, ierr 
<P> 
! local sum 
DO j= 1, n 
  sum(j) = 0.0 
  DO i = 1, m 
    sum(j) = sum(j) + a(i)*b(i,j) 
  END DO 
END DO 
<P> 
! global sum 
CALL MPI_ALLREDUCE(sum, c, n, MPI_REAL, MPI_SUM, 0, comm, ierr) 
<P> 
! return result at all nodes 
RETURN 
</tt></pre> 
  
  
<P> 

<P>
<HR>
<A HREF="node80.html#Node81"><IMG SRC="previous.gif"></A><A HREF="node76.html#Node76"><IMG SRC="up.gif"></A><A HREF="node83.html#Node83"><IMG SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node76.html#Node76"> Global Reduction Operations</a>
<b>Next: </b><A HREF="node83.html#Node83"> Reduce-Scatter</a>
<b>Previous: </b><A HREF="node80.html#Node81"> Example of User-defined Reduce</a>
<P>
<HR>
Return to <A HREF="node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-20-html/node306.html">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-1.1 of June 12, 1995<BR>
HTML Generated on August 6, 1997
</FONT>
</BODY>
</HTML>
