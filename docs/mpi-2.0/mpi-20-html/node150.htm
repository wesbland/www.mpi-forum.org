<HTML>
<!-- This file was generated by tohtml from collective-2.tex -->
<!-- with the command
tohtml -default -endpage ../mpi2-forum-tail.htm -basedef ../mpi2defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex mpi2-report.tex 
-->
<TITLE>Reductions</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node150">7.3.3. Reductions</a></H2>
<A HREF="node145.htm#Node149"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node151.htm#Node151"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<b>Next: </b><A HREF="node151.htm#Node151"> Other Operations</a>
<b>Previous: </b><A HREF="node145.htm#Node149"> ``All'' Forms and All-to-all</a>
<P>
<TABLE><TR><TD COLSPAN=2>MPI_REDUCE(sendbuf, recvbuf, count, datatype, op,  
root, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> address of send buffer (choice)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice,  
significant only at root)</TD></TR>  
<TR><TD> IN  count</TD><TD> number of elements in send buffer (integer)</TD></TR>  
<TR><TD> IN  datatype</TD><TD> data type of elements of send buffer (handle)</TD></TR>  
<TR><TD> IN  op</TD><TD> reduce operation (handle)</TD></TR>  
<TR><TD> IN  root</TD><TD> rank of root process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD>  communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Reduce(const void* sendbuf, void* recvbuf,  int count, const MPI::Datatype&amp; datatype, const MPI::Op&amp; op,  int root) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing the  
value   
 MPI_IN_PLACE to the argument  sendbuf at the root.  
In such case, the input data is taken at the root from the receive buffer,  
where it will be replaced by the output data.  
<P> 
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Only send buffer arguments are significant in group B and only receive  
buffer arguments are significant at the root.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLREDUCE(sendbuf, recvbuf, count, datatype, op, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD> starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD> number of elements in send buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD> data type of elements of send buffer (handle)</TD></TR>  
<TR><TD> IN op</TD><TD> operation (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Allreduce(const void* sendbuf, void* recvbuf,  int count, const MPI::Datatype&amp; datatype, const MPI::Op&amp; op) const = 0 <BR></tt>  
  
<P> 
<P> 
The ``in place'' option  for intracommunicators is specified by passing the  
value   
 MPI_IN_PLACE to the argument  sendbuf at the root.  
In such case, the input data is taken at each process from the receive buffer,  
where it will be replaced by the output data.  
<P> 
If  comm is an intercommunicator, then the result of the reduction  
of the data provided by processes in group A is stored at each process  
in group B, and vice versa.  Both groups should provide the same  
 count value.  
<P> 
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_REDUCE_SCATTER(sendbuf, recvbuf, recvcounts,  
datatype, op, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD> starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcounts</TD><TD> integer array specifying the  
number of elements in result distributed to each process.  
Array must be identical on all calling processes.</TD></TR>  
<TR><TD> IN datatype</TD><TD> data type of elements of input buffer (handle)</TD></TR>  
<TR><TD> IN op</TD><TD> operation (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Reduce_scatter(const void* sendbuf,  void* recvbuf, int recvcounts[], const MPI::Datatype&amp; datatype,  const MPI::Op&amp; op) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE in   
the  sendbuf argument.  In this case, the input data is taken  
from the top of the receive buffer.  Note that the area occupied by  
the input data may be  
either longer or shorter than the data filled by the output data.  
<P> 
If  comm is an intercommunicator, then the result of the reduction  
of the data provided by processes in group A is scattered among  
processes in group B, and vice versa.  Within each group, all  
processes provide the same  recvcounts argument, and the sum  
of the  recvcounts entries should be the same for the two groups.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
The last restriction is needed so that the length of the send  
buffer can be determined by the sum of the local  recvcounts entries.  
Otherwise, a communication is needed to figure out how many elements  
are reduced.  
 (<em> End of rationale.</em>) <BR> 
  

<P>
<HR>
<A HREF="node145.htm#Node149"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node151.htm#Node151"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<b>Next: </b><A HREF="node151.htm#Node151"> Other Operations</a>
<b>Previous: </b><A HREF="node145.htm#Node149"> ``All'' Forms and All-to-all</a>
<P>
<HR>
Return to <A HREF="node306.htm">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-11-html/node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 18, 1997<BR>
HTML Generated on September 10, 2001
</FONT>
</BODY>
</HTML>
