<HTML>
<!-- This file was generated by tohtml from collective-2.tex -->
<!-- with the command
tohtml -default -endpage ../mpi2-forum-tail.htm -basedef ../mpi2defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex mpi2-report.tex 
-->
<TITLE>Operations that Move Data</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node145">7.3.2. Operations that Move Data</a></H2>
<A HREF="node144.htm#Node144"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node146"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<b>Next: </b><A HREF="node145.htm#Node146"> Broadcast</a>
<b>Previous: </b><A HREF="node144.htm#Node144"> Intercommunicator Collective Operations</a>
<P>
  
Two additions are made to many collective communication calls:  
<ul> 
 
<li>Collective communication can occur ``in place''   
for intracommunicators,  
with the output  
buffer being identical to the input buffer.  This is specified by  
providing a special argument value,  MPI_IN_PLACE, instead of the  
send buffer or the receive buffer argument.  
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
The ``in place'' operations are provided to reduce unnecessary memory motion by  
both the  MPI implementation and by the user.  Note that while the simple check  
of testing whether the send and receive buffers have the same address will  
work for some cases (e.g.,  MPI_ALLREDUCE), they are inadequate in  
others (e.g.,  MPI_GATHER, with root not equal to zero).  Further,  
Fortran explicitly prohibits aliasing of arguments; the approach of using a  
special value to denote ``in place'' operation eliminates that difficulty.  
 (<em> End of rationale.</em>) <BR> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
By allowing the ``in place'' option, the receive buffer in many of the  
collective calls becomes a send-and-receive buffer.  For this reason, a  
Fortran binding that includes <tt>INTENT</tt> must mark these as <tt>INOUT</tt>,  
not <tt>OUT</tt>.  
<P> 
Note that  MPI_IN_PLACE is a special kind of value; it has the  
same   
restrictions on its use that  MPI_BOTTOM has.  
<P> 
Some intracommunicator collective operations do not support the ``in place''  
option (e.g.,  MPI_ALLTOALLV).  
 (<em> End of advice to users.</em>) <BR> 
  
 
<li>Collective communication applies to intercommunicators.  If the  
operation is rooted (e.g., broadcast, gather, scatter), then the transfer is  
unidirectional.  
The direction of the transfer is indicated  
by a special value of the root argument.  
  
In this case, for the group containing the root process, all processes in  
the group must call the routine using a special argument for the root.  
The root process uses the special root value  MPI_ROOT; all other  
processes   
in the same group as the root use  MPI_PROC_NULL.  
All processes in the other group (the group that  
is the remote group relative to the root process) must call the collective  
routine and provide the rank of the root.  
  
If the operation is unrooted (e.g., alltoall), then the  
transfer is bidirectional.  
<P> 
Note that the ``in place'' option for intracommunicators does not apply to  
intercommunicators since in the intercommunicator case there is no  
communication from a process to itself.  
</ul> 
<BR> 
  
<P> 
<P> 
<P> 
 
<BR> 
<em> Rationale.</em>  
<P> 
Rooted operations are unidirectional by nature, and there is a clear  
way of specifying direction.  Non-rooted operations, such as  
all-to-all, will often occur as part of an exchange, where it makes  
sense to communicate in both  directions at once.  
 (<em> End of rationale.</em>) <BR> 
In the following, the definitions of the collective routines are provided to  
enhance the readability and understanding of the associated text.  They do not  
change the definitions of the argument lists from  MPI-1.  
The C and Fortran language bindings for these routines are unchanged from  
 MPI-1, and are   
not repeated here.  Since new C++ bindings for the intercommunicator  
versions are required, they are included.  
The text provided for each routine is appended to the  
definition of the routine in  MPI-1.  
<P> 
<menu> 
</menu> 

<P>
<HR>
<A HREF="node144.htm#Node144"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node146"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<b>Next: </b><A HREF="node145.htm#Node146"> Broadcast</a>
<b>Previous: </b><A HREF="node144.htm#Node144"> Intercommunicator Collective Operations</a>
<P>
<HR><H3><A NAME="Node146">7.3.2.1. Broadcast</a></H3>
<A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node147"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node145.htm#Node147"> Gather</a>
<b>Previous: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<P>
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_BCAST(buffer, count, datatype, root, comm)</TD></TR>  
<TR><TD> INOUT  buffer</TD><TD>starting address of buffer (choice)</TD></TR>  
<TR><TD> IN  count</TD><TD> number of entries in buffer (integer)</TD></TR>  
<TR><TD> IN  datatype</TD><TD> data type of buffer (handle)</TD></TR>  
<TR><TD> IN  root</TD><TD> rank of broadcast root (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Bcast(void* buffer, int count, const MPI::Datatype&amp; datatype, int root) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option is not meaningful here.    
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Data is broadcast from the root to all processes in group B.  
The receive  
buffer arguments of the processes in group B must be consistent with  
the send buffer argument of the root.  
<P> 

<P>
<HR>
<A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node147"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node145.htm#Node147"> Gather</a>
<b>Previous: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<P>
<HR><H3><A NAME="Node147">7.3.2.2. Gather</a></H3>
<A HREF="node145.htm#Node146"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node148"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node145.htm#Node148"> Scatter</a>
<b>Previous: </b><A HREF="node145.htm#Node146"> Broadcast</a>
<P>
<TABLE><TR><TD COLSPAN=2>MPI_GATHER(sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, root, comm) </TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements in send buffer (integer)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice,  
significant only at root)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements for any single receive (integer,  
significant only at root)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of recv buffer elements  
(handle, significant only at root)</TD></TR>  
<TR><TD> IN  root</TD><TD> rank of receiving process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Gather(const void* sendbuf, int sendcount,  const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount,  const MPI::Datatype&amp; recvtype, int root) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE as   
the value of  sendbuf at the root.  In such a case,  
 sendcount and  sendtype are ignored, and the  
contribution of the root to the gathered vector is assumed to be already  
in the correct place in the receive buffer  
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Data is gathered from all processes in group B to the root.  
The send  
buffer arguments of the processes in group B must be consistent with  
the receive buffer argument of the root.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_GATHERV(sendbuf, sendcount, sendtype, recvbuf,  
recvcounts, displs, recvtype, root, comm) </TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements in send buffer (integer)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice,  
significant only at root)</TD></TR>  
<TR><TD> IN  recvcounts</TD><TD> integer array (of length group size)  
containing the number of elements that are received from each process  
(significant only at root)</TD></TR>  
<TR><TD> IN  displs</TD><TD> integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement relative to  recvbuf at  
which to place the incoming data from process <tt> i</tt> (significant only  
at root)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of recv buffer elements  
(handle, significant only at root)</TD></TR>  
<TR><TD> IN  root</TD><TD> rank of receiving process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Gatherv(const void* sendbuf, int sendcount,  const MPI::Datatype&amp; sendtype, void* recvbuf, const int recvcounts[], const int displs[], const MPI::Datatype&amp; recvtype, int root) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE as   
the value of  sendbuf at the root.  In such a case,  
 sendcount and  sendtype are ignored, and the  
contribution of the root to the gathered vector is assumed to be already  
in the correct place in the receive buffer  
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Data is gathered from all processes in group B to the root.  
The send  
buffer arguments of the processes in group B must be consistent with  
the receive buffer argument of the root.  
<P> 

<P>
<HR>
<A HREF="node145.htm#Node146"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node148"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node145.htm#Node148"> Scatter</a>
<b>Previous: </b><A HREF="node145.htm#Node146"> Broadcast</a>
<P>
<HR><H3><A NAME="Node148">7.3.2.3. Scatter</a></H3>
<A HREF="node145.htm#Node147"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node149"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node145.htm#Node149"> ``All'' Forms and All-to-all</a>
<b>Previous: </b><A HREF="node145.htm#Node147"> Gather</a>
<P>
<TABLE><TR><TD COLSPAN=2>MPI_SCATTER(sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, root, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> address of send buffer (choice, significant  
only at root)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements sent to each process  
(integer, significant only at root)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements  
(handle, significant only at root)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements in receive buffer (integer)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  root</TD><TD>  rank of sending process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Scatter(const void* sendbuf, int sendcount,  const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount,  const MPI::Datatype&amp; recvtype, int root) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE as   
the value of  recvbuf at the root.  In such case,  
 recvcount and  recvtype are ignored, and root  
``sends'' no data to itself. The scattered vector is still assumed to  
contain <I>n</I> segments, where <I>n</I> is the group size; the <em> root</em>-th  
segment, which root should ``send to itself,'' is not moved.  
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Data is scattered from the root to all processes in  
group B.  The receive buffer arguments of the processes in group B  
must be consistent with the send buffer argument of the root.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_SCATTERV(sendbuf, sendcounts, displs, sendtype,  
recvbuf, recvcount, recvtype, root, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> address of send buffer (choice, significant  
only at root)</TD></TR>  
<TR><TD> IN  sendcounts</TD><TD> integer array (of length group size)  
specifying the number of elements to send to each processor </TD></TR>  
<TR><TD> IN  displs</TD><TD> integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement (relative to  sendbuf from  
which to take the outgoing data to process <tt> i</tt></TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements in receive buffer (integer)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  root</TD><TD>  rank of sending process (integer)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Scatterv(const void* sendbuf, const int sendcounts[], const int displs[], const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount, const MPI::Datatype&amp; recvtype, int root) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing  
 MPI_IN_PLACE as   
the value of  recvbuf at the root.  In such case,  
 recvcount and  recvtype are ignored, and root  
``sends'' no data to itself. The scattered vector is still assumed to  
contain <I>n</I> segments, where <I>n</I> is the group size; the <em> root</em>-th  
segment, which root should ``send to itself,'' is not moved.  
<P> 
If  comm is an intercommunicator, then the call involves all   
processes in the intercommunicator, but with one group (group A) defining the  
root process.  All processes in the other group (group B) pass the same value  
in argument   
 root, which is the rank of the root in group A.  The root  
passes the value  MPI_ROOT in  root.  
All other processes in group A pass the value  MPI_PROC_NULL in  
 root.   
Data is scattered from the root to all processes in  
group B.  The receive buffer arguments of the processes in group B  
must be consistent with the send buffer argument of the root.  
<P> 

<P>
<HR>
<A HREF="node145.htm#Node147"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node149"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node145.htm#Node149"> ``All'' Forms and All-to-all</a>
<b>Previous: </b><A HREF="node145.htm#Node147"> Gather</a>
<P>
<HR><H3><A NAME="Node149">7.3.2.4. ``All'' Forms and All-to-all</a></H3>
<A HREF="node145.htm#Node148"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node150.htm#Node150"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node150.htm#Node150"> Reductions</a>
<b>Previous: </b><A HREF="node145.htm#Node148"> Scatter</a>
<P>
<TABLE><TR><TD COLSPAN=2>MPI_ALLGATHER(sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements in send buffer (integer)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements received from any  
process (integer)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  comm</TD><TD>  communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Allgather(const void* sendbuf, int sendcount, const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount, const MPI::Datatype&amp; recvtype) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing the  
value   
 MPI_IN_PLACE to the argument  sendbuf at all processes.  
 sendcount and  sendtype are ignored.  Then the input data  
of each process is assumed to be in the area where that  
process would receive its own contribution to the receive buffer.  
Specifically, the outcome of a call to  MPI_ALLGATHER in the   
``in place'' case is as if all processes executed <I>n</I> calls to   
<BR> 
<pre><tt>    MPI_GATHER( MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, recvbuf, recvcount,  
                                 recvtype, root, comm ) 
</tt></pre> 
for <tt> root = 0, ..., n - 1</tt>.  
<P> 
If  comm is an intercommunicator, then each process in group A  
contributes a data item; these items are concatenated and the result  
is stored at each process in group B.  Conversely the concatenation of the  
contributions of the processes in group B is stored at each process in  
group A.   The send buffer arguments in group A must be consistent  
with the receive buffer arguments in group B, and vice versa.  
<P> 
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
The communication pattern of  MPI_ALLGATHER executed on an  
intercommunication domain need not be symmetric.  The number of items  
sent by processes in group A (as specified by the arguments  
 sendcount, sendtype in group A and the arguments  
 recvcount, recvtype in group B), need not equal the number of  
items sent by processes in group B (as specified by the arguments  
 sendcount, sendtype in group B and the arguments  
 recvcount, recvtype in group A).  In particular, one can move  
data in only one direction by specifying  sendcount = 0 for  
the communication in the reverse direction.  
<P> 
 (<em> End of advice to users.</em>) <BR> 
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLGATHERV(sendbuf, sendcount, sendtype, recvbuf,  
recvcounts, displs, recvtype, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements in send buffer (integer)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcounts</TD><TD> integer array (of length group size)  
containing the number of elements that are received from each process</TD></TR>  
<TR><TD> IN  displs</TD><TD> integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement (relative to  recvbuf) at  
which to place the incoming data from process <tt> i</tt></TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  comm</TD><TD>  communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Allgatherv(const void* sendbuf, int sendcount,  const MPI::Datatype&amp; sendtype, void* recvbuf,  const int recvcounts[], const int displs[],  const MPI::Datatype&amp; recvtype) const = 0 <BR></tt>  
  
<P> 
The ``in place'' option  for intracommunicators is specified by passing the  
value   
 MPI_IN_PLACE to the argument  sendbuf at all processes.  
 sendcount and  sendtype are ignored.  Then the input data  
of each process is assumed to be in the area where that  
process would receive its own contribution to the receive buffer.  
Specifically, the outcome of a call to  MPI_ALLGATHER in the   
``in place'' case is as if all processes executed <I>n</I> calls to   
<BR> 
<pre><tt>    MPI_GATHERV( MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, recvbuf, recvcounts,  
                                 displs, recvtype, root, comm ) 
</tt></pre> 
for <tt> root = 0, ..., n - 1</tt>.  
<P> 
If  comm is an intercommunicator, then each process in group A  
contributes a data item; these items are concatenated and the result  
is stored at each process in group B.  Conversely the concatenation of the  
contributions of the processes in group B is stored at each process in  
group A.   The send buffer arguments in group A must be consistent  
with the receive buffer arguments in group B, and vice versa.  
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLTOALL(sendbuf, sendcount, sendtype, recvbuf,  
recvcount, recvtype, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcount</TD><TD> number of elements sent to each process (integer)</TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcount</TD><TD> number of elements received from any  
process (integer)</TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Alltoall(const void* sendbuf, int sendcount, const MPI::Datatype&amp; sendtype, void* recvbuf, int recvcount, const MPI::Datatype&amp; recvtype) const = 0 <BR></tt>  
  
<P> 
No ``in place'' option is supported.  
<P> 
<P> 
<P> 
If  comm is an intercommunicator, then the outcome is as if  
each process in group A sends a message to each process in group B,  
and vice versa.  The <I>j</I>-th send buffer of process <I>i</I> in group A should  
be consistent with the <I>i</I>-th receive buffer of process <I>j</I> in group B,  
and vice versa.  
<P> 
<P> 
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
When all-to-all is executed on an intercommunication domain, then  
the number of data items sent from processes in group A to processes  
in group B need not equal the number of items sent in the reverse  
direction.  In particular, one can have unidirectional communication  
by specifying  sendcount = 0 in the reverse direction.  
<P> 
 (<em> End of advice to users.</em>) <BR> 
<TABLE><TR><TD COLSPAN=2>MPI_ALLTOALLV(sendbuf, sendcounts, sdispls, sendtype,  
recvbuf, recvcounts, rdispls, recvtype, comm)</TD></TR>  
<TR><TD> IN  sendbuf</TD><TD> starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN  sendcounts</TD><TD> integer array equal to the group size  
specifying the number of elements to send to each processor</TD></TR>  
<TR><TD> IN  sdispls</TD><TD> integer array (of length group size).  Entry  
<tt> j</tt> specifies the displacement (relative to  sendbuf) from  
which to take the outgoing data destined for process <tt> j</tt></TD></TR>  
<TR><TD> IN  sendtype</TD><TD> data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT  recvbuf</TD><TD> address of receive buffer (choice)</TD></TR>  
<TR><TD> IN  recvcounts</TD><TD> integer array equal to the group size  
specifying the number of elements that can be received from  
each processor</TD></TR>  
<TR><TD> IN  rdispls</TD><TD> integer array (of length group size).  Entry  
<tt> i</tt> specifies the displacement (relative to  recvbuf) at  
which to place the incoming data from process <tt> i</tt></TD></TR>  
<TR><TD> IN  recvtype</TD><TD> data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN  comm</TD><TD> communicator (handle)</TD></TR>  
</TABLE>  
<P> 
  
 <tt> void MPI::Comm::Alltoallv(const void* sendbuf,  const int sendcounts[], const int sdispls[],  const MPI::Datatype&amp; sendtype, void* recvbuf,  const int recvcounts[], const int rdispls[],  const MPI::Datatype&amp; recvtype) const = 0 <BR></tt>  
  
<P> 
No ``in place'' option is supported.  
<P> 
If  comm is an intercommunicator, then the outcome is as if  
each process in group A sends a message to each process in group B,  
and vice versa.  The <I>j</I>-th send buffer of process <I>i</I> in group A should  
be consistent with the <I>i</I>-th receive buffer of process <I>j</I> in group B,  
and vice versa.  
<P> 

<P>
<HR>
<A HREF="node145.htm#Node148"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node150.htm#Node150"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Next: </b><A HREF="node150.htm#Node150"> Reductions</a>
<b>Previous: </b><A HREF="node145.htm#Node148"> Scatter</a>
<P>
<HR>
Return to <A HREF="node306.htm">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-11-html/node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 18, 1997<BR>
HTML Generated on September 10, 2001
</FONT>
</BODY>
</HTML>
