<HTML>
<!-- This file was generated by tohtml from one-side-2.tex -->
<!-- with the command
tohtml -default -endpage ../mpi2-forum-tail.htm -basedef ../mpi2defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex mpi2-report.tex 
-->
<TITLE>Put</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node122">6.3.1. Put</a></H2>
<A HREF="node121.htm#Node121"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node121.htm#Node121"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node123.htm#Node123"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node121.htm#Node121"> Communication Calls</a>
<b>Next: </b><A HREF="node123.htm#Node123"> Get</a>
<b>Previous: </b><A HREF="node121.htm#Node121"> Communication Calls</a>
<P>
The execution of a put operation is similar to the execution of a  
send by the origin process and a matching receive by the target process.  
The obvious difference is that all arguments are provided by one  
call --- the call executed by the origin process.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_PUT(origin_addr, origin_count, origin_datatype, target_rank, target_disp, target_count, target_datatype, win)</TD></TR> <TR><TD> IN origin_addr</TD><TD>initial address of origin buffer (choice)</TD></TR> <TR><TD> IN origin_count</TD><TD>number of entries in origin buffer  
(nonnegative integer)</TD></TR>  
<TR><TD> IN origin_datatype</TD><TD>datatype of each entry in origin buffer  
(handle)</TD></TR>  
<TR><TD> IN target_rank</TD><TD>rank of target (nonnegative integer)</TD></TR>  
<P> 
<TR><TD> IN target_disp</TD><TD>displacement from start of window to target  
buffer  
(nonnegative integer)</TD></TR>  
<P> 
<TR><TD> IN target_count</TD><TD>number of entries in target buffer  
(nonnegative  
integer)</TD></TR>  
<TR><TD> IN target_datatype</TD><TD>datatype of each entry in target buffer  
(handle)</TD></TR>  
<P> 
<TR><TD> IN win</TD><TD>window object used for communication (handle)</TD></TR>  
<P> 
</TABLE>  
<P> 
 <tt> int MPI_Put(void *origin_addr, int origin_count, MPI_Datatype origin_datatype, int target_rank, MPI_Aint target_disp, int target_count, MPI_Datatype target_datatype, MPI_Win win) <BR></tt>  
<P> 
 <tt> MPI_PUT(ORIGIN_ADDR, ORIGIN_COUNT, ORIGIN_DATATYPE, TARGET_RANK, TARGET_DISP, TARGET_COUNT, TARGET_DATATYPE, WIN, IERROR)<BR> &lt;type&gt; ORIGIN_ADDR(*) <BR>INTEGER(KIND=MPI_ADDRESS_KIND) TARGET_DISP <BR>INTEGER ORIGIN_COUNT, ORIGIN_DATATYPE, TARGET_RANK, TARGET_COUNT, TARGET_DATATYPE,  WIN, IERROR <BR></tt>  
<P> 
 <tt> void MPI::Win::Put(const void* origin_addr, int origin_count, const MPI::Datatype&amp; origin_datatype, int  target_rank, MPI::Aint target_disp, int target_count, const  MPI::Datatype&amp; target_datatype) const <BR></tt>  
<P> 
Transfers  origin_count  
successive entries of the type specified by the  
 origin_datatype, starting at address  origin_addr  
on the origin node to the target node specified by the  win,  target_rank pair.  
The data are written in  
the target buffer at address  
 target_addr = window_base +  
target_disp<I>&#215;</I>disp_unit, where  window_base and  
 disp_unit  
are the base address and window displacement unit  
specified at window initialization, by the target process.  
<P> 
The target buffer is specified by the arguments  
 target_count  
and  target_datatype.  
<P> 
The data transfer is the same as that which would occur  
if the origin process executed a send  
operation  
with arguments  origin_addr, origin_count, origin_datatype,  
target_rank, tag, comm, and the target process executed a  
receive operation with arguments  
 target_addr,  
target_count, target_datatype, source, tag, comm, where  
 target_addr is the target buffer address computed as  
explained above, and  comm is a communicator for the group of  win.  
<P> 
The communication must satisfy the same constraints as for a similar message-passing communication.  The  target_datatype may not specify overlapping entries in the target buffer.  
The message sent must fit, without truncation, in the target buffer.  
Furthermore, the target buffer  
must fit in the target window.  
<P> 
The  target_datatype argument  
is a handle to a datatype object defined at the origin process.  
However, this object is interpreted at the target process: the outcome  
is as if the target datatype object was defined at the target process,  
by the same sequence of calls used to define it at the origin process.  
  
The target datatype must   
contain only relative displacements, not absolute addresses.  The same  
holds for get and accumulate.  
  
<P> 
 
<BR> 
<em> Advice to users.</em>  
<P> 
The  target_datatype argument is a handle to a datatype  
object  
that is defined at the origin process, even though it defines a data  
layout in  
the target process memory.  This causes no problems in a homogeneous  
environment, or in a heterogeneous environment, if only portable  
datatypes are used (portable datatypes are defined in  
Section <a href="node8.htm#Node8">Semantic Terms 
</a>).  
<P> 
The performance of a put transfer can be significantly affected, on  
some systems,  
from the choice of window location and the shape and location  
of the origin and target buffer: transfers to a target window in memory  
allocated by  MPI_ALLOC_MEM may be much faster on shared  
memory systems;  
transfers from contiguous buffers will be  
faster on most, if not all, systems;  the alignment of the  
communication buffers may also impact performance.  
 (<em> End of advice to users.</em>) <BR> 
 
<BR> 
<em> Advice  
        to implementors.</em>  
<P> 
A high quality implementation will attempt to  
prevent remote accesses to memory outside the  
window that was exposed by the process.  This, both for debugging  
purposes, and for protection with client-server codes that use  RMA.  
I.e., a high-quality implementation will check, if possible,  
window bounds on each  RMA call,  
and raise an  MPI exception at the origin call if an out-of-bound   
situation occurred.  
Note that the condition can be checked at the origin.  
Of course, the added safety achieved by such checks has to be weighed  
against the added cost of such checks.  
 (<em> End of advice to implementors.</em>) <BR> 

<P>
<HR>
<A HREF="node121.htm#Node121"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node121.htm#Node121"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node123.htm#Node123"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node121.htm#Node121"> Communication Calls</a>
<b>Next: </b><A HREF="node123.htm#Node123"> Get</a>
<b>Previous: </b><A HREF="node121.htm#Node121"> Communication Calls</a>
<P>
<HR>
Return to <A HREF="node306.htm">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-11-html/node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 18, 1997<BR>
HTML Generated on September 10, 2001
</FONT>
</BODY>
</HTML>
