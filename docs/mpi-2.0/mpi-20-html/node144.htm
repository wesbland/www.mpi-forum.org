<HTML>
<!-- This file was generated by tohtml from collective-2.tex -->
<!-- with the command
tohtml -default -endpage ../mpi2-forum-tail.htm -basedef ../mpi2defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex mpi2-report.tex 
-->
<TITLE>Intercommunicator Collective Operations</TITLE>
<BODY BGCOLOR="#FFFFFF">
<HR><H2><A NAME="Node144">7.3.1. Intercommunicator Collective Operations</a></H2>
<A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<b>Next: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Previous: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<P>
  
<P> 
  
<P> 
<P> 
In the  MPI-1 standard (Section 4.2), collective operations only apply to  
intracommunicators; however, most  MPI  
collective operations can be generalized to intercommunicators.  
To understand how  MPI can be extended,  
we can view most  MPI intracommunicator  
collective operations as fitting one of the following categories (see, for  
instance,   
[<a href="node250.htm#-Bib20">20</a>]):  
<dl> 
 
<dt> 
<b>All-To-All</b><dd> 
All processes contribute to the result.  All processes  
  receive the result.    
  <ul> 
   
<li> MPI_Allgather, MPI_Allgatherv  
   
<li> MPI_Alltoall, MPI_Alltoallv  
   
<li> MPI_Allreduce, MPI_Reduce_scatter  
  </ul> 
<BR> 
 
<dt> 
<b>All-To-One</b><dd> 
All processes contribute to the result.  One process  
  receives the result.  
  <ul> 
   
<li> MPI_Gather, MPI_Gatherv  
   
<li> MPI_Reduce  
  </ul> 
<BR> 
 
<dt> 
<b>One-To-All</b><dd> 
One process contributes to the result.  All processes  
  receive the result.  
  <ul> 
   
<li> MPI_Bcast  
   
<li> MPI_Scatter, MPI_Scatterv  
  </ul> 
<BR> 
 
<dt> 
<b>Other</b><dd> 
Collective operations that do not fit into one of the above  
  categories.  
  <ul> 
   
<li> MPI_Scan  
   
<li> MPI_Barrier  
  </ul> 
<BR> 
</dl> 
<BR> 
The  MPI_Barrier operation does not fit into this classification  
since no data is being moved (other than the implicit fact that a barrier has  
been called).  The data movement pattern of  MPI_Scan does not fit  
this taxonomy.  
<P> 
<P> 
The extension of collective communication from intracommunicators to  
intercommunicators is best described in terms of the left and right groups.  
For example, an all-to-all  
 MPI_Allgather operation can be described as collecting data  
from all members of one group with the result appearing in all members  
of the other group (see Figure <a href="node144.htm#Figure11">11 
</a>).  As another  
example, a one-to-all  MPI_Bcast operation sends data from one  
member of one group to all members of the other group.  
Collective computation operations such as  MPI_REDUCE_SCATTER have  
a   
similar interpretation (see Figure <a href="node144.htm#Figure12">12 
</a>).  
For intracommunicators, these two groups  
are the same.  For intercommunicators, these two groups are distinct.  
For the all-to-all operations, each such operation is described in two phases,  
so that it   
has a symmetric, full-duplex behavior.  
<P> 
For  MPI-2, the following intracommunicator collective operations also  
apply to intercommunicators:  
<ul> 
 
<li> MPI_BCAST,  
 
<li> MPI_GATHER, MPI_GATHERV,  
 
<li> MPI_SCATTER, MPI_SCATTERV,  
 
<li> MPI_ALLGATHER, MPI_ALLGATHERV,  
 
<li> MPI_ALLTOALL, MPI_ALLTOALLV, MPI_ALLTOALLW  
 
<li> MPI_REDUCE, MPI_ALLREDUCE,  
 
<li> MPI_REDUCE_SCATTER,  
 
<li> MPI_BARRIER.  
</ul> 
<BR> 
( MPI_ALLTOALLW is a new function described in  
Section <a href="node152.htm#Node152">Generalized All-to-all Function 
</a>.)    
<P> 
These functions use exactly the same argument list as their  MPI-1  
counterparts and also work on intracommunicators, as expected.  No new  
language bindings are consequently needed for Fortran or C.  
  
However, in C++, the bindings have been "relaxed"; these member  
functions have been moved from the <tt> MPI::Intercomm</tt> class to the  
<tt> MPI::Comm</tt> class.  But since the collective operations do not make  
sense on a C++ <tt> MPI::Comm</tt> (since it is neither an intercommunicator nor  
an intracommunicator), the functions are all pure virtual.  In an  
 MPI-2 implementation, the bindings in this chapter supersede the  
corresponding bindings for  MPI-1.2.  
    
<P> 
<P> 
  <CENTER><P><IMG WIDTH=533 HEIGHT=406 SRC="collective-allgather.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 11: </b><A NAME="Figure11"></a><P> 
[ ]Intercommunicator allgather.  The focus of data to one process is  
    represented, not mandated by the semantics.   
    The two phases do allgathers in both directions.  
    
  
  <CENTER><P><IMG WIDTH=533 HEIGHT=406 SRC="collective-reduce_scatter.gif"><P>
</CENTER>  
  <BR> 
<b>Figure 12: </b><A NAME="Figure12"></a><P> 
[ ]Intercommunicator reduce-scatter.  The focus of data to one process  
    is represented, not mandated by the semantics.  
    The two phases do reduce-scatters in both directions.  
    

<P>
<HR>
<A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="previous.gif"></A><A HREF="node143.htm#Node143"><IMG WIDTH=16 HEIGHT=16 SRC="up.gif"></A><A HREF="node145.htm#Node145"><IMG WIDTH=16 HEIGHT=16 SRC="next.gif"></A><BR>
<b>Up: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<b>Next: </b><A HREF="node145.htm#Node145"> Operations that Move Data</a>
<b>Previous: </b><A HREF="node143.htm#Node143"> Extended Collective Operations</a>
<P>
<HR>
Return to <A HREF="node306.htm">MPI-2 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/docs/mpi-11-html/node182.html">MPI 1.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>MPI-2.0 of July 18, 1997<BR>
HTML Generated on September 10, 2001
</FONT>
</BODY>
</HTML>
