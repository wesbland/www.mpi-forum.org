<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-io/io-2.tex -->
<!-- with the command
tohtml -default -basedef mpi3defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi3-forum-tail.htm -Wnoredef -o mpi31-report.tex mpi-report.tex 
-->
<title>Split Collective Data Access Routines</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node330">297. Split Collective Data Access Routines</span></h2>
<a href="node326.htm#Node329"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node318.htm#Node318"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node331.htm#Node331"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node318.htm#Node318"> Data Access</a>
<b>Next: </b><a href="node331.htm#Node331"> File Interoperability</a>
<b>Previous: </b><a href="node326.htm#Node329"> Seek</a>
<p>
  
  
<P> 
<font face="sans-serif"> MPI</font> provides a restricted form of ``nonblocking collective'' I/O  
operations for all data accesses  
using split collective data access routines.  
These routines are referred to as ``split'' collective routines  
because a single collective operation is split in two:  
a begin routine and an end routine.  
The begin routine begins the operation,  
much like a nonblocking data access (e.g., <font face="sans-serif"> MPI_FILE_IREAD</font>).  
The end routine completes the operation,  
much like the matching test or wait (e.g., <font face="sans-serif"> MPI_WAIT</font>).  
As with nonblocking data access operations,  
the user must not use the buffer  
passed to a begin routine while the routine is outstanding;  
the operation must be completed with an end routine before it  
is safe to free buffers, etc.  
<P> 
Split collective data access operations on a file handle <font face="sans-serif"> fh</font>  
are subject to the semantic rules given below.  
<P> 
<ul> 
 
<li>On any <font face="sans-serif"> MPI</font> process,  
      each file handle may have at most one active split collective  
      operation at any time.  
<P> 
 
<li>Begin calls are collective over the group of processes  
that participated in the collective open and follow the ordering  
rules for collective calls.  
<P> 
 
<li>End calls are collective over the group of processes that  
participated in the collective open and follow the ordering  
  rules for collective calls.  
  Each end call matches  
  the preceding begin call for the same collective  
  operation.  When an ``end'' call is made, exactly one unmatched ``begin''  
  call for the same operation must precede it.  
<P> 
 
<li>An implementation is free to implement any split collective  
data access routine using the corresponding blocking collective  
routine when either the begin call (e.g.,  
<font face="sans-serif"> MPI_FILE_READ_ALL_BEGIN</font>) or the end call (e.g.,  
<font face="sans-serif"> MPI_FILE_READ_ALL_END</font>) is issued.  The begin and end calls are  
provided to allow the user and <font face="sans-serif"> MPI</font> implementation to optimize the  
collective operation.  
<P> 
 
<li>Split collective operations do not match the corresponding  
regular collective operation.  
For example, in a single collective read operation,  
an <font face="sans-serif"> MPI_FILE_READ_ALL</font> on one process does not match  
an <font face="sans-serif"> MPI_FILE_READ_ALL_BEGIN</font>/<font face="sans-serif"> MPI_FILE_READ_ALL_END</font> pair  
on another process.  
<P> 
 
<li>Split collective routines must specify a buffer  
in both the begin and end routines.  By specifying the buffer  
that receives data in the end routine, we can avoid   
the problems described   
in ``A Problem with Code Movements and Register Optimization,''  
Section <a href="node427.htm#Node427">Problems with Code Movement and Register Optimization 
</a>,  
but not all of the problems, such as those described in   
Sections <a href="node422.htm#Node422">Problems Due to Data Copying and Sequence Association with Subscript Triplets 
</a>, <a href="node423.htm#Node423">Problems Due to Data Copying and Sequence Association with Vector Subscripts 
</a>,  
and <a href="node426.htm#Node426">Optimization Problems, an Overview 
</a>.  
<P> 
 
<li>No collective I/O operations are permitted on a file handle  
concurrently with a split collective access on that file handle  
(i.e., between the begin and end of the access). That is  
<br> 
<pre><tt>                MPI_File_read_all_begin(fh, ...); 
                ... 
                MPI_File_read_all(fh, ...); 
                ... 
                MPI_File_read_all_end(fh, ...); 
</tt></pre> 
is erroneous.  
<P> 
 
<li>In a multithreaded implementation, any split collective  
                begin and end operation called by a process must be  
                called from the same thread.  This restriction is made  
                to simplify the implementation in the multithreaded  
                case.  
                (Note that we have already disallowed having two  
                threads begin a split collective operation on the same  
                file handle since only one split collective operation can be  
                active on a file handle at any time.)  
</ul> 
<br> 
The arguments for these routines have the same meaning as for  
the equivalent collective versions  
(e.g., the argument definitions for <font face="sans-serif"> MPI_FILE_READ_ALL_BEGIN</font>  
and <font face="sans-serif"> MPI_FILE_READ_ALL_END</font> are equivalent to the  
arguments for <font face="sans-serif"> MPI_FILE_READ_ALL</font>).  
The begin routine (e.g., <font face="sans-serif"> MPI_FILE_READ_ALL_BEGIN</font>)  
begins a split collective operation that,  
when completed with the matching end routine  
(i.e., <font face="sans-serif"> MPI_FILE_READ_ALL_END</font>)  
produces the result as defined for the equivalent collective  
routine (i.e., <font face="sans-serif"> MPI_FILE_READ_ALL</font>).  
<P> 
For the purpose of consistency semantics  
(Section <a href="node339.htm#Node339">File Consistency 
</a>),  
a matched pair of split collective data access operations  
(e.g., <font face="sans-serif"> MPI_FILE_READ_ALL_BEGIN</font> and <font face="sans-serif"> MPI_FILE_READ_ALL_END</font>)  
compose a single data access.  
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_AT_ALL_BEGIN(fh, offset, buf, count, datatype)</TD></TR>  
<TR><TD> IN fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN offset</TD><TD>file offset (integer)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_at_all_begin(MPI_File fh, MPI_Offset offset, void *buf, int count, MPI_Datatype datatype) <br></tt>  
 <tt> MPI_File_read_at_all_begin(fh, offset, buf, count, datatype, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>INTEGER(KIND=MPI_OFFSET_KIND), INTENT(IN) :: offset <br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buf <br>INTEGER, INTENT(IN) :: count <br>TYPE(MPI_Datatype), INTENT(IN) :: datatype <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_READ_AT_ALL_BEGIN(FH, OFFSET, BUF, COUNT, DATATYPE, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, COUNT, DATATYPE, IERROR <br>INTEGER(KIND=MPI_OFFSET_KIND) OFFSET <br></tt>  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_AT_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> IN fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_at_all_end(MPI_File fh, void *buf, MPI_Status *status) <br></tt>  
 <tt> MPI_File_read_at_all_end(fh, buf, status, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buf <br>TYPE(MPI_Status) :: status <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_READ_AT_ALL_END(FH, BUF, STATUS, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <br></tt>  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_AT_ALL_BEGIN(fh, offset, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN offset</TD><TD>file offset (integer)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_at_all_begin(MPI_File fh, MPI_Offset offset, const void *buf, int count, MPI_Datatype datatype) <br></tt>  
 <tt> MPI_File_write_at_all_begin(fh, offset, buf, count, datatype, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>INTEGER(KIND=MPI_OFFSET_KIND), INTENT(IN) :: offset <br>TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf <br>INTEGER, INTENT(IN) :: count <br>TYPE(MPI_Datatype), INTENT(IN) :: datatype <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_WRITE_AT_ALL_BEGIN(FH, OFFSET, BUF, COUNT, DATATYPE, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, COUNT, DATATYPE, IERROR <br>INTEGER(KIND=MPI_OFFSET_KIND) OFFSET <br></tt>  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_AT_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_at_all_end(MPI_File fh, const void *buf, MPI_Status *status) <br></tt>  
 <tt> MPI_File_write_at_all_end(fh, buf, status, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf <br>TYPE(MPI_Status) :: status <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_WRITE_AT_ALL_END(FH, BUF, STATUS, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <br></tt>  
  
  
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ALL_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_all_begin(MPI_File fh, void *buf, int count, MPI_Datatype datatype) <br></tt>  
 <tt> MPI_File_read_all_begin(fh, buf, count, datatype, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buf <br>INTEGER, INTENT(IN) :: count <br>TYPE(MPI_Datatype), INTENT(IN) :: datatype <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_READ_ALL_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, COUNT, DATATYPE, IERROR <br></tt>  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_all_end(MPI_File fh, void *buf, MPI_Status *status) <br></tt>  
 <tt> MPI_File_read_all_end(fh, buf, status, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buf <br>TYPE(MPI_Status) :: status <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_READ_ALL_END(FH, BUF, STATUS, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <br></tt>  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ALL_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_all_begin(MPI_File fh, const void *buf, int count, MPI_Datatype datatype) <br></tt>  
 <tt> MPI_File_write_all_begin(fh, buf, count, datatype, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf <br>INTEGER, INTENT(IN) :: count <br>TYPE(MPI_Datatype), INTENT(IN) :: datatype <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_WRITE_ALL_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, COUNT, DATATYPE, IERROR <br></tt>  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ALL_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_all_end(MPI_File fh, const void *buf, MPI_Status *status) <br></tt>  
 <tt> MPI_File_write_all_end(fh, buf, status, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf <br>TYPE(MPI_Status) :: status <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_WRITE_ALL_END(FH, BUF, STATUS, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <br></tt>  
  
  
<P> 
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ORDERED_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_ordered_begin(MPI_File fh, void *buf, int count, MPI_Datatype datatype) <br></tt>  
 <tt> MPI_File_read_ordered_begin(fh, buf, count, datatype, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buf <br>INTEGER, INTENT(IN) :: count <br>TYPE(MPI_Datatype), INTENT(IN) :: datatype <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_READ_ORDERED_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, COUNT, DATATYPE, IERROR <br></tt>  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_READ_ORDERED_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> OUT buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_read_ordered_end(MPI_File fh, void *buf, MPI_Status *status) <br></tt>  
 <tt> MPI_File_read_ordered_end(fh, buf, status, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buf <br>TYPE(MPI_Status) :: status <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_READ_ORDERED_END(FH, BUF, STATUS, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <br></tt>  
  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ORDERED_BEGIN(fh, buf, count, datatype)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> IN count</TD><TD>number of elements in buffer (integer)</TD></TR>  
<TR><TD> IN datatype</TD><TD>datatype of each buffer element (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_ordered_begin(MPI_File fh, const void *buf, int count, MPI_Datatype datatype) <br></tt>  
 <tt> MPI_File_write_ordered_begin(fh, buf, count, datatype, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf <br>INTEGER, INTENT(IN) :: count <br>TYPE(MPI_Datatype), INTENT(IN) :: datatype <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_WRITE_ORDERED_BEGIN(FH, BUF, COUNT, DATATYPE, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, COUNT, DATATYPE, IERROR <br></tt>  
  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_FILE_WRITE_ORDERED_END(fh, buf, status)</TD></TR>  
<TR><TD> INOUT fh</TD><TD>file handle (handle)</TD></TR>  
<TR><TD> IN buf</TD><TD>initial address of buffer (choice)</TD></TR>  
<TR><TD> OUT status</TD><TD>status object (Status)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_File_write_ordered_end(MPI_File fh, const void *buf, MPI_Status *status) <br></tt>  
 <tt> MPI_File_write_ordered_end(fh, buf, status, ierror) <br> TYPE(MPI_File), INTENT(IN) :: fh <br>TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf <br>TYPE(MPI_Status) :: status <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_FILE_WRITE_ORDERED_END(FH, BUF, STATUS, IERROR)<br> &lt;type&gt; BUF(*) <br>INTEGER FH, STATUS(MPI_STATUS_SIZE), IERROR <br></tt>  
  
  
<P> 

<P>
<hr>
<a href="node326.htm#Node329"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node318.htm#Node318"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node331.htm#Node331"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node318.htm#Node318"> Data Access</a>
<b>Next: </b><a href="node331.htm#Node331"> File Interoperability</a>
<b>Previous: </b><a href="node326.htm#Node329"> Seek</a>
<p>
<HR>
Return to <A HREF="node523.htm">MPI-3.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-3.1 of June 4, 2015<BR>
HTML Generated on June 4, 2015
</FONT>
</body>
</html>
