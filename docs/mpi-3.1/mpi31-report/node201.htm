<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-topol/topol.tex -->
<!-- with the command
tohtml -default -basedef mpi3defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi3-forum-tail.htm -Wnoredef -o mpi31-report.tex mpi-report.tex 
-->
<title>Neighborhood Gather</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node201">189. Neighborhood Gather</span></h2>
<a href="node200.htm#Node200"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node200.htm#Node200"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node202.htm#Node202"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node200.htm#Node200"> </a>
<b>Next: </b><a href="node202.htm#Node202"> Neighbor Alltoall</a>
<b>Previous: </b><a href="node200.htm#Node200"> </a>
<p>
In this function, each process <i>i</i> gathers data items from each process  
<i>j</i> if an edge <i>(j,i)</i> exists in the topology graph, and each  
process <i>i</i> sends the same data items to all processes <i>j</i> where an edge <i>(i,j)</i>  
exists. The send buffer is sent to each neighboring process and the  
<i>l</i>-th block in the receive buffer is received from the <i>l</i>-th neighbor.   
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_NEIGHBOR_ALLGATHER(sendbuf, sendcount, sendtype,  
recvbuf, recvcount, recvtype, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN sendcount</TD><TD>number of elements sent to each neighbor (non-negative integer)</TD></TR>  
<TR><TD> IN sendtype</TD><TD>data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcount</TD><TD>number of elements received from each neighbor (non-negative integer)</TD></TR>  
<TR><TD> IN recvtype</TD><TD>data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator with topology structure (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Neighbor_allgather(const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
<P> 
 <tt> MPI_Neighbor_allgather(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, ierror) <br> TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf <br>TYPE(*), DIMENSION(..) :: recvbuf <br>INTEGER, INTENT(IN) :: sendcount, recvcount <br>TYPE(MPI_Datatype), INTENT(IN) :: sendtype, recvtype <br>TYPE(MPI_Comm), INTENT(IN) :: comm <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_NEIGHBOR_ALLGATHER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, COMM, IERROR)<br> <i>&lt;</i>type<i>&gt;</i> SENDBUF(*), RECVBUF(*)<br>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, COMM, IERROR <br></tt>  
<P> 
This function supports Cartesian communicators, graph communicators, and  
distributed graph communicators as described  
in Section <a href="node200.htm#Node200"> 
</a>.  
If <font face="sans-serif"> comm</font> is a distributed graph communicator, the outcome is as  
if each process executed sends to each of its outgoing neighbors and   
receives from each of its incoming neighbors:  
<P> 
<br> 
<pre><tt>MPI_Dist_graph_neighbors_count(comm,&amp;indegree,&amp;outdegree,&amp;weighted); 
int *srcs=(int*)malloc(indegree*sizeof(int)); 
int *dsts=(int*)malloc(outdegree*sizeof(int)); 
MPI_Dist_graph_neighbors(comm,indegree,srcs,MPI_UNWEIGHTED, 
                         outdegree,dsts,MPI_UNWEIGHTED); 
int k,l; 
 
/* assume sendbuf and recvbuf are of type (char*) */ 
for(k=0; k&lt;outdegree; ++k)  
  MPI_Isend(sendbuf,sendcount,sendtype,dsts[k],...);  
 
for(l=0; l&lt;indegree; ++l)  
  MPI_Irecv(recvbuf+l*recvcount*extent(recvtype),recvcount,recvtype, 
            srcs[l],...);  
 
MPI_Waitall(...); 
</tt></pre> 
Figure <a href="node201.htm#Node201">Neighborhood Gather 
</a> shows the neighborhood gather  
communication of one process with outgoing neighbors <i>d<SUB>0</SUB>... d<SUB>3</SUB></i> and  
incoming neighbors <i>s<SUB>0</SUB>... s<SUB>5</SUB></i>. The process will send its  
<font face="sans-serif"> sendbuf</font> to all four <font face="sans-serif"> destinations</font> (outgoing neighbors)  
and it will receive the contribution from all six <font face="sans-serif"> sources</font>  
(incoming neighbors) into separate locations of its receive buffer.  
<P> 
  <div style=\"text-align:center\"></div><P><span id="node201.htm#Equation17"><img src="img177.gif" alt="Image file"></span><P>
  
  <br> 
Neighborhood gather communication example.<P> 
  
    
All arguments are significant on all processes and the argument  
<font face="sans-serif"> comm</font> must have identical values on all processes.  
<P> 
The type signature associated with <font face="sans-serif"> sendcount, sendtype</font>,  
at a process must be equal to the type signature associated with  
<font face="sans-serif"> recvcount, recvtype</font> at all other processes.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of   
communicating processes.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
For optimization reasons, the same type signature is required  
independently of whether the topology graph is connected or not.  
 (<em> End of rationale.</em>) <br> 
The ``in place'' option is not meaningful for this operation.  
<P> 
The vector variant of <font face="sans-serif"> MPI_NEIGHBOR_ALLGATHER</font> allows one to gather  
different numbers of elements from each neighbor.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_NEIGHBOR_ALLGATHERV(sendbuf, sendcount, sendtype,  
recvbuf, recvcounts, displs, recvtype, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN sendcount</TD><TD>number of elements sent to each neighbor (non-negative integer)</TD></TR>  
<TR><TD> IN sendtype</TD><TD>data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcounts</TD><TD>non-negative integer array (of length  
indegree) containing the number of elements that are received from each  
neighbor</TD></TR>  
<TR><TD> IN displs</TD><TD>integer array (of length indegree). Entry <font face="sans-serif"> i</font>  
specifies the displacement (relative to <font face="sans-serif"> recvbuf</font>) at which to  
place the incoming data from neighbor <font face="sans-serif"> i</font></TD></TR>  
<TR><TD> IN recvtype</TD><TD>data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator with topology structure (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Neighbor_allgatherv(const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, const int recvcounts[], const int displs[], MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
<P> 
 <tt> MPI_Neighbor_allgatherv(sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm, ierror) <br> TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf <br>TYPE(*), DIMENSION(..) :: recvbuf <br>INTEGER, INTENT(IN) :: sendcount, recvcounts(*), displs(*) <br>TYPE(MPI_Datatype), INTENT(IN) :: sendtype, recvtype <br>TYPE(MPI_Comm), INTENT(IN) :: comm <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_NEIGHBOR_ALLGATHERV(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNTS, DISPLS, RECVTYPE, COMM, IERROR)<br> <i>&lt;</i>type<i>&gt;</i> SENDBUF(*), RECVBUF(*)<br>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNTS(*), DISPLS(*), RECVTYPE, COMM,<br>    IERROR <br></tt>  
<P> 
This function supports Cartesian communicators, graph communicators, and  
distributed graph communicators as described  
in Section <a href="node200.htm#Node200"> 
</a>.  
If <font face="sans-serif"> comm</font> is a distributed graph communicator, the outcome is as if each  
process executed sends to each of its outgoing neighbors and receives from  
each of its incoming neighbors:  
<P> 
<br> 
<pre><tt>MPI_Dist_graph_neighbors_count(comm,&amp;indegree,&amp;outdegree,&amp;weighted); 
int *srcs=(int*)malloc(indegree*sizeof(int)); 
int *dsts=(int*)malloc(outdegree*sizeof(int)); 
MPI_Dist_graph_neighbors(comm,indegree,srcs,MPI_UNWEIGHTED, 
                         outdegree,dsts,MPI_UNWEIGHTED); 
int k,l; 
 
/* assume sendbuf and recvbuf are of type (char*) */ 
for(k=0; k&lt;outdegree; ++k)  
  MPI_Isend(sendbuf,sendcount,sendtype,dsts[k],...);  
 
for(l=0; l&lt;indegree; ++l)  
  MPI_Irecv(recvbuf+displs[l]*extent(recvtype),recvcounts[l],recvtype, 
            srcs[l],...);  
 
MPI_Waitall(...); 
</tt></pre> 
The type signature associated with <font face="sans-serif"> sendcount, sendtype</font>, at  
process <font face="sans-serif"> j</font> must be equal to the type signature associated with  
<font face="sans-serif"> recvcounts</font><font face="sans-serif"> [l]</font>, <font face="sans-serif"> recvtype</font> at any other process  
with <font face="sans-serif"> srcs[l]==j</font>.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of   
communicating processes.  
Distinct type maps between sender and receiver are still allowed.  
The data received from the <font face="sans-serif"> l</font>-th neighbor is placed into  
<font face="sans-serif"> recvbuf</font> beginning at offset <font face="sans-serif"> displs</font><font face="sans-serif"> [l]</font>  
elements (in terms of the <font face="sans-serif"> recvtype</font>).   
<P> 
The ``in place'' option is not meaningful for this operation.  
<P> 
All arguments are significant on all processes and the argument  
<font face="sans-serif"> comm</font> must have identical values on all processes.  
<P> 

<P>
<hr>
<a href="node200.htm#Node200"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node200.htm#Node200"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node202.htm#Node202"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node200.htm#Node200"> </a>
<b>Next: </b><a href="node202.htm#Node202"> Neighbor Alltoall</a>
<b>Previous: </b><a href="node200.htm#Node200"> </a>
<p>
<HR>
Return to <A HREF="node523.htm">MPI-3.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-3.1 of June 4, 2015<BR>
HTML Generated on June 4, 2015
</FONT>
</body>
</html>
