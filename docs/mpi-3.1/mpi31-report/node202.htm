<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-topol/topol.tex -->
<!-- with the command
tohtml -default -basedef mpi3defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi3-forum-tail.htm -Wnoredef -o mpi31-report.tex mpi-report.tex 
-->
<title>Neighbor Alltoall</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node202">190. Neighbor Alltoall</span></h2>
<a href="node201.htm#Node201"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node200.htm#Node200"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node203.htm#Node203"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node200.htm#Node200"> </a>
<b>Next: </b><a href="node203.htm#Node203"> </a>
<b>Previous: </b><a href="node201.htm#Node201"> Neighborhood Gather</a>
<p>
In this function, each process <i>i</i> receives data items from each process  
<i>j</i> if an edge <i>(j,i)</i> exists in the topology graph or Cartesian  
topology. Similarly, each process <i>i</i> sends data items to all processes  
<i>j</i> where an edge <i>(i,j)</i> exists. This call is more general than  
<font face="sans-serif"> MPI_NEIGHBOR_ALLGATHER</font> in that different data items can be  
sent to each neighbor. The <i>k</i>-th block in send buffer is sent to the  
<i>k</i>-th neighboring process and the <i>l</i>-th block in the receive buffer is  
received from the <i>l</i>-th neighbor.    
<P> 
<P><img width=371 height=231 src="img178.gif" alt="Image file"><P>
 <tt> int MPI_Neighbor_alltoall(const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
<P> 
 <tt> MPI_Neighbor_alltoall(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, ierror) <br> TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf <br>TYPE(*), DIMENSION(..) :: recvbuf <br>INTEGER, INTENT(IN) :: sendcount, recvcount <br>TYPE(MPI_Datatype), INTENT(IN) :: sendtype, recvtype <br>TYPE(MPI_Comm), INTENT(IN) :: comm <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_NEIGHBOR_ALLTOALL(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, COMM, IERROR)<br> <i>&lt;</i>type<i>&gt;</i> SENDBUF(*), RECVBUF(*)<br>INTEGER SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, COMM, IERROR <br></tt>  
<P> 
This function supports Cartesian communicators, graph communicators, and  
distributed graph communicators as described   
in Section <a href="node200.htm#Node200"> 
</a>.  
If <font face="sans-serif"> comm</font> is a distributed graph communicator, the outcome is as if each  
process executed sends to each of its outgoing neighbors and receives from  
each of its incoming neighbors:  
<P> 
<br> 
<pre><tt>MPI_Dist_graph_neighbors_count(comm,&amp;indegree,&amp;outdegree,&amp;weighted); 
int *srcs=(int*)malloc(indegree*sizeof(int)); 
int *dsts=(int*)malloc(outdegree*sizeof(int)); 
MPI_Dist_graph_neighbors(comm,indegree,srcs,MPI_UNWEIGHTED, 
                         outdegree,dsts,MPI_UNWEIGHTED); 
int k,l; 
 
/* assume sendbuf and recvbuf are of type (char*) */ 
for(k=0; k&lt;outdegree; ++k) 
  MPI_Isend(sendbuf+k*sendcount*extent(sendtype),sendcount,sendtype, 
            dsts[k],...);  
 
for(l=0; l&lt;indegree; ++l)  
  MPI_Irecv(recvbuf+l*recvcount*extent(recvtype),recvcount,recvtype, 
            srcs[l],...);  
 
MPI_Waitall(...); 
</tt></pre> 
The type signature associated with <font face="sans-serif"> sendcount, sendtype</font>,  
at a process must be equal to the type signature associated with  
<font face="sans-serif"> recvcount, recvtype</font> at any other process.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of   
communicating processes.  
Distinct type maps between sender and receiver are still allowed.  
 <P> 
The ``in place'' option is not meaningful for this operation.  
<P> 
All arguments are significant on all processes and the argument  
<font face="sans-serif"> comm</font> must have identical values on all processes.  
<P> 
The vector variant of <font face="sans-serif"> MPI_NEIGHBOR_ALLTOALL</font> allows  
sending/receiving different numbers of elements to and from each neighbor.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_NEIGHBOR_ALLTOALLV(sendbuf, sendcounts, sdispls, sendtype,  
recvbuf, recvcounts, rdispls, recvtype, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN sendcounts</TD><TD>non-negative integer array (of length  
outdegree) specifying the number of elements to send to each neighbor</TD></TR>  
<TR><TD> IN sdispls</TD><TD>integer array (of length outdegree). Entry <font face="sans-serif"> j</font>  
specifies the displacement (relative to <font face="sans-serif"> sendbuf</font>) from which to send   
the outgoing data to neighbor <font face="sans-serif"> j</font></TD></TR>  
<TR><TD> IN sendtype</TD><TD>data type of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcounts</TD><TD>non-negative integer array (of length  
indegree) specifying the number of elements that are received from  
each neighbor</TD></TR>  
<TR><TD> IN rdispls</TD><TD>integer array (of length indegree). Entry <font face="sans-serif"> i</font>  
specifies the displacement (relative to <font face="sans-serif"> recvbuf</font>) at which to  
place the incoming data from neighbor <font face="sans-serif"> i</font></TD></TR>  
<TR><TD> IN recvtype</TD><TD>data type of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator with topology structure (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Neighbor_alltoallv(const void* sendbuf, const int sendcounts[], const int sdispls[], MPI_Datatype sendtype, void* recvbuf, const int recvcounts[], const int rdispls[], MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
<P> 
 <tt> MPI_Neighbor_alltoallv(sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm, ierror) <br> TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf <br>TYPE(*), DIMENSION(..) :: recvbuf <br>INTEGER, INTENT(IN) :: sendcounts(*), sdispls(*), recvcounts(*),<br>    rdispls(*) <br>TYPE(MPI_Datatype), INTENT(IN) :: sendtype, recvtype <br>TYPE(MPI_Comm), INTENT(IN) :: comm <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_NEIGHBOR_ALLTOALLV(SENDBUF, SENDCOUNTS, SDISPLS, SENDTYPE, RECVBUF, RECVCOUNTS, RDISPLS,<br>    RECVTYPE, COMM, IERROR)<br> <i>&lt;</i>type<i>&gt;</i> SENDBUF(*), RECVBUF(*)<br>INTEGER SENDCOUNTS(*), SDISPLS(*), SENDTYPE, RECVCOUNTS(*), RDISPLS(*), RECVTYPE, COMM, IERROR <br></tt>  
<P> 
This function supports Cartesian communicators, graph communicators, and  
distributed graph communicators as described  
in Section <a href="node200.htm#Node200"> 
</a>.  
If <font face="sans-serif"> comm</font> is a distributed graph communicator, the outcome is as if each  
process executed sends to each of its outgoing neighbors and receives from  
each of its incoming neighbors:  
<P> 
<br> 
<pre><tt>MPI_Dist_graph_neighbors_count(comm,&amp;indegree,&amp;outdegree,&amp;weighted); 
int *srcs=(int*)malloc(indegree*sizeof(int)); 
int *dsts=(int*)malloc(outdegree*sizeof(int)); 
MPI_Dist_graph_neighbors(comm,indegree,srcs,MPI_UNWEIGHTED, 
                         outdegree,dsts,MPI_UNWEIGHTED); 
int k,l; 
 
/* assume sendbuf and recvbuf are of type (char*) */ 
for(k=0; k&lt;outdegree; ++k)  
  MPI_Isend(sendbuf+sdispls[k]*extent(sendtype),sendcounts[k],sendtype, 
            dsts[k],...);  
 
for(l=0; l&lt;indegree; ++l)  
  MPI_Irecv(recvbuf+rdispls[l]*extent(recvtype),recvcounts[l],recvtype, 
            srcs[l],...);  
 
MPI_Waitall(...); 
</tt></pre> 
The type signature associated with  
<font face="sans-serif"> sendcounts</font><font face="sans-serif"> [k]</font>, <font face="sans-serif"> sendtype</font>  
with <font face="sans-serif"> dsts[k]==j</font> at process <font face="sans-serif"> i</font>  
must be equal to the type signature associated with  
<font face="sans-serif"> recvcounts</font><font face="sans-serif"> [l]</font>, <font face="sans-serif"> recvtype</font>  
with <font face="sans-serif"> srcs[l]==i</font> at process <font face="sans-serif"> j</font>.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of   
communicating processes.  
Distinct type maps between sender and receiver are still allowed.  
The data in the <font face="sans-serif"> sendbuf</font> beginning at offset   
<font face="sans-serif"> sdispls</font><font face="sans-serif"> [k]</font> elements (in terms of the <font face="sans-serif"> sendtype</font>)   
is sent to the <font face="sans-serif"> k</font>-th outgoing neighbor.  
The data received from the <font face="sans-serif"> l</font>-th incoming neighbor is placed  
into <font face="sans-serif"> recvbuf</font> beginning at offset <font face="sans-serif"> rdispls</font><font face="sans-serif"> [l]</font>  
elements (in terms of the <font face="sans-serif"> recvtype</font>).   
 <P> 
The ``in place'' option is not meaningful for this operation.  
<P> 
All arguments are significant on all processes and the argument  
<font face="sans-serif"> comm</font> must have identical values on all processes.  
<P> 
<font face="sans-serif"> MPI_NEIGHBOR_ALLTOALLW</font> allows one to send and receive with  
different datatypes to and from each neighbor.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_NEIGHBOR_ALLTOALLW(sendbuf, sendcounts, sdispls, sendtypes,  
recvbuf, recvcounts, rdispls, recvtypes, comm)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN sendcounts</TD><TD>non-negative integer array (of length  
outdegree) specifying the number of elements to send to each neighbor</TD></TR>  
<TR><TD> IN sdispls</TD><TD>integer array (of length outdegree). Entry <font face="sans-serif"> j</font> specifies the displacement in bytes (relative to <font face="sans-serif"> sendbuf</font>) from which to take the outgoing data destined for neighbor <font face="sans-serif"> j</font> (array of integers)</TD></TR>  
<TR><TD> IN sendtypes</TD><TD>array of datatypes (of length outdegree). Entry <font face="sans-serif"> j</font> specifies the type of data to send to neighbor <font face="sans-serif"> j</font> (array of handles)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcounts</TD><TD>non-negative integer array (of length  
indegree) specifying the number of elements that are received from each neighbor</TD></TR>  
<TR><TD> IN rdispls</TD><TD>integer array (of length indegree). Entry <font face="sans-serif"> i</font> specifies the displacement in bytes (relative to <font face="sans-serif"> recvbuf</font>) at which to place the incoming data from neighbor <font face="sans-serif"> i</font> (array of integers)</TD></TR>  
<TR><TD> IN recvtypes</TD><TD>array of datatypes (of length indegree).  
Entry <font face="sans-serif"> i</font> specifies the type of data received from neighbor <font face="sans-serif"> i</font> (array of handles)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator with topology structure (handle)</TD></TR>  
</TABLE>  
<P> 
 <tt> int MPI_Neighbor_alltoallw(const void* sendbuf, const int sendcounts[], const MPI_Aint sdispls[], const MPI_Datatype sendtypes[], void* recvbuf, const int recvcounts[], const MPI_Aint rdispls[], const MPI_Datatype recvtypes[], MPI_Comm comm) <br></tt>  
<P> 
 <tt> MPI_Neighbor_alltoallw(sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm, ierror) <br> TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf <br>TYPE(*), DIMENSION(..) :: recvbuf <br>INTEGER, INTENT(IN) :: sendcounts(*), recvcounts(*) <br>INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(IN) :: sdispls(*), rdispls(*) <br>TYPE(MPI_Datatype), INTENT(IN) :: sendtypes(*), recvtypes(*) <br>TYPE(MPI_Comm), INTENT(IN) :: comm <br>INTEGER, OPTIONAL, INTENT(OUT) :: ierror <br></tt>  
 <tt> MPI_NEIGHBOR_ALLTOALLW(SENDBUF, SENDCOUNTS, SDISPLS, SENDTYPES, RECVBUF, RECVCOUNTS, RDISPLS, RECVTYPES, COMM, IERROR)<br> <i>&lt;</i>type<i>&gt;</i> SENDBUF(*), RECVBUF(*)<br>INTEGER(KIND=MPI_ADDRESS_KIND) SDISPLS(*), RDISPLS(*)<br>INTEGER SENDCOUNTS(*), SENDTYPES(*), RECVCOUNTS(*), RECVTYPES(*), COMM,<br>    IERROR <br></tt>  
<P> 
This function supports Cartesian communicators, graph communicators, and  
distributed graph communicators as described  
in Section <a href="node200.htm#Node200"> 
</a>.  
If <font face="sans-serif"> comm</font> is a distributed graph communicator, the outcome is as if each  
process executed sends to each of its outgoing neighbors and receives from  
each of its incoming neighbors:  
<P> 
<br> 
<pre><tt>MPI_Dist_graph_neighbors_count(comm,&amp;indegree,&amp;outdegree,&amp;weighted); 
int *srcs=(int*)malloc(indegree*sizeof(int)); 
int *dsts=(int*)malloc(outdegree*sizeof(int)); 
MPI_Dist_graph_neighbors(comm,indegree,srcs,MPI_UNWEIGHTED, 
                         outdegree,dsts,MPI_UNWEIGHTED); 
int k,l; 
 
/* assume sendbuf and recvbuf are of type (char*) */ 
for(k=0; k&lt;outdegree; ++k)  
  MPI_Isend(sendbuf+sdispls[k],sendcounts[k], sendtypes[k],dsts[k],...);  
 
for(l=0; l&lt;indegree; ++l)  
  MPI_Irecv(recvbuf+rdispls[l],recvcounts[l], recvtypes[l],srcs[l],...);  
 
MPI_Waitall(...); 
</tt></pre> 
The type signature associated with  
<font face="sans-serif"> sendcounts</font><font face="sans-serif"> [k]</font>, <font face="sans-serif"> sendtypes</font><font face="sans-serif"> [k]</font>  
with <font face="sans-serif"> dsts[k]==j</font> at process <font face="sans-serif"> i</font>  
must be equal to the type signature associated with  
<font face="sans-serif"> recvcounts</font><font face="sans-serif"> [l]</font>, <font face="sans-serif"> recvtypes</font><font face="sans-serif"> [l]</font>  
with <font face="sans-serif"> srcs[l]==i</font> at process <font face="sans-serif"> j</font>.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of   
communicating processes.  
Distinct type maps between sender and receiver are still allowed.  
 <P> 
The ``in place'' option is not meaningful for this operation.  
<P> 
All arguments are significant on all processes and the argument  
<font face="sans-serif"> comm</font> must have identical values on all processes.  
<P> 

<P>
<hr>
<a href="node201.htm#Node201"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node200.htm#Node200"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node203.htm#Node203"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node200.htm#Node200"> </a>
<b>Next: </b><a href="node203.htm#Node203"> </a>
<b>Previous: </b><a href="node201.htm#Node201"> Neighborhood Gather</a>
<p>
<HR>
Return to <A HREF="node523.htm">MPI-3.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-3.1 of June 4, 2015<BR>
HTML Generated on June 4, 2015
</FONT>
</body>
</html>
