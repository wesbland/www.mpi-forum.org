<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-coll/coll.tex -->
<!-- with the command
tohtml -default -basedef mpi3defs.txt -numbers -indexname myindex -dosnl -htables -quietlatex -allgif -endpage mpi3-forum-tail.htm -Wnoredef -o mpi31-report.tex mpi-report.tex 
-->
<title>Applying Collective Operations to Intercommunicators</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node98">93. Applying Collective Operations to Intercommunicators</span></h2>
<a href="node97.htm#Node97"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node96.htm#Node96"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node99.htm#Node99"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node96.htm#Node96"> Communicator Argument</a>
<b>Next: </b><a href="node99.htm#Node99"> Specifics for Intercommunicator Collective Operations</a>
<b>Previous: </b><a href="node97.htm#Node97"> Specifics for Intracommunicator Collective Operations</a>
<p>
  
  
  
<P> 
To understand how collective operations apply to intercommunicators,  
we can view most <font face="sans-serif"> MPI</font> intracommunicator  
collective operations as fitting one of the following categories (see, for  
instance,   
[<a href="node522.htm#-Bib56">56</a>]):  
<dl> 
 
<dt> 
<b>All-To-All</b><dd> 
 All processes contribute to the result.  All processes  
  receive the result.    
  <ul> 
   
<li><font face="sans-serif"> MPI_ALLGATHER</font>, <font face="sans-serif"> MPI_IALLGATHER</font>,    
  <font face="sans-serif"> MPI_ALLGATHERV</font>, <font face="sans-serif"> MPI_IALLGATHERV</font>  
   
<li><font face="sans-serif"> MPI_ALLTOALL</font>, <font face="sans-serif"> MPI_IALLTOALL</font>,   
  <font face="sans-serif"> MPI_ALLTOALLV</font>, <font face="sans-serif"> MPI_IALLTOALLV</font>,  
  <font face="sans-serif"> MPI_ALLTOALLW</font>, <font face="sans-serif"> MPI_IALLTOALLW</font>  
   
<li><font face="sans-serif"> MPI_ALLREDUCE</font>, <font face="sans-serif"> MPI_IALLREDUCE</font>,    
  <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font>,  
  <font face="sans-serif"> MPI_IREDUCE_SCATTER_BLOCK</font>, <font face="sans-serif"> MPI_REDUCE_SCATTER</font>,  
  <font face="sans-serif"> MPI_IREDUCE_SCATTER</font>  
   
<li><font face="sans-serif"> MPI_BARRIER</font>, <font face="sans-serif"> MPI_IBARRIER</font>   
  </ul> 
<br> 
 
<dt> 
<b>All-To-One</b><dd> 
 All processes contribute to the result.  One process  
  receives the result.  
  <ul> 
   
<li><font face="sans-serif"> MPI_GATHER</font>, <font face="sans-serif"> MPI_IGATHER</font>,   
  <font face="sans-serif"> MPI_GATHERV</font>, <font face="sans-serif"> MPI_IGATHERV</font>  
   
<li><font face="sans-serif"> MPI_REDUCE</font>, <font face="sans-serif"> MPI_IREDUCE</font>  
  </ul> 
<br> 
 
<dt> 
<b>One-To-All</b><dd> 
 One process contributes to the result.  All processes  
  receive the result.  
  <ul> 
   
<li><font face="sans-serif"> MPI_BCAST</font>, <font face="sans-serif"> MPI_IBCAST</font>  
   
<li><font face="sans-serif"> MPI_SCATTER</font>, <font face="sans-serif"> MPI_ISCATTER</font>,    
  <font face="sans-serif"> MPI_SCATTERV</font>, <font face="sans-serif"> MPI_ISCATTERV</font>  
  </ul> 
<br> 
 
<dt> 
<b>Other</b><dd> 
 Collective operations that do not fit into one of the above  
  categories.  
  <ul> 
   
<li><font face="sans-serif"> MPI_SCAN</font>, <font face="sans-serif"> MPI_ISCAN</font>,    
  <font face="sans-serif"> MPI_EXSCAN</font>, <font face="sans-serif"> MPI_IEXSCAN</font>  
  </ul> 
<br> 
</dl> 
<br> 
The data movement patterns of <font face="sans-serif"> MPI_SCAN</font>, <font face="sans-serif"> MPI_ISCAN</font>,   
<font face="sans-serif"> MPI_EXSCAN</font>, and <font face="sans-serif"> MPI_IEXSCAN</font>  
do not fit this taxonomy.  
<P> 
<P> 
The application of collective communication to  
intercommunicators is best described in terms of two groups.  
For example, an all-to-all  
<font face="sans-serif"> MPI_ALLGATHER</font> operation can be described as collecting data  
from all members of one group with the result appearing in all members  
of the other group (see Figure <a href="node98.htm#Figure2">2 
</a>).  As another  
example, a one-to-all <font face="sans-serif"> MPI_BCAST</font> operation sends data from one  
member of one group to all members of the other group.  
Collective computation operations such as <font face="sans-serif"> MPI_REDUCE_SCATTER</font> have  
a   
similar interpretation (see Figure <a href="node98.htm#Figure3">3 
</a>).  
For intracommunicators, these two groups  
are the same.  For intercommunicators, these two groups are distinct.  
For the all-to-all operations, each such operation is described in two phases,  
so that it   
has a symmetric, full-duplex behavior.  
<P> 
The following collective operations also apply to intercommunicators:  
<ul> 
 
<li><font face="sans-serif"> MPI_BARRIER</font>, <font face="sans-serif"> MPI_IBARRIER</font>  
 
<li><font face="sans-serif"> MPI_BCAST</font>, <font face="sans-serif"> MPI_IBCAST</font>  
 
<li><font face="sans-serif"> MPI_GATHER</font>, <font face="sans-serif"> MPI_IGATHER</font>,   
      <font face="sans-serif"> MPI_GATHERV</font>, <font face="sans-serif"> MPI_IGATHERV</font>,  
 
<li><font face="sans-serif"> MPI_SCATTER</font>, <font face="sans-serif"> MPI_ISCATTER</font>,   
      <font face="sans-serif"> MPI_SCATTERV</font>, <font face="sans-serif"> MPI_ISCATTERV</font>,  
 
<li><font face="sans-serif"> MPI_ALLGATHER</font>, <font face="sans-serif"> MPI_IALLGATHER</font>,   
      <font face="sans-serif"> MPI_ALLGATHERV</font>, <font face="sans-serif"> MPI_IALLGATHERV</font>,  
 
<li><font face="sans-serif"> MPI_ALLTOALL</font>, <font face="sans-serif"> MPI_IALLTOALL</font>,   
      <font face="sans-serif"> MPI_ALLTOALLV</font>, <font face="sans-serif"> MPI_IALLTOALLV</font>,   
      <font face="sans-serif"> MPI_ALLTOALLW</font>, <font face="sans-serif"> MPI_IALLTOALLW</font>,  
 
<li><font face="sans-serif"> MPI_ALLREDUCE</font>, <font face="sans-serif"> MPI_IALLREDUCE</font>,   
      <font face="sans-serif"> MPI_REDUCE</font>, <font face="sans-serif"> MPI_IREDUCE</font>,  
 
<li><font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font>,  
<font face="sans-serif"> MPI_IREDUCE_SCATTER_BLOCK</font>, <font face="sans-serif"> MPI_REDUCE_SCATTER</font>,  
<font face="sans-serif"> MPI_IREDUCE_SCATTER</font>.  
</ul> 
<br> 
  <div style=\"text-align:center\"><P><img width=533 height=406 src="collective-allgather.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 2: </b><span id="Figure2">Intercommunicator allgather.  The focus of data to one process is
    represented, not mandated by the semantics. 
    The two phases do allgathers in both directions.</span><P> 
  
    
  <div style=\"text-align:center\"><P><img width=533 height=406 src="collective-reduce_scatter.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 3: </b><span id="Figure3">Intercommunicator reduce-scatter.  The focus of data to one process
    is represented, not mandated by the semantics.
    The two phases do reduce-scatters in both directions.</span><P> 
  
    

<P>
<hr>
<a href="node97.htm#Node97"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node96.htm#Node96"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node99.htm#Node99"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node96.htm#Node96"> Communicator Argument</a>
<b>Next: </b><a href="node99.htm#Node99"> Specifics for Intercommunicator Collective Operations</a>
<b>Previous: </b><a href="node97.htm#Node97"> Specifics for Intracommunicator Collective Operations</a>
<p>
<HR>
Return to <A HREF="node523.htm">MPI-3.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-3.1 of June 4, 2015<BR>
HTML Generated on June 4, 2015
</FONT>
</body>
</html>
